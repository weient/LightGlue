nohup: ignoring input
EPOCH 1:
  batch 1000 loss: 393.1348576431274
LOSS train 393.1348576431274 valid 202.2722625732422
EPOCH 2:
  batch 1000 loss: 201.87467063331604
LOSS train 201.87467063331604 valid 186.8014373779297
EPOCH 3:
  batch 1000 loss: 197.54171680831908
LOSS train 197.54171680831908 valid 189.8244171142578
EPOCH 4:
  batch 1000 loss: 204.8621675338745
LOSS train 204.8621675338745 valid 175.22303771972656
EPOCH 5:
  batch 1000 loss: 189.00294676017762
LOSS train 189.00294676017762 valid 186.91876220703125
EPOCH 6:
  batch 1000 loss: 183.85166113471985
LOSS train 183.85166113471985 valid 174.92091369628906
EPOCH 7:
  batch 1000 loss: 168.68961987495422
LOSS train 168.68961987495422 valid 174.50717163085938
EPOCH 8:
  batch 1000 loss: 173.84507694816588
LOSS train 173.84507694816588 valid 172.90399169921875
EPOCH 9:
  batch 1000 loss: 180.77893732643128
LOSS train 180.77893732643128 valid 179.21253967285156
EPOCH 10:
  batch 1000 loss: 163.84232120895385
LOSS train 163.84232120895385 valid 176.2169647216797
EPOCH 11:
  batch 1000 loss: 169.43548614311217
LOSS train 169.43548614311217 valid 178.38247680664062
EPOCH 12:
  batch 1000 loss: 156.90061681556702
LOSS train 156.90061681556702 valid 179.7265167236328
EPOCH 13:
  batch 1000 loss: 174.8845243396759
LOSS train 174.8845243396759 valid 173.40756225585938
EPOCH 14:
  batch 1000 loss: 164.47892239379883
LOSS train 164.47892239379883 valid 169.89552307128906
EPOCH 15:
  batch 1000 loss: 171.34981368255615
LOSS train 171.34981368255615 valid 168.80381774902344
EPOCH 16:
  batch 1000 loss: 163.13562947273255
LOSS train 163.13562947273255 valid 170.60496520996094
EPOCH 17:
  batch 1000 loss: 184.2326885032654
LOSS train 184.2326885032654 valid 133.3302459716797
EPOCH 18:
  batch 1000 loss: 121.00996751594543
LOSS train 121.00996751594543 valid 123.82283020019531
EPOCH 19:
  batch 1000 loss: 121.79708342933655
LOSS train 121.79708342933655 valid 120.2315444946289
EPOCH 20:
  batch 1000 loss: 122.62575343704223
LOSS train 122.62575343704223 valid 120.03350830078125
EPOCH 21:
  batch 1000 loss: 125.32434154701232
LOSS train 125.32434154701232 valid 118.74459838867188
EPOCH 22:
  batch 1000 loss: 109.77035085868836
LOSS train 109.77035085868836 valid 120.86749267578125
EPOCH 23:
  batch 1000 loss: 118.59337738800049
LOSS train 118.59337738800049 valid 117.84748840332031
EPOCH 24:
  batch 1000 loss: 123.3535322227478
LOSS train 123.3535322227478 valid 116.60155487060547
EPOCH 25:
  batch 1000 loss: 110.52761895370483
LOSS train 110.52761895370483 valid 117.12515258789062
EPOCH 26:
  batch 1000 loss: 106.01290125656128
LOSS train 106.01290125656128 valid 114.69803619384766
EPOCH 27:
  batch 1000 loss: 115.99789167022705
LOSS train 115.99789167022705 valid 115.3576431274414
EPOCH 28:
  batch 1000 loss: 105.59987474441529
LOSS train 105.59987474441529 valid 114.40839385986328
EPOCH 29:
  batch 1000 loss: 106.06166521835327
LOSS train 106.06166521835327 valid 113.75819396972656
EPOCH 30:
  batch 1000 loss: 100.03178890991211
LOSS train 100.03178890991211 valid 115.05817413330078
EPOCH 31:
  batch 1000 loss: 97.33782961654663
LOSS train 97.33782961654663 valid 115.18635559082031
EPOCH 32:
  batch 1000 loss: 105.24180673027038
LOSS train 105.24180673027038 valid 113.80255889892578
EPOCH 33:
  batch 1000 loss: 117.84361615753174
LOSS train 117.84361615753174 valid 110.22483825683594
EPOCH 34:
  batch 1000 loss: 106.47361814308167
LOSS train 106.47361814308167 valid 102.48680114746094
EPOCH 35:
  batch 1000 loss: 94.95052744102478
LOSS train 94.95052744102478 valid 82.2303237915039
EPOCH 36:
  batch 1000 loss: 78.57948148727417
LOSS train 78.57948148727417 valid 90.06831359863281
EPOCH 37:
  batch 1000 loss: 87.03633982658386
LOSS train 87.03633982658386 valid 80.2148208618164
EPOCH 38:
  batch 1000 loss: 77.39897254753113
LOSS train 77.39897254753113 valid 77.49900817871094
EPOCH 39:
  batch 1000 loss: 79.56805044555664
LOSS train 79.56805044555664 valid 77.75853729248047
EPOCH 40:
  batch 1000 loss: 71.92403659439087
LOSS train 71.92403659439087 valid 76.35449981689453
EPOCH 41:
  batch 1000 loss: 80.39117007255554
LOSS train 80.39117007255554 valid 77.48898315429688
EPOCH 42:
  batch 1000 loss: 67.42017180633545
LOSS train 67.42017180633545 valid 78.42463684082031
EPOCH 43:
  batch 1000 loss: 75.26792608451844
LOSS train 75.26792608451844 valid 76.41734313964844
EPOCH 44:
  batch 1000 loss: 68.43331448745728
LOSS train 68.43331448745728 valid 74.61686706542969
EPOCH 45:
  batch 1000 loss: 72.11248868370056
LOSS train 72.11248868370056 valid 74.58867645263672
EPOCH 46:
  batch 1000 loss: 81.55435725784302
LOSS train 81.55435725784302 valid 76.82255554199219
EPOCH 47:
  batch 1000 loss: 72.27735064888
LOSS train 72.27735064888 valid 76.57232666015625
EPOCH 48:
  batch 1000 loss: 68.50316276168823
LOSS train 68.50316276168823 valid 74.47185516357422
EPOCH 49:
  batch 1000 loss: 71.33663784980774
LOSS train 71.33663784980774 valid 77.00578308105469
EPOCH 50:
  batch 1000 loss: 69.43431566619873
LOSS train 69.43431566619873 valid 74.59956359863281
EPOCH 51:
  batch 1000 loss: 75.6574606552124
LOSS train 75.6574606552124 valid 77.95028686523438
EPOCH 52:
  batch 1000 loss: 75.37733237075805
LOSS train 75.37733237075805 valid 75.46273040771484
EPOCH 53:
  batch 1000 loss: 76.0029175567627
LOSS train 76.0029175567627 valid 74.52125549316406
EPOCH 54:
  batch 1000 loss: 71.34552628707885
LOSS train 71.34552628707885 valid 74.35945129394531
EPOCH 55:
  batch 1000 loss: 67.99926194763184
LOSS train 67.99926194763184 valid 75.35868835449219
EPOCH 56:
  batch 1000 loss: 70.11148665428162
LOSS train 70.11148665428162 valid 76.17276000976562
EPOCH 57:
  batch 1000 loss: 74.31901478385926
LOSS train 74.31901478385926 valid 74.59514617919922
EPOCH 58:
  batch 1000 loss: 70.16041338539124
LOSS train 70.16041338539124 valid 75.30146026611328
EPOCH 59:
  batch 1000 loss: 73.57791560173035
LOSS train 73.57791560173035 valid 75.13536071777344
EPOCH 60:
  batch 1000 loss: 77.8820069141388
Epoch 00060: reducing learning rate of group 0 to 5.0000e-04.
LOSS train 77.8820069141388 valid 74.5990219116211
EPOCH 61:
  batch 1000 loss: 73.64018263149262
LOSS train 73.64018263149262 valid 73.51069641113281
EPOCH 62:
  batch 1000 loss: 74.91414808082581
LOSS train 74.91414808082581 valid 72.35763549804688
EPOCH 63:
  batch 1000 loss: 69.32862916851043
LOSS train 69.32862916851043 valid 73.0840835571289
EPOCH 64:
  batch 1000 loss: 72.2709991722107
LOSS train 72.2709991722107 valid 72.91117095947266
EPOCH 65:
  batch 1000 loss: 70.59924608802795
LOSS train 70.59924608802795 valid 72.90435791015625
EPOCH 66:
  batch 1000 loss: 66.61225086402894
LOSS train 66.61225086402894 valid 72.70825958251953
EPOCH 67:
  batch 1000 loss: 63.9907236995697
LOSS train 63.9907236995697 valid 72.9009017944336
EPOCH 68:
  batch 1000 loss: 70.12456186294555
Epoch 00068: reducing learning rate of group 0 to 2.5000e-04.
LOSS train 70.12456186294555 valid 72.79924774169922
EPOCH 69:
  batch 1000 loss: 66.343014503479
LOSS train 66.343014503479 valid 71.89274597167969
EPOCH 70:
  batch 1000 loss: 64.23982336330414
LOSS train 64.23982336330414 valid 71.80840301513672
EPOCH 71:
  batch 1000 loss: 68.6235318107605
LOSS train 68.6235318107605 valid 71.70990753173828
EPOCH 72:
  batch 1000 loss: 71.07357342243195
LOSS train 71.07357342243195 valid 71.902587890625
EPOCH 73:
  batch 1000 loss: 63.79359349632263
LOSS train 63.79359349632263 valid 71.51579284667969
EPOCH 74:
  batch 1000 loss: 72.35738501739502
LOSS train 72.35738501739502 valid 71.70672607421875
EPOCH 75:
  batch 1000 loss: 69.15323383712769
LOSS train 69.15323383712769 valid 71.91633605957031
EPOCH 76:
  batch 1000 loss: 63.568469472885134
LOSS train 63.568469472885134 valid 71.27346801757812
EPOCH 77:
  batch 1000 loss: 67.46304146385192
LOSS train 67.46304146385192 valid 71.7898941040039
EPOCH 78:
  batch 1000 loss: 63.09337763881683
LOSS train 63.09337763881683 valid 71.35980987548828
EPOCH 79:
  batch 1000 loss: 63.698235900878906
LOSS train 63.698235900878906 valid 71.388671875
EPOCH 80:
  batch 1000 loss: 61.536752285003665
LOSS train 61.536752285003665 valid 71.2027587890625
EPOCH 81:
  batch 1000 loss: 68.61087050819397
LOSS train 68.61087050819397 valid 71.45659637451172
EPOCH 82:
  batch 1000 loss: 63.98661284732819
LOSS train 63.98661284732819 valid 71.52708435058594
EPOCH 83:
  batch 1000 loss: 61.680193863868716
LOSS train 61.680193863868716 valid 71.29839324951172
EPOCH 84:
  batch 1000 loss: 72.67415019893646
LOSS train 72.67415019893646 valid 71.34967041015625
EPOCH 85:
  batch 1000 loss: 68.37788112545013
LOSS train 68.37788112545013 valid 71.43375396728516
EPOCH 86:
  batch 1000 loss: 70.994269780159
LOSS train 70.994269780159 valid 71.1623764038086
EPOCH 87:
  batch 1000 loss: 71.43637018203735
LOSS train 71.43637018203735 valid 71.25817108154297
EPOCH 88:
  batch 1000 loss: 62.54191121196747
LOSS train 62.54191121196747 valid 71.32466125488281
EPOCH 89:
  batch 1000 loss: 61.58190026855469
LOSS train 61.58190026855469 valid 71.68445587158203
EPOCH 90:
  batch 1000 loss: 66.49539233303071
LOSS train 66.49539233303071 valid 71.46971130371094
EPOCH 91:
  batch 1000 loss: 69.63695602321624
LOSS train 69.63695602321624 valid 71.46880340576172
EPOCH 92:
  batch 1000 loss: 64.79087701034545
LOSS train 64.79087701034545 valid 71.06006622314453
EPOCH 93:
  batch 1000 loss: 66.64203884220123
LOSS train 66.64203884220123 valid 71.21981811523438
EPOCH 94:
  batch 1000 loss: 64.50980147457123
LOSS train 64.50980147457123 valid 71.06668853759766
EPOCH 95:
  batch 1000 loss: 68.12418395614624
LOSS train 68.12418395614624 valid 71.40563201904297
EPOCH 96:
  batch 1000 loss: 66.28366795539856
LOSS train 66.28366795539856 valid 71.16636657714844
EPOCH 97:
  batch 1000 loss: 68.17738929367066
LOSS train 68.17738929367066 valid 71.39415740966797
EPOCH 98:
  batch 1000 loss: 70.6727737197876
LOSS train 70.6727737197876 valid 71.02774047851562
EPOCH 99:
  batch 1000 loss: 66.78259421348572
LOSS train 66.78259421348572 valid 71.22532653808594
EPOCH 100:
  batch 1000 loss: 70.43590357017517
LOSS train 70.43590357017517 valid 70.79558563232422
EPOCH 101:
  batch 1000 loss: 67.46837400913239
LOSS train 67.46837400913239 valid 71.23881530761719
EPOCH 102:
  batch 1000 loss: 67.13127000427247
LOSS train 67.13127000427247 valid 70.987060546875
EPOCH 103:
  batch 1000 loss: 66.84789143657684
LOSS train 66.84789143657684 valid 71.34080505371094
EPOCH 104:
  batch 1000 loss: 69.39408406066894
LOSS train 69.39408406066894 valid 70.92279052734375
EPOCH 105:
  batch 1000 loss: 65.94603734970093
LOSS train 65.94603734970093 valid 70.90762329101562
EPOCH 106:
  batch 1000 loss: 64.43424918746948
Epoch 00106: reducing learning rate of group 0 to 1.2500e-04.
LOSS train 64.43424918746948 valid 71.07098388671875
EPOCH 107:
  batch 1000 loss: 63.20931100845337
LOSS train 63.20931100845337 valid 70.72769165039062
EPOCH 108:
  batch 1000 loss: 67.40590976715087
LOSS train 67.40590976715087 valid 70.63385772705078
EPOCH 109:
  batch 1000 loss: 67.06892975521087
LOSS train 67.06892975521087 valid 70.59654235839844
EPOCH 110:
  batch 1000 loss: 61.27098119354248
LOSS train 61.27098119354248 valid 70.57511138916016
EPOCH 111:
  batch 1000 loss: 63.94230481147766
LOSS train 63.94230481147766 valid 70.6087417602539
EPOCH 112:
  batch 1000 loss: 67.78755107307434
LOSS train 67.78755107307434 valid 70.51882934570312
EPOCH 113:
  batch 1000 loss: 63.18299460411072
LOSS train 63.18299460411072 valid 70.63822174072266
EPOCH 114:
  batch 1000 loss: 69.23518105602264
LOSS train 69.23518105602264 valid 70.57213592529297
EPOCH 115:
  batch 1000 loss: 66.44667983818054
LOSS train 66.44667983818054 valid 70.6174545288086
EPOCH 116:
  batch 1000 loss: 67.17167020130158
LOSS train 67.17167020130158 valid 70.53077697753906
EPOCH 117:
  batch 1000 loss: 66.40456192684174
LOSS train 66.40456192684174 valid 70.50899505615234
EPOCH 118:
  batch 1000 loss: 69.23411513614654
LOSS train 69.23411513614654 valid 70.44246673583984
EPOCH 119:
  batch 1000 loss: 66.51446925735473
LOSS train 66.51446925735473 valid 70.3860855102539
EPOCH 120:
  batch 1000 loss: 68.47926236629486
LOSS train 68.47926236629486 valid 70.7486343383789
EPOCH 121:
  batch 1000 loss: 65.82249796676636
LOSS train 65.82249796676636 valid 70.72953033447266
EPOCH 122:
  batch 1000 loss: 66.3314912891388
LOSS train 66.3314912891388 valid 70.61917877197266
EPOCH 123:
  batch 1000 loss: 64.34521838569641
LOSS train 64.34521838569641 valid 70.61106872558594
EPOCH 124:
  batch 1000 loss: 64.12966330432891
LOSS train 64.12966330432891 valid 70.54996490478516
EPOCH 125:
  batch 1000 loss: 63.50423410987854
Epoch 00125: reducing learning rate of group 0 to 6.2500e-05.
LOSS train 63.50423410987854 valid 70.48589324951172
EPOCH 126:
  batch 1000 loss: 63.44446752262115
LOSS train 63.44446752262115 valid 70.3296890258789
EPOCH 127:
  batch 1000 loss: 62.98355749320984
LOSS train 62.98355749320984 valid 70.49969482421875
EPOCH 128:
  batch 1000 loss: 63.15468787002563
LOSS train 63.15468787002563 valid 70.3198471069336
EPOCH 129:
  batch 1000 loss: 62.83395282554626
LOSS train 62.83395282554626 valid 70.47679901123047
EPOCH 130:
  batch 1000 loss: 67.35948290729523
LOSS train 67.35948290729523 valid 70.2997055053711
EPOCH 131:
  batch 1000 loss: 65.58050835514068
LOSS train 65.58050835514068 valid 70.34447479248047
EPOCH 132:
  batch 1000 loss: 65.01535097980499
LOSS train 65.01535097980499 valid 70.44351959228516
EPOCH 133:
  batch 1000 loss: 63.20753565311432
LOSS train 63.20753565311432 valid 70.29906463623047
EPOCH 134:
  batch 1000 loss: 63.78463016319275
LOSS train 63.78463016319275 valid 70.31187438964844
EPOCH 135:
  batch 1000 loss: 59.62061015319824
LOSS train 59.62061015319824 valid 70.31053924560547
EPOCH 136:
  batch 1000 loss: 66.85285239124298
LOSS train 66.85285239124298 valid 70.23873901367188
EPOCH 137:
  batch 1000 loss: 67.18456197547913
LOSS train 67.18456197547913 valid 70.2042465209961
EPOCH 138:
  batch 1000 loss: 68.65645184993744
LOSS train 68.65645184993744 valid 70.32488250732422
EPOCH 139:
  batch 1000 loss: 67.03143853569031
LOSS train 67.03143853569031 valid 70.36231994628906
EPOCH 140:
  batch 1000 loss: 69.08515539073944
LOSS train 69.08515539073944 valid 70.35707092285156
EPOCH 141:
  batch 1000 loss: 64.11818766880036
LOSS train 64.11818766880036 valid 70.34683990478516
EPOCH 142:
  batch 1000 loss: 65.77371604251861
LOSS train 65.77371604251861 valid 70.2851791381836
EPOCH 143:
  batch 1000 loss: 65.2411306362152
Epoch 00143: reducing learning rate of group 0 to 3.1250e-05.
LOSS train 65.2411306362152 valid 70.25238037109375
EPOCH 144:
  batch 1000 loss: 64.48527594852447
LOSS train 64.48527594852447 valid 70.17555236816406
EPOCH 145:
  batch 1000 loss: 62.72950030994415
LOSS train 62.72950030994415 valid 70.18560791015625
EPOCH 146:
  batch 1000 loss: 67.80765954589843
LOSS train 67.80765954589843 valid 70.26901245117188
EPOCH 147:
  batch 1000 loss: 64.24387703704834
LOSS train 64.24387703704834 valid 70.1347427368164
EPOCH 148:
  batch 1000 loss: 61.666462266922
LOSS train 61.666462266922 valid 70.17781829833984
EPOCH 149:
  batch 1000 loss: 65.69867101097107
LOSS train 65.69867101097107 valid 70.17418670654297
EPOCH 150:
  batch 1000 loss: 65.01091931915283
LOSS train 65.01091931915283 valid 70.02327728271484
EPOCH 151:
  batch 1000 loss: 66.7647456331253
LOSS train 66.7647456331253 valid 70.07744598388672
EPOCH 152:
  batch 1000 loss: 68.81101133155823
LOSS train 68.81101133155823 valid 70.07514190673828
EPOCH 153:
  batch 1000 loss: 63.1982008562088
LOSS train 63.1982008562088 valid 70.08116912841797
EPOCH 154:
  batch 1000 loss: 66.128362534523
LOSS train 66.128362534523 valid 70.20308685302734
EPOCH 155:
  batch 1000 loss: 60.14905125999451
LOSS train 60.14905125999451 valid 70.095947265625
EPOCH 156:
  batch 1000 loss: 68.49650740146637
Epoch 00156: reducing learning rate of group 0 to 1.5625e-05.
LOSS train 68.49650740146637 valid 70.16130828857422
EPOCH 157:
  batch 1000 loss: 68.63223882770538
LOSS train 68.63223882770538 valid 70.09365844726562
EPOCH 158:
  batch 1000 loss: 63.710846551895145
LOSS train 63.710846551895145 valid 70.00254821777344
EPOCH 159:
  batch 1000 loss: 63.14162605476379
LOSS train 63.14162605476379 valid 70.20486450195312
EPOCH 160:
  batch 1000 loss: 64.0321936416626
LOSS train 64.0321936416626 valid 69.9556884765625
EPOCH 161:
  batch 1000 loss: 65.80469406795501
LOSS train 65.80469406795501 valid 70.10247802734375
EPOCH 162:
  batch 1000 loss: 65.11439449310302
LOSS train 65.11439449310302 valid 70.13909912109375
EPOCH 163:
  batch 1000 loss: 64.77054427623749
LOSS train 64.77054427623749 valid 70.01438903808594
EPOCH 164:
  batch 1000 loss: 66.15411722946168
LOSS train 66.15411722946168 valid 70.14583587646484
EPOCH 165:
  batch 1000 loss: 61.69658713054657
LOSS train 61.69658713054657 valid 70.00565338134766
EPOCH 166:
  batch 1000 loss: 62.437401425361635
Epoch 00166: reducing learning rate of group 0 to 7.8125e-06.
LOSS train 62.437401425361635 valid 70.17314910888672
EPOCH 167:
  batch 1000 loss: 66.00074970912934
LOSS train 66.00074970912934 valid 70.00840759277344
EPOCH 168:
  batch 1000 loss: 62.112518818855285
LOSS train 62.112518818855285 valid 70.07852172851562
EPOCH 169:
  batch 1000 loss: 63.96421948814392
LOSS train 63.96421948814392 valid 70.10470581054688
EPOCH 170:
  batch 1000 loss: 64.53221524333954
LOSS train 64.53221524333954 valid 70.0473861694336
EPOCH 171:
  batch 1000 loss: 66.3784795665741
LOSS train 66.3784795665741 valid 69.98796081542969
EPOCH 172:
  batch 1000 loss: 61.33278868198395
Epoch 00172: reducing learning rate of group 0 to 3.9063e-06.
LOSS train 61.33278868198395 valid 70.04598999023438
EPOCH 173:
  batch 1000 loss: 67.3019836730957
LOSS train 67.3019836730957 valid 70.06986236572266
EPOCH 174:
  batch 1000 loss: 61.917521879196165
LOSS train 61.917521879196165 valid 70.10653686523438
EPOCH 175:
  batch 1000 loss: 64.80811980724334
LOSS train 64.80811980724334 valid 69.9731216430664
EPOCH 176:
  batch 1000 loss: 61.71998866653443
LOSS train 61.71998866653443 valid 70.04370880126953
EPOCH 177:
  batch 1000 loss: 66.7025752544403
LOSS train 66.7025752544403 valid 70.00817108154297
EPOCH 178:
  batch 1000 loss: 64.78368611717224
LOSS train 64.78368611717224 valid 69.9450454711914
EPOCH 179:
  batch 1000 loss: 61.49107913589477
LOSS train 61.49107913589477 valid 70.10603332519531
EPOCH 180:
  batch 1000 loss: 63.51288335037231
LOSS train 63.51288335037231 valid 69.91545867919922
EPOCH 181:
  batch 1000 loss: 63.923909152030944
LOSS train 63.923909152030944 valid 70.0034408569336
EPOCH 182:
  batch 1000 loss: 64.48960907649995
LOSS train 64.48960907649995 valid 69.99081420898438
EPOCH 183:
  batch 1000 loss: 66.95309886550903
LOSS train 66.95309886550903 valid 69.94580841064453
EPOCH 184:
  batch 1000 loss: 61.99694357585907
LOSS train 61.99694357585907 valid 70.08367156982422
EPOCH 185:
  batch 1000 loss: 62.03312469387055
LOSS train 62.03312469387055 valid 69.95699310302734
EPOCH 186:
  batch 1000 loss: 59.613561561584476
Epoch 00186: reducing learning rate of group 0 to 1.9531e-06.
LOSS train 59.613561561584476 valid 69.99382781982422
EPOCH 187:
  batch 1000 loss: 65.71744212913514
LOSS train 65.71744212913514 valid 70.05968475341797
EPOCH 188:
  batch 1000 loss: 69.06727360153198
LOSS train 69.06727360153198 valid 70.09131622314453
EPOCH 189:
  batch 1000 loss: 66.33597114467621
LOSS train 66.33597114467621 valid 69.95933532714844
EPOCH 190:
  batch 1000 loss: 65.48345303249359
LOSS train 65.48345303249359 valid 69.96329498291016
EPOCH 191:
  batch 1000 loss: 68.62405664443969
LOSS train 68.62405664443969 valid 69.92345428466797
EPOCH 192:
  batch 1000 loss: 67.01129207229614
Epoch 00192: reducing learning rate of group 0 to 9.7656e-07.
LOSS train 67.01129207229614 valid 69.9564437866211
EPOCH 193:
  batch 1000 loss: 63.99183996200561
LOSS train 63.99183996200561 valid 69.94232940673828
EPOCH 194:
  batch 1000 loss: 63.5921332359314
LOSS train 63.5921332359314 valid 70.09236907958984
EPOCH 195:
  batch 1000 loss: 62.10961402416229
LOSS train 62.10961402416229 valid 70.00515747070312
EPOCH 196:
  batch 1000 loss: 59.85936384010315
LOSS train 59.85936384010315 valid 70.06427764892578
EPOCH 197:
  batch 1000 loss: 68.86383688640595
LOSS train 68.86383688640595 valid 69.95697021484375
EPOCH 198:
  batch 1000 loss: 64.75741753959656
Epoch 00198: reducing learning rate of group 0 to 4.8828e-07.
LOSS train 64.75741753959656 valid 69.96490478515625
EPOCH 199:
  batch 1000 loss: 64.07610875701904
LOSS train 64.07610875701904 valid 70.0857925415039
EPOCH 200:
  batch 1000 loss: 65.14002301216125
LOSS train 65.14002301216125 valid 70.03987121582031
EPOCH 201:
  batch 1000 loss: 61.05525560092926
LOSS train 61.05525560092926 valid 69.89408111572266
EPOCH 202:
  batch 1000 loss: 63.845787952423095
LOSS train 63.845787952423095 valid 69.86444854736328
EPOCH 203:
  batch 1000 loss: 62.201551863670346
LOSS train 62.201551863670346 valid 69.95629119873047
EPOCH 204:
  batch 1000 loss: 65.90842069625855
LOSS train 65.90842069625855 valid 69.94231414794922
EPOCH 205:
  batch 1000 loss: 59.60480800437927
LOSS train 59.60480800437927 valid 70.0345230102539
EPOCH 206:
  batch 1000 loss: 62.1778708152771
LOSS train 62.1778708152771 valid 69.94677734375
EPOCH 207:
  batch 1000 loss: 61.95646780872345
LOSS train 61.95646780872345 valid 70.07391357421875
EPOCH 208:
  batch 1000 loss: 67.48505363082886
Epoch 00208: reducing learning rate of group 0 to 2.4414e-07.
LOSS train 67.48505363082886 valid 70.03124237060547
EPOCH 209:
  batch 1000 loss: 65.13370151233673
LOSS train 65.13370151233673 valid 69.95565795898438
EPOCH 210:
  batch 1000 loss: 60.66431011962891
LOSS train 60.66431011962891 valid 70.0374984741211
EPOCH 211:
  batch 1000 loss: 66.83481661319733
LOSS train 66.83481661319733 valid 70.03689575195312
EPOCH 212:
  batch 1000 loss: 63.038937772750856
LOSS train 63.038937772750856 valid 70.03545379638672
EPOCH 213:
  batch 1000 loss: 63.38632286262512
LOSS train 63.38632286262512 valid 69.9937515258789
EPOCH 214:
  batch 1000 loss: 60.972322577476504
Epoch 00214: reducing learning rate of group 0 to 1.2207e-07.
LOSS train 60.972322577476504 valid 69.86669158935547
EPOCH 215:
  batch 1000 loss: 63.12767009735107
LOSS train 63.12767009735107 valid 70.03755187988281
EPOCH 216:
  batch 1000 loss: 66.61386860370636
LOSS train 66.61386860370636 valid 70.03682708740234
EPOCH 217:
  batch 1000 loss: 59.3279519443512
LOSS train 59.3279519443512 valid 69.98927307128906
EPOCH 218:
  batch 1000 loss: 61.007664980888364
LOSS train 61.007664980888364 valid 69.9393081665039
EPOCH 219:
  batch 1000 loss: 65.48432080078125
LOSS train 65.48432080078125 valid 70.02125549316406
EPOCH 220:
  batch 1000 loss: 63.72475217437744
Epoch 00220: reducing learning rate of group 0 to 6.1035e-08.
LOSS train 63.72475217437744 valid 70.03422546386719
EPOCH 221:
  batch 1000 loss: 65.37751764583588
LOSS train 65.37751764583588 valid 70.03271484375
EPOCH 222:
  batch 1000 loss: 61.485869888305665
LOSS train 61.485869888305665 valid 70.03633880615234
EPOCH 223:
  batch 1000 loss: 61.22542369556427
LOSS train 61.22542369556427 valid 69.93890380859375
EPOCH 224:
  batch 1000 loss: 62.58953480052948
LOSS train 62.58953480052948 valid 70.01518249511719
EPOCH 225:
  batch 1000 loss: 64.24581236362457
LOSS train 64.24581236362457 valid 70.08294677734375
EPOCH 226:
  batch 1000 loss: 59.92534412860871
Epoch 00226: reducing learning rate of group 0 to 3.0518e-08.
LOSS train 59.92534412860871 valid 69.94918060302734
EPOCH 227:
  batch 1000 loss: 63.63609616470337
LOSS train 63.63609616470337 valid 69.94851684570312
EPOCH 228:
  batch 1000 loss: 64.71037619495392
LOSS train 64.71037619495392 valid 69.83837127685547
EPOCH 229:
  batch 1000 loss: 64.15715844726563
LOSS train 64.15715844726563 valid 70.04107666015625
EPOCH 230:
  batch 1000 loss: 61.835427283287046
LOSS train 61.835427283287046 valid 70.04585266113281
EPOCH 231:
  batch 1000 loss: 61.74436827659607
LOSS train 61.74436827659607 valid 70.0374755859375
EPOCH 232:
  batch 1000 loss: 59.16467450428009
LOSS train 59.16467450428009 valid 69.93869018554688
EPOCH 233:
  batch 1000 loss: 66.36536839389801
LOSS train 66.36536839389801 valid 69.83389282226562
EPOCH 234:
  batch 1000 loss: 60.776358253479
Epoch 00234: reducing learning rate of group 0 to 1.5259e-08.
LOSS train 60.776358253479 valid 70.03465270996094
EPOCH 235:
  batch 1000 loss: 62.792993755340575
LOSS train 62.792993755340575 valid 69.93209075927734
EPOCH 236:
  batch 1000 loss: 64.35381297397613
LOSS train 64.35381297397613 valid 69.8756332397461
EPOCH 237:
  batch 1000 loss: 65.89676929569245
LOSS train 65.89676929569245 valid 69.94303894042969
EPOCH 238:
  batch 1000 loss: 64.5894382686615
LOSS train 64.5894382686615 valid 69.94853973388672
EPOCH 239:
  batch 1000 loss: 57.99197047615051
LOSS train 57.99197047615051 valid 70.08422088623047
EPOCH 240:
  batch 1000 loss: 64.15256463623047
LOSS train 64.15256463623047 valid 69.9816665649414
EPOCH 241:
  batch 1000 loss: 61.16440567684174
LOSS train 61.16440567684174 valid 70.03580474853516
EPOCH 242:
  batch 1000 loss: 66.57267845344543
LOSS train 66.57267845344543 valid 69.85263061523438
EPOCH 243:
  batch 1000 loss: 60.87278881072998
LOSS train 60.87278881072998 valid 70.02515411376953
EPOCH 244:
  batch 1000 loss: 62.457025292396544
LOSS train 62.457025292396544 valid 69.85130310058594
EPOCH 245:
  batch 1000 loss: 61.87717859172821
LOSS train 61.87717859172821 valid 69.96135711669922
EPOCH 246:
  batch 1000 loss: 64.16753641986847
LOSS train 64.16753641986847 valid 69.94927978515625
EPOCH 247:
  batch 1000 loss: 63.65393064498901
LOSS train 63.65393064498901 valid 70.03257751464844
EPOCH 248:
  batch 1000 loss: 66.9170011768341
LOSS train 66.9170011768341 valid 69.9622573852539
EPOCH 249:
  batch 1000 loss: 63.892489751815795
LOSS train 63.892489751815795 valid 69.95560455322266
EPOCH 250:
  batch 1000 loss: 62.176603548049926
LOSS train 62.176603548049926 valid 69.96006774902344
EPOCH 251:
  batch 1000 loss: 60.78272353458404
LOSS train 60.78272353458404 valid 69.94937133789062
EPOCH 252:
  batch 1000 loss: 65.4227576932907
LOSS train 65.4227576932907 valid 69.95574951171875
EPOCH 253:
  batch 1000 loss: 66.11007575130462
LOSS train 66.11007575130462 valid 69.98411560058594
EPOCH 254:
  batch 1000 loss: 65.03735335254669
LOSS train 65.03735335254669 valid 70.00436401367188
EPOCH 255:
  batch 1000 loss: 67.32477867031098
LOSS train 67.32477867031098 valid 70.12706756591797
EPOCH 256:
  batch 1000 loss: 66.20056286525727
LOSS train 66.20056286525727 valid 69.99163055419922
EPOCH 257:
  batch 1000 loss: 64.90475224018097
LOSS train 64.90475224018097 valid 69.95722961425781
EPOCH 258:
  batch 1000 loss: 62.50689011287689
LOSS train 62.50689011287689 valid 70.08843994140625
EPOCH 259:
  batch 1000 loss: 64.62029634475708
LOSS train 64.62029634475708 valid 69.95155334472656
EPOCH 260:
  batch 1000 loss: 60.40448596000672
LOSS train 60.40448596000672 valid 69.9535140991211
EPOCH 261:
  batch 1000 loss: 66.10388014698029
LOSS train 66.10388014698029 valid 69.99053192138672
EPOCH 262:
  batch 1000 loss: 63.58449334430695
LOSS train 63.58449334430695 valid 69.99398803710938
EPOCH 263:
  batch 1000 loss: 62.016312182426454
LOSS train 62.016312182426454 valid 69.84559631347656
EPOCH 264:
  batch 1000 loss: 63.936295822143556
LOSS train 63.936295822143556 valid 70.04411315917969
EPOCH 265:
  batch 1000 loss: 63.63658922576904
LOSS train 63.63658922576904 valid 69.95311737060547
EPOCH 266:
  batch 1000 loss: 67.8938535079956
LOSS train 67.8938535079956 valid 69.98573303222656
EPOCH 267:
  batch 1000 loss: 65.65777557659149
LOSS train 65.65777557659149 valid 70.0295639038086
EPOCH 268:
  batch 1000 loss: 62.58563198184967
LOSS train 62.58563198184967 valid 69.9486312866211
EPOCH 269:
  batch 1000 loss: 61.323655842781065
LOSS train 61.323655842781065 valid 70.05567932128906
EPOCH 270:
  batch 1000 loss: 66.49595330142975
LOSS train 66.49595330142975 valid 69.95304107666016
EPOCH 271:
  batch 1000 loss: 61.872787267684934
LOSS train 61.872787267684934 valid 70.035888671875
EPOCH 272:
  batch 1000 loss: 60.56761983585358
LOSS train 60.56761983585358 valid 69.95216369628906
EPOCH 273:
  batch 1000 loss: 66.22664355659485
LOSS train 66.22664355659485 valid 70.01909637451172
EPOCH 274:
  batch 1000 loss: 60.332325971603396
LOSS train 60.332325971603396 valid 69.9859619140625
EPOCH 275:
  batch 1000 loss: 59.3839545879364
LOSS train 59.3839545879364 valid 69.93782043457031
EPOCH 276:
  batch 1000 loss: 63.31945418167114
LOSS train 63.31945418167114 valid 70.0367660522461
EPOCH 277:
  batch 1000 loss: 66.62841273784638
LOSS train 66.62841273784638 valid 70.02460479736328
EPOCH 278:
  batch 1000 loss: 62.611923008918765
LOSS train 62.611923008918765 valid 69.93992614746094
EPOCH 279:
  batch 1000 loss: 64.92705894470215
LOSS train 64.92705894470215 valid 70.01815795898438
EPOCH 280:
  batch 1000 loss: 62.97443108654022
LOSS train 62.97443108654022 valid 69.85540771484375
EPOCH 281:
  batch 1000 loss: 63.47896087551117
LOSS train 63.47896087551117 valid 69.8607406616211
EPOCH 282:
  batch 1000 loss: 67.18747247219086
LOSS train 67.18747247219086 valid 70.07044982910156
EPOCH 283:
  batch 1000 loss: 62.90992304039001
LOSS train 62.90992304039001 valid 69.93404388427734
EPOCH 284:

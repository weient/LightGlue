nohup: ignoring input
EPOCH 1:
  batch 1000 loss: 6.712632025957108
LOSS train 6.712632025957108 valid 5.836293697357178
EPOCH 2:
  batch 1000 loss: 5.742134800195694
LOSS train 5.742134800195694 valid 5.691840648651123
EPOCH 3:
  batch 1000 loss: 5.54024760723114
LOSS train 5.54024760723114 valid 5.420375823974609
EPOCH 4:
  batch 1000 loss: 5.280945404052734
LOSS train 5.280945404052734 valid 5.249546051025391
EPOCH 5:
  batch 1000 loss: 5.123047669410705
LOSS train 5.123047669410705 valid 5.067034721374512
EPOCH 6:
  batch 1000 loss: 4.922932869672775
LOSS train 4.922932869672775 valid 4.854072570800781
EPOCH 7:
  batch 1000 loss: 4.813498521327973
LOSS train 4.813498521327973 valid 4.755368709564209
EPOCH 8:
  batch 1000 loss: 4.648067572832107
LOSS train 4.648067572832107 valid 4.626628875732422
EPOCH 9:
  batch 1000 loss: 4.556799768686295
LOSS train 4.556799768686295 valid 4.514214038848877
EPOCH 10:
  batch 1000 loss: 4.452436877727509
LOSS train 4.452436877727509 valid 4.442854404449463
EPOCH 11:
  batch 1000 loss: 4.3915136888027195
LOSS train 4.3915136888027195 valid 4.3543267250061035
EPOCH 12:
  batch 1000 loss: 4.329131807565689
LOSS train 4.329131807565689 valid 4.453696250915527
EPOCH 13:
  batch 1000 loss: 4.297698770999909
LOSS train 4.297698770999909 valid 4.286040306091309
EPOCH 14:
  batch 1000 loss: 4.220571283340454
LOSS train 4.220571283340454 valid 4.238611221313477
EPOCH 15:
  batch 1000 loss: 4.164751957416534
LOSS train 4.164751957416534 valid 4.216233730316162
EPOCH 16:
  batch 1000 loss: 4.178526401758194
LOSS train 4.178526401758194 valid 4.171228408813477
EPOCH 17:
  batch 1000 loss: 4.176506547927857
LOSS train 4.176506547927857 valid 4.124565601348877
EPOCH 18:
  batch 1000 loss: 4.0914644711017605
LOSS train 4.0914644711017605 valid 4.101702690124512
EPOCH 19:
  batch 1000 loss: 4.05947361254692
LOSS train 4.05947361254692 valid 4.094748497009277
EPOCH 20:
  batch 1000 loss: 4.065766620874405
LOSS train 4.065766620874405 valid 4.044708728790283
EPOCH 21:
  batch 1000 loss: 4.0552658822536465
LOSS train 4.0552658822536465 valid 4.056580543518066
EPOCH 22:
  batch 1000 loss: 4.027321723222733
LOSS train 4.027321723222733 valid 4.043173789978027
EPOCH 23:
  batch 1000 loss: 4.0101745889186855
LOSS train 4.0101745889186855 valid 4.00366735458374
EPOCH 24:
  batch 1000 loss: 3.991252536058426
LOSS train 3.991252536058426 valid 4.079832553863525
EPOCH 25:
  batch 1000 loss: 3.974270723581314
LOSS train 3.974270723581314 valid 3.984066963195801
EPOCH 26:
  batch 1000 loss: 4.0036795954704285
LOSS train 4.0036795954704285 valid 3.9932315349578857
EPOCH 27:
  batch 1000 loss: 3.9559530725479126
LOSS train 3.9559530725479126 valid 4.049929618835449
EPOCH 28:
  batch 1000 loss: 3.9515826213359833
LOSS train 3.9515826213359833 valid 3.9605636596679688
EPOCH 29:
  batch 1000 loss: 3.8999558169841766
LOSS train 3.8999558169841766 valid 3.9607558250427246
EPOCH 30:
  batch 1000 loss: 3.8902901556491853
LOSS train 3.8902901556491853 valid 3.8848512172698975
EPOCH 31:
  batch 1000 loss: 3.87518803024292
LOSS train 3.87518803024292 valid 3.9177560806274414
EPOCH 32:
  batch 1000 loss: 3.9050299160480497
LOSS train 3.9050299160480497 valid 3.925388813018799
EPOCH 33:
  batch 1000 loss: 3.868807097196579
LOSS train 3.868807097196579 valid 4.030628681182861
EPOCH 34:
  batch 1000 loss: 3.8914956641197205
LOSS train 3.8914956641197205 valid 3.8869810104370117
EPOCH 35:
  batch 1000 loss: 3.851137476682663
LOSS train 3.851137476682663 valid 3.8646416664123535
EPOCH 36:
  batch 1000 loss: 3.836048235177994
LOSS train 3.836048235177994 valid 3.8843376636505127
EPOCH 37:
  batch 1000 loss: 3.8539500160217286
LOSS train 3.8539500160217286 valid 3.819653272628784
EPOCH 38:
  batch 1000 loss: 3.7827050545215606
LOSS train 3.7827050545215606 valid 3.832157850265503
EPOCH 39:
  batch 1000 loss: 3.805601687669754
LOSS train 3.805601687669754 valid 3.8326706886291504
EPOCH 40:
  batch 1000 loss: 3.8331366419792174
LOSS train 3.8331366419792174 valid 3.79628324508667
EPOCH 41:
  batch 1000 loss: 3.790003232240677
LOSS train 3.790003232240677 valid 3.773599863052368
EPOCH 42:
  batch 1000 loss: 3.778493188619614
LOSS train 3.778493188619614 valid 3.8065032958984375
EPOCH 43:
  batch 1000 loss: 3.8183179976940154
LOSS train 3.8183179976940154 valid 3.762356996536255
EPOCH 44:
  batch 1000 loss: 3.752748031616211
LOSS train 3.752748031616211 valid 3.83414626121521
EPOCH 45:
  batch 1000 loss: 3.7671258668899537
LOSS train 3.7671258668899537 valid 3.777492046356201
EPOCH 46:
  batch 1000 loss: 3.77321433866024
LOSS train 3.77321433866024 valid 3.7467713356018066
EPOCH 47:
  batch 1000 loss: 3.735704821586609
LOSS train 3.735704821586609 valid 3.768524408340454
EPOCH 48:
  batch 1000 loss: 3.7485839698314667
LOSS train 3.7485839698314667 valid 3.7559688091278076
EPOCH 49:
  batch 1000 loss: 3.762303940296173
LOSS train 3.762303940296173 valid 3.7713677883148193
EPOCH 50:
  batch 1000 loss: 3.719201154112816
LOSS train 3.719201154112816 valid 3.7240452766418457
EPOCH 51:
  batch 1000 loss: 3.7184727869033813
LOSS train 3.7184727869033813 valid 3.7184407711029053
EPOCH 52:
  batch 1000 loss: 3.735196459889412
LOSS train 3.735196459889412 valid 3.766695261001587
EPOCH 53:
  batch 1000 loss: 3.714330265045166
LOSS train 3.714330265045166 valid 3.7175495624542236
EPOCH 54:
  batch 1000 loss: 3.6893944108486174
LOSS train 3.6893944108486174 valid 3.7026052474975586
EPOCH 55:
  batch 1000 loss: 3.6899991270303727
LOSS train 3.6899991270303727 valid 3.7632334232330322
EPOCH 56:
  batch 1000 loss: 3.7055497875213623
LOSS train 3.7055497875213623 valid 3.696826934814453
EPOCH 57:
  batch 1000 loss: 3.7025108945369722
LOSS train 3.7025108945369722 valid 3.706240653991699
EPOCH 58:
  batch 1000 loss: 3.660539699077606
LOSS train 3.660539699077606 valid 3.703258991241455
EPOCH 59:
  batch 1000 loss: 3.664838753938675
LOSS train 3.664838753938675 valid 3.669381618499756
EPOCH 60:
  batch 1000 loss: 3.6613085680007935
LOSS train 3.6613085680007935 valid 3.8041720390319824
EPOCH 61:
  batch 1000 loss: 3.676923630475998
LOSS train 3.676923630475998 valid 3.7275586128234863
EPOCH 62:
  batch 1000 loss: 3.6753820974826814
LOSS train 3.6753820974826814 valid 3.715952157974243
EPOCH 63:
  batch 1000 loss: 3.672889555811882
LOSS train 3.672889555811882 valid 3.699551820755005
EPOCH 64:
  batch 1000 loss: 3.6592903277873994
LOSS train 3.6592903277873994 valid 3.6678762435913086
EPOCH 65:
  batch 1000 loss: 3.673228166103363
LOSS train 3.673228166103363 valid 3.6732523441314697
EPOCH 66:
  batch 1000 loss: 3.6926354229450227
LOSS train 3.6926354229450227 valid 3.6729726791381836
EPOCH 67:
  batch 1000 loss: 3.6756828336715697
LOSS train 3.6756828336715697 valid 3.6728615760803223
EPOCH 68:
  batch 1000 loss: 3.623164450407028
LOSS train 3.623164450407028 valid 3.6673026084899902
EPOCH 69:
  batch 1000 loss: 3.6131447851657867
LOSS train 3.6131447851657867 valid 3.7617056369781494
EPOCH 70:
  batch 1000 loss: 3.6588847707509995
LOSS train 3.6588847707509995 valid 3.7000820636749268
EPOCH 71:
  batch 1000 loss: 3.634646794080734
LOSS train 3.634646794080734 valid 3.663581132888794
EPOCH 72:
  batch 1000 loss: 3.663287008047104
LOSS train 3.663287008047104 valid 3.663208484649658
EPOCH 73:
  batch 1000 loss: 3.619243585109711
LOSS train 3.619243585109711 valid 3.6876039505004883
EPOCH 74:
  batch 1000 loss: 3.6430735062360764
LOSS train 3.6430735062360764 valid 3.717405080795288
EPOCH 75:
  batch 1000 loss: 3.6430803220272066
LOSS train 3.6430803220272066 valid 3.621210813522339
EPOCH 76:
  batch 1000 loss: 3.614855398774147
LOSS train 3.614855398774147 valid 3.6228158473968506
EPOCH 77:
  batch 1000 loss: 3.6172911833524704
LOSS train 3.6172911833524704 valid 3.643101930618286
EPOCH 78:
  batch 1000 loss: 3.5999214208126067
LOSS train 3.5999214208126067 valid 3.5973119735717773
EPOCH 79:
  batch 1000 loss: 3.590188322544098
LOSS train 3.590188322544098 valid 3.643824577331543
EPOCH 80:
  batch 1000 loss: 3.611785061597824
LOSS train 3.611785061597824 valid 3.6124637126922607
EPOCH 81:
  batch 1000 loss: 3.614068650960922
LOSS train 3.614068650960922 valid 3.615351438522339
EPOCH 82:
  batch 1000 loss: 3.6038067729473116
LOSS train 3.6038067729473116 valid 3.602158546447754
EPOCH 83:
  batch 1000 loss: 3.6047478755712508
LOSS train 3.6047478755712508 valid 3.6595137119293213
EPOCH 84:
  batch 1000 loss: 3.60574759721756
Epoch 00084: reducing learning rate of group 0 to 5.0000e-04.
LOSS train 3.60574759721756 valid 3.6247105598449707
EPOCH 85:
  batch 1000 loss: 3.5345509301424025
LOSS train 3.5345509301424025 valid 3.5536723136901855
EPOCH 86:
  batch 1000 loss: 3.5386277570724487
LOSS train 3.5386277570724487 valid 3.550922155380249
EPOCH 87:
  batch 1000 loss: 3.530643111228943
LOSS train 3.530643111228943 valid 3.5610480308532715
EPOCH 88:
  batch 1000 loss: 3.513060051560402
LOSS train 3.513060051560402 valid 3.552619695663452
EPOCH 89:
  batch 1000 loss: 3.526299792766571
LOSS train 3.526299792766571 valid 3.555316686630249
EPOCH 90:
  batch 1000 loss: 3.5121262810230256
LOSS train 3.5121262810230256 valid 3.5738375186920166
EPOCH 91:
  batch 1000 loss: 3.538203749537468
LOSS train 3.538203749537468 valid 3.5414247512817383
EPOCH 92:
  batch 1000 loss: 3.5337103762626647
LOSS train 3.5337103762626647 valid 3.574924945831299
EPOCH 93:
  batch 1000 loss: 3.5153551191091537
LOSS train 3.5153551191091537 valid 3.551403522491455
EPOCH 94:
  batch 1000 loss: 3.536121648430824
LOSS train 3.536121648430824 valid 3.555354118347168
EPOCH 95:
  batch 1000 loss: 3.5301022839546206
LOSS train 3.5301022839546206 valid 3.5536627769470215
EPOCH 96:
  batch 1000 loss: 3.539244983911514
LOSS train 3.539244983911514 valid 3.5635826587677
EPOCH 97:
  batch 1000 loss: 3.5292112927436827
Epoch 00097: reducing learning rate of group 0 to 2.5000e-04.
LOSS train 3.5292112927436827 valid 3.5547754764556885
EPOCH 98:
  batch 1000 loss: 3.527162063956261
LOSS train 3.527162063956261 valid 3.5272974967956543
EPOCH 99:
  batch 1000 loss: 3.505067066550255
LOSS train 3.505067066550255 valid 3.5295958518981934
EPOCH 100:
  batch 1000 loss: 3.523376252412796
LOSS train 3.523376252412796 valid 3.5276882648468018
EPOCH 101:
  batch 1000 loss: 3.5071970049142838
LOSS train 3.5071970049142838 valid 3.5178275108337402
EPOCH 102:
  batch 1000 loss: 3.5044324864149092
LOSS train 3.5044324864149092 valid 3.513948440551758
EPOCH 103:
  batch 1000 loss: 3.4865669131278993
LOSS train 3.4865669131278993 valid 3.519385814666748
EPOCH 104:
  batch 1000 loss: 3.5002909824848176
LOSS train 3.5002909824848176 valid 3.5244593620300293
EPOCH 105:
  batch 1000 loss: 3.4963454328775407
LOSS train 3.4963454328775407 valid 3.5308151245117188
EPOCH 106:
  batch 1000 loss: 3.4883903024196625
LOSS train 3.4883903024196625 valid 3.529101610183716
EPOCH 107:
  batch 1000 loss: 3.494144495487213
LOSS train 3.494144495487213 valid 3.5242671966552734
EPOCH 108:
  batch 1000 loss: 3.5164287585020064
Epoch 00108: reducing learning rate of group 0 to 1.2500e-04.
LOSS train 3.5164287585020064 valid 3.517665386199951
EPOCH 109:
  batch 1000 loss: 3.4957689681053163
LOSS train 3.4957689681053163 valid 3.5093929767608643
EPOCH 110:
  batch 1000 loss: 3.4911227338314057
LOSS train 3.4911227338314057 valid 3.506171703338623
EPOCH 111:
  batch 1000 loss: 3.461997387647629
LOSS train 3.461997387647629 valid 3.5155909061431885
EPOCH 112:
  batch 1000 loss: 3.4799418321847915
LOSS train 3.4799418321847915 valid 3.5038774013519287
EPOCH 113:
  batch 1000 loss: 3.5065727870464327
LOSS train 3.5065727870464327 valid 3.5027599334716797
EPOCH 114:
  batch 1000 loss: 3.474125627040863
LOSS train 3.474125627040863 valid 3.4990692138671875
EPOCH 115:
  batch 1000 loss: 3.492424302577972
LOSS train 3.492424302577972 valid 3.5021111965179443
EPOCH 116:
  batch 1000 loss: 3.4945679788589477
LOSS train 3.4945679788589477 valid 3.5005075931549072
EPOCH 117:
  batch 1000 loss: 3.473874385595322
LOSS train 3.473874385595322 valid 3.502885103225708
EPOCH 118:
  batch 1000 loss: 3.482933896303177
LOSS train 3.482933896303177 valid 3.497424602508545
EPOCH 119:
  batch 1000 loss: 3.470023580789566
LOSS train 3.470023580789566 valid 3.49796986579895
EPOCH 120:
  batch 1000 loss: 3.461020628929138
LOSS train 3.461020628929138 valid 3.494016170501709
EPOCH 121:
  batch 1000 loss: 3.486693274617195
LOSS train 3.486693274617195 valid 3.4963486194610596
EPOCH 122:
  batch 1000 loss: 3.4744765079021454
LOSS train 3.4744765079021454 valid 3.493384838104248
EPOCH 123:
  batch 1000 loss: 3.4554335284233093
LOSS train 3.4554335284233093 valid 3.4942641258239746
EPOCH 124:
  batch 1000 loss: 3.464742083787918
LOSS train 3.464742083787918 valid 3.494723081588745
EPOCH 125:
  batch 1000 loss: 3.475441864848137
LOSS train 3.475441864848137 valid 3.4922196865081787
EPOCH 126:
  batch 1000 loss: 3.492158412694931
LOSS train 3.492158412694931 valid 3.4929072856903076
EPOCH 127:
  batch 1000 loss: 3.4556556180715563
LOSS train 3.4556556180715563 valid 3.492151975631714
EPOCH 128:
  batch 1000 loss: 3.469840607881546
LOSS train 3.469840607881546 valid 3.4871373176574707
EPOCH 129:
  batch 1000 loss: 3.471967625379562
LOSS train 3.471967625379562 valid 3.4870424270629883
EPOCH 130:
  batch 1000 loss: 3.472891749858856
LOSS train 3.472891749858856 valid 3.4852843284606934
EPOCH 131:
  batch 1000 loss: 3.455394292831421
LOSS train 3.455394292831421 valid 3.4877994060516357
EPOCH 132:
  batch 1000 loss: 3.4836272975206377
LOSS train 3.4836272975206377 valid 3.4820499420166016
EPOCH 133:
  batch 1000 loss: 3.4766424124240873
LOSS train 3.4766424124240873 valid 3.4854533672332764
EPOCH 134:
  batch 1000 loss: 3.455855802178383
LOSS train 3.455855802178383 valid 3.479907989501953
EPOCH 135:
  batch 1000 loss: 3.448658839225769
LOSS train 3.448658839225769 valid 3.4808132648468018
EPOCH 136:
  batch 1000 loss: 3.4633178424835207
LOSS train 3.4633178424835207 valid 3.4836924076080322
EPOCH 137:
  batch 1000 loss: 3.4689456871747972
LOSS train 3.4689456871747972 valid 3.482820749282837
EPOCH 138:
  batch 1000 loss: 3.4457251899242403
LOSS train 3.4457251899242403 valid 3.4754316806793213
EPOCH 139:
  batch 1000 loss: 3.4524774055480956
LOSS train 3.4524774055480956 valid 3.47379207611084
EPOCH 140:
  batch 1000 loss: 3.464065430164337
LOSS train 3.464065430164337 valid 3.478747606277466
EPOCH 141:
  batch 1000 loss: 3.434356717348099
LOSS train 3.434356717348099 valid 3.4719278812408447
EPOCH 142:
  batch 1000 loss: 3.4607909017801286
LOSS train 3.4607909017801286 valid 3.4722487926483154
EPOCH 143:
  batch 1000 loss: 3.455094743490219
LOSS train 3.455094743490219 valid 3.471743583679199
EPOCH 144:
  batch 1000 loss: 3.431060256242752
LOSS train 3.431060256242752 valid 3.4728240966796875
EPOCH 145:
  batch 1000 loss: 3.4485138165950775
LOSS train 3.4485138165950775 valid 3.471365451812744
EPOCH 146:
  batch 1000 loss: 3.461757171154022
LOSS train 3.461757171154022 valid 3.4764537811279297
EPOCH 147:
  batch 1000 loss: 3.452416281580925
LOSS train 3.452416281580925 valid 3.4703333377838135
EPOCH 148:
  batch 1000 loss: 3.436850788831711
LOSS train 3.436850788831711 valid 3.4686951637268066
EPOCH 149:
  batch 1000 loss: 3.4475345985889434
LOSS train 3.4475345985889434 valid 3.46781063079834
EPOCH 150:
  batch 1000 loss: 3.4402614390850066
LOSS train 3.4402614390850066 valid 3.468601942062378
EPOCH 151:
  batch 1000 loss: 3.435438000679016
LOSS train 3.435438000679016 valid 3.470503568649292
EPOCH 152:
  batch 1000 loss: 3.445119384288788
LOSS train 3.445119384288788 valid 3.4695589542388916
EPOCH 153:
  batch 1000 loss: 3.4332109932899475
LOSS train 3.4332109932899475 valid 3.471796751022339
EPOCH 154:
  batch 1000 loss: 3.430244411468506
LOSS train 3.430244411468506 valid 3.4682488441467285
EPOCH 155:
  batch 1000 loss: 3.457096625804901
Epoch 00155: reducing learning rate of group 0 to 6.2500e-05.
LOSS train 3.457096625804901 valid 3.468858003616333
EPOCH 156:
  batch 1000 loss: 3.4451838855743406
LOSS train 3.4451838855743406 valid 3.4625308513641357
EPOCH 157:
  batch 1000 loss: 3.44073282122612
LOSS train 3.44073282122612 valid 3.4621634483337402
EPOCH 158:
  batch 1000 loss: 3.432185463666916
LOSS train 3.432185463666916 valid 3.460618019104004
EPOCH 159:
  batch 1000 loss: 3.435114234447479
LOSS train 3.435114234447479 valid 3.4608309268951416
EPOCH 160:
  batch 1000 loss: 3.4318240002393723
LOSS train 3.4318240002393723 valid 3.4617111682891846
EPOCH 161:
  batch 1000 loss: 3.4277101995944976
LOSS train 3.4277101995944976 valid 3.4607789516448975
EPOCH 162:
  batch 1000 loss: 3.4333599408864974
LOSS train 3.4333599408864974 valid 3.458085536956787
EPOCH 163:
  batch 1000 loss: 3.4297978637218476
LOSS train 3.4297978637218476 valid 3.4573276042938232
EPOCH 164:
  batch 1000 loss: 3.430209581375122
LOSS train 3.430209581375122 valid 3.4574384689331055
EPOCH 165:
  batch 1000 loss: 3.4237764220237734
LOSS train 3.4237764220237734 valid 3.4584388732910156
EPOCH 166:
  batch 1000 loss: 3.446220244407654
LOSS train 3.446220244407654 valid 3.4577624797821045
EPOCH 167:
  batch 1000 loss: 3.4397049922943115
LOSS train 3.4397049922943115 valid 3.455752372741699
EPOCH 168:
  batch 1000 loss: 3.4373803532123564
LOSS train 3.4373803532123564 valid 3.4574074745178223
EPOCH 169:
  batch 1000 loss: 3.423631497144699
LOSS train 3.423631497144699 valid 3.4567291736602783
EPOCH 170:
  batch 1000 loss: 3.4201161296367646
LOSS train 3.4201161296367646 valid 3.4559099674224854
EPOCH 171:
  batch 1000 loss: 3.426706718325615
LOSS train 3.426706718325615 valid 3.453761100769043
EPOCH 172:
  batch 1000 loss: 3.420021855354309
LOSS train 3.420021855354309 valid 3.453855037689209
EPOCH 173:
  batch 1000 loss: 3.443764791607857
LOSS train 3.443764791607857 valid 3.4564552307128906
EPOCH 174:
  batch 1000 loss: 3.439222767353058
LOSS train 3.439222767353058 valid 3.454281806945801
EPOCH 175:
  batch 1000 loss: 3.4348579527139664
LOSS train 3.4348579527139664 valid 3.456869125366211
EPOCH 176:
  batch 1000 loss: 3.435661932468414
LOSS train 3.435661932468414 valid 3.4535531997680664
EPOCH 177:
  batch 1000 loss: 3.4473042622804644
LOSS train 3.4473042622804644 valid 3.4533727169036865
EPOCH 178:
  batch 1000 loss: 3.4333986023664473
LOSS train 3.4333986023664473 valid 3.4523138999938965
EPOCH 179:
  batch 1000 loss: 3.4231560777425765
LOSS train 3.4231560777425765 valid 3.451354503631592
EPOCH 180:
  batch 1000 loss: 3.4244287824630737
LOSS train 3.4244287824630737 valid 3.4520368576049805
EPOCH 181:
  batch 1000 loss: 3.4197582194805145
LOSS train 3.4197582194805145 valid 3.4504244327545166
EPOCH 182:
  batch 1000 loss: 3.421542323946953
LOSS train 3.421542323946953 valid 3.4524784088134766
EPOCH 183:
  batch 1000 loss: 3.426171401500702
LOSS train 3.426171401500702 valid 3.4509475231170654
EPOCH 184:
  batch 1000 loss: 3.4298035559654236
LOSS train 3.4298035559654236 valid 3.450895309448242
EPOCH 185:
  batch 1000 loss: 3.4295879673957823
LOSS train 3.4295879673957823 valid 3.450511932373047
EPOCH 186:
  batch 1000 loss: 3.4342195930480957
LOSS train 3.4342195930480957 valid 3.451815605163574
EPOCH 187:
  batch 1000 loss: 3.4428134425878523
Epoch 00187: reducing learning rate of group 0 to 3.1250e-05.
LOSS train 3.4428134425878523 valid 3.453645944595337
EPOCH 188:
  batch 1000 loss: 3.4317938320636747
LOSS train 3.4317938320636747 valid 3.4505014419555664
EPOCH 189:
  batch 1000 loss: 3.425133087992668
LOSS train 3.425133087992668 valid 3.4487838745117188
EPOCH 190:
  batch 1000 loss: 3.41373713696003
LOSS train 3.41373713696003 valid 3.450103282928467
EPOCH 191:
  batch 1000 loss: 3.4345696153640746
LOSS train 3.4345696153640746 valid 3.4490818977355957
EPOCH 192:
  batch 1000 loss: 3.425827454209328
LOSS train 3.425827454209328 valid 3.44852352142334
EPOCH 193:
  batch 1000 loss: 3.3951732280254365
LOSS train 3.3951732280254365 valid 3.4487881660461426
EPOCH 194:
  batch 1000 loss: 3.4311937886476516
LOSS train 3.4311937886476516 valid 3.4506285190582275
EPOCH 195:
  batch 1000 loss: 3.40326802611351
LOSS train 3.40326802611351 valid 3.446774482727051
EPOCH 196:
  batch 1000 loss: 3.4111105160713198
LOSS train 3.4111105160713198 valid 3.4481749534606934
EPOCH 197:
  batch 1000 loss: 3.433177084326744
LOSS train 3.433177084326744 valid 3.449268341064453
EPOCH 198:
  batch 1000 loss: 3.426055184721947
LOSS train 3.426055184721947 valid 3.447068691253662
EPOCH 199:
  batch 1000 loss: 3.414289310336113
LOSS train 3.414289310336113 valid 3.4467687606811523
EPOCH 200:
  batch 1000 loss: 3.415769578695297
LOSS train 3.415769578695297 valid 3.447866678237915
EPOCH 201:
  batch 1000 loss: 3.4249277229309083
Epoch 00201: reducing learning rate of group 0 to 1.5625e-05.
LOSS train 3.4249277229309083 valid 3.4485151767730713
EPOCH 202:
  batch 1000 loss: 3.420625280380249
LOSS train 3.420625280380249 valid 3.4468841552734375
EPOCH 203:
  batch 1000 loss: 3.412623375415802
LOSS train 3.412623375415802 valid 3.4470367431640625
EPOCH 204:
  batch 1000 loss: 3.432106142878532
LOSS train 3.432106142878532 valid 3.4472692012786865
EPOCH 205:
  batch 1000 loss: 3.4199275434017182
LOSS train 3.4199275434017182 valid 3.444678544998169
EPOCH 206:
  batch 1000 loss: 3.41552154815197
LOSS train 3.41552154815197 valid 3.445993423461914
EPOCH 207:
  batch 1000 loss: 3.425121586561203
LOSS train 3.425121586561203 valid 3.446180820465088
EPOCH 208:
  batch 1000 loss: 3.4145709960460664
LOSS train 3.4145709960460664 valid 3.444648027420044
EPOCH 209:
  batch 1000 loss: 3.425463984847069
LOSS train 3.425463984847069 valid 3.4461328983306885
EPOCH 210:
  batch 1000 loss: 3.433076778769493
LOSS train 3.433076778769493 valid 3.4468326568603516
EPOCH 211:
  batch 1000 loss: 3.4108233686685563
Epoch 00211: reducing learning rate of group 0 to 7.8125e-06.
LOSS train 3.4108233686685563 valid 3.444948196411133
EPOCH 212:
  batch 1000 loss: 3.4181390612125395
LOSS train 3.4181390612125395 valid 3.4448938369750977
EPOCH 213:
  batch 1000 loss: 3.4147161058187483
LOSS train 3.4147161058187483 valid 3.444962978363037
EPOCH 214:
  batch 1000 loss: 3.400938616514206
LOSS train 3.400938616514206 valid 3.4457695484161377
EPOCH 215:
  batch 1000 loss: 3.3996110298633577
LOSS train 3.3996110298633577 valid 3.4460060596466064
EPOCH 216:
  batch 1000 loss: 3.4300587491989134
LOSS train 3.4300587491989134 valid 3.4451096057891846
EPOCH 217:
  batch 1000 loss: 3.410433489561081
Epoch 00217: reducing learning rate of group 0 to 3.9063e-06.
LOSS train 3.410433489561081 valid 3.4459402561187744
EPOCH 218:
  batch 1000 loss: 3.4226088703870774
LOSS train 3.4226088703870774 valid 3.4450745582580566
EPOCH 219:
  batch 1000 loss: 3.407822762012482
LOSS train 3.407822762012482 valid 3.444521427154541
EPOCH 220:
  batch 1000 loss: 3.411813141822815
LOSS train 3.411813141822815 valid 3.444427013397217
EPOCH 221:
  batch 1000 loss: 3.4131007195711134
LOSS train 3.4131007195711134 valid 3.444441795349121
EPOCH 222:
  batch 1000 loss: 3.426182829260826
LOSS train 3.426182829260826 valid 3.4446256160736084
EPOCH 223:
  batch 1000 loss: 3.4336772646903992
LOSS train 3.4336772646903992 valid 3.443925142288208
EPOCH 224:
  batch 1000 loss: 3.4180737524032594
LOSS train 3.4180737524032594 valid 3.443481206893921
EPOCH 225:
  batch 1000 loss: 3.4102802386283875
LOSS train 3.4102802386283875 valid 3.4429099559783936
EPOCH 226:
  batch 1000 loss: 3.4021983077526095
LOSS train 3.4021983077526095 valid 3.445075750350952
EPOCH 227:
  batch 1000 loss: 3.4126407525539397
LOSS train 3.4126407525539397 valid 3.444512367248535
EPOCH 228:
  batch 1000 loss: 3.402866866827011
LOSS train 3.402866866827011 valid 3.4433281421661377
EPOCH 229:
  batch 1000 loss: 3.4266516600847243
LOSS train 3.4266516600847243 valid 3.4440698623657227
EPOCH 230:
  batch 1000 loss: 3.402377194404602
LOSS train 3.402377194404602 valid 3.444429874420166
EPOCH 231:
  batch 1000 loss: 3.4401162102222442
Epoch 00231: reducing learning rate of group 0 to 1.9531e-06.
LOSS train 3.4401162102222442 valid 3.444338083267212
EPOCH 232:
  batch 1000 loss: 3.3964253656864165

nohup: ignoring input
EPOCH 1:
  batch 1000 loss: 6.9601011853218075
LOSS train 6.9601011853218075 valid 5.651647567749023
EPOCH 2:
  batch 1000 loss: 5.439168417215347
LOSS train 5.439168417215347 valid 5.287853240966797
EPOCH 3:
  batch 1000 loss: 5.225644144296646
LOSS train 5.225644144296646 valid 5.165949821472168
EPOCH 4:
  batch 1000 loss: 5.133384460687638
LOSS train 5.133384460687638 valid 5.066787242889404
EPOCH 5:
  batch 1000 loss: 4.989818743944168
LOSS train 4.989818743944168 valid 4.951977729797363
EPOCH 6:
  batch 1000 loss: 4.915482998847962
LOSS train 4.915482998847962 valid 4.904392242431641
EPOCH 7:
  batch 1000 loss: 4.803392642736435
LOSS train 4.803392642736435 valid 4.773505210876465
EPOCH 8:
  batch 1000 loss: 4.690932014465332
LOSS train 4.690932014465332 valid 4.451822280883789
EPOCH 9:
  batch 1000 loss: 4.339106412887573
LOSS train 4.339106412887573 valid 4.2924065589904785
EPOCH 10:
  batch 1000 loss: 4.218680140972137
LOSS train 4.218680140972137 valid 4.171230792999268
EPOCH 11:
  batch 1000 loss: 4.11883903169632
LOSS train 4.11883903169632 valid 4.18109130859375
EPOCH 12:
  batch 1000 loss: 4.074292697191239
LOSS train 4.074292697191239 valid 4.034206867218018
EPOCH 13:
  batch 1000 loss: 4.020161187887192
LOSS train 4.020161187887192 valid 4.030468940734863
EPOCH 14:
  batch 1000 loss: 4.008065075874328
LOSS train 4.008065075874328 valid 3.977461576461792
EPOCH 15:
  batch 1000 loss: 3.9372093431949615
LOSS train 3.9372093431949615 valid 3.954110860824585
EPOCH 16:
  batch 1000 loss: 3.8973454661369322
LOSS train 3.8973454661369322 valid 3.9063949584960938
EPOCH 17:
  batch 1000 loss: 3.8706371684074403
LOSS train 3.8706371684074403 valid 3.930180549621582
EPOCH 18:
  batch 1000 loss: 3.853361129999161
LOSS train 3.853361129999161 valid 3.881787061691284
EPOCH 19:
  batch 1000 loss: 3.8483340413570404
LOSS train 3.8483340413570404 valid 3.852558135986328
EPOCH 20:
  batch 1000 loss: 3.808293225765228
LOSS train 3.808293225765228 valid 3.8662679195404053
EPOCH 21:
  batch 1000 loss: 3.8212922034263612
LOSS train 3.8212922034263612 valid 3.890840530395508
EPOCH 22:
  batch 1000 loss: 3.814745803117752
LOSS train 3.814745803117752 valid 3.8335390090942383
EPOCH 23:
  batch 1000 loss: 3.8060247621536254
LOSS train 3.8060247621536254 valid 3.820197343826294
EPOCH 24:
  batch 1000 loss: 3.7862857604026794
LOSS train 3.7862857604026794 valid 3.7887933254241943
EPOCH 25:
  batch 1000 loss: 3.74335409116745
LOSS train 3.74335409116745 valid 3.8857977390289307
EPOCH 26:
  batch 1000 loss: 3.7504894568920135
LOSS train 3.7504894568920135 valid 3.763038158416748
EPOCH 27:
  batch 1000 loss: 3.758169812440872
LOSS train 3.758169812440872 valid 3.810847520828247
EPOCH 28:
  batch 1000 loss: 3.777697252988815
LOSS train 3.777697252988815 valid 3.730980634689331
EPOCH 29:
  batch 1000 loss: 3.6992912278175356
LOSS train 3.6992912278175356 valid 3.813619375228882
EPOCH 30:
  batch 1000 loss: 3.706312467813492
LOSS train 3.706312467813492 valid 3.7399396896362305
EPOCH 31:
  batch 1000 loss: 3.7041865684986113
LOSS train 3.7041865684986113 valid 3.726961612701416
EPOCH 32:
  batch 1000 loss: 3.7146519520282744
LOSS train 3.7146519520282744 valid 3.7224676609039307
EPOCH 33:
  batch 1000 loss: 3.701965542793274
LOSS train 3.701965542793274 valid 3.7352406978607178
EPOCH 34:
  batch 1000 loss: 3.7178391778469084
LOSS train 3.7178391778469084 valid 3.7285633087158203
EPOCH 35:
  batch 1000 loss: 3.7242489371299743
LOSS train 3.7242489371299743 valid 3.864297866821289
EPOCH 36:
  batch 1000 loss: 3.6799472398757933
LOSS train 3.6799472398757933 valid 3.7203385829925537
EPOCH 37:
  batch 1000 loss: 3.6749528765678408
LOSS train 3.6749528765678408 valid 3.7096431255340576
EPOCH 38:
  batch 1000 loss: 3.6985099160671235
LOSS train 3.6985099160671235 valid 3.7246038913726807
EPOCH 39:
  batch 1000 loss: 3.671895374774933
LOSS train 3.671895374774933 valid 3.7715821266174316
EPOCH 40:
  batch 1000 loss: 3.653494481086731
LOSS train 3.653494481086731 valid 3.747192621231079
EPOCH 41:
  batch 1000 loss: 3.6597433750629427
LOSS train 3.6597433750629427 valid 3.7002317905426025
EPOCH 42:
  batch 1000 loss: 3.6715488831996916
LOSS train 3.6715488831996916 valid 3.6728241443634033
EPOCH 43:
  batch 1000 loss: 3.6498341505527496
LOSS train 3.6498341505527496 valid 3.7135298252105713
EPOCH 44:
  batch 1000 loss: 3.676266773700714
LOSS train 3.676266773700714 valid 3.7843523025512695
EPOCH 45:
  batch 1000 loss: 3.6554982204437256
LOSS train 3.6554982204437256 valid 3.674314498901367
EPOCH 46:
  batch 1000 loss: 3.6387628698349
LOSS train 3.6387628698349 valid 3.6997592449188232
EPOCH 47:
  batch 1000 loss: 3.6476330978870393
LOSS train 3.6476330978870393 valid 3.756260395050049
EPOCH 48:
  batch 1000 loss: 3.6788651015758513
Epoch 00048: reducing learning rate of group 0 to 5.0000e-04.
LOSS train 3.6788651015758513 valid 3.692129135131836
EPOCH 49:
  batch 1000 loss: 3.597733160495758
LOSS train 3.597733160495758 valid 3.612900972366333
EPOCH 50:
  batch 1000 loss: 3.583861126422882
LOSS train 3.583861126422882 valid 3.609657049179077
EPOCH 51:
  batch 1000 loss: 3.5851471452713013
LOSS train 3.5851471452713013 valid 3.6219022274017334
EPOCH 52:
  batch 1000 loss: 3.577431322813034
LOSS train 3.577431322813034 valid 3.59470534324646
EPOCH 53:
  batch 1000 loss: 3.572752527475357
LOSS train 3.572752527475357 valid 3.6154911518096924
EPOCH 54:
  batch 1000 loss: 3.546922619342804
LOSS train 3.546922619342804 valid 3.5763022899627686
EPOCH 55:
  batch 1000 loss: 3.547475536108017
LOSS train 3.547475536108017 valid 3.5897693634033203
EPOCH 56:
  batch 1000 loss: 3.567939172029495
LOSS train 3.567939172029495 valid 3.577145576477051
EPOCH 57:
  batch 1000 loss: 3.5402581102848054
LOSS train 3.5402581102848054 valid 3.5695743560791016
EPOCH 58:
  batch 1000 loss: 3.5487322878837584
LOSS train 3.5487322878837584 valid 3.582533836364746
EPOCH 59:
  batch 1000 loss: 3.545114363193512
LOSS train 3.545114363193512 valid 3.5722663402557373
EPOCH 60:
  batch 1000 loss: 3.561111071586609
LOSS train 3.561111071586609 valid 3.5815060138702393
EPOCH 61:
  batch 1000 loss: 3.579754384994507
LOSS train 3.579754384994507 valid 3.5656044483184814
EPOCH 62:
  batch 1000 loss: 3.5513687038421633
LOSS train 3.5513687038421633 valid 3.607060194015503
EPOCH 63:
  batch 1000 loss: 3.5422677326202394
LOSS train 3.5422677326202394 valid 3.5730321407318115
EPOCH 64:
  batch 1000 loss: 3.546236249923706
LOSS train 3.546236249923706 valid 3.5622029304504395
EPOCH 65:
  batch 1000 loss: 3.561969598054886
LOSS train 3.561969598054886 valid 3.5723743438720703
EPOCH 66:
  batch 1000 loss: 3.533786835670471
LOSS train 3.533786835670471 valid 3.5704238414764404
EPOCH 67:
  batch 1000 loss: 3.5384334540367126
LOSS train 3.5384334540367126 valid 3.5680088996887207
EPOCH 68:
  batch 1000 loss: 3.536177839279175
LOSS train 3.536177839279175 valid 3.560351848602295
EPOCH 69:
  batch 1000 loss: 3.534167162656784
LOSS train 3.534167162656784 valid 3.5637009143829346
EPOCH 70:
  batch 1000 loss: 3.557401580810547
LOSS train 3.557401580810547 valid 3.5706050395965576
EPOCH 71:
  batch 1000 loss: 3.538649673700333
LOSS train 3.538649673700333 valid 3.5550124645233154
EPOCH 72:
  batch 1000 loss: 3.5364807660579682
LOSS train 3.5364807660579682 valid 3.5530357360839844
EPOCH 73:
  batch 1000 loss: 3.5312961032390593
LOSS train 3.5312961032390593 valid 3.560973882675171
EPOCH 74:
  batch 1000 loss: 3.5283312668800355
LOSS train 3.5283312668800355 valid 3.5682947635650635
EPOCH 75:
  batch 1000 loss: 3.5257935700416563
LOSS train 3.5257935700416563 valid 3.5622177124023438
EPOCH 76:
  batch 1000 loss: 3.551179237604141
LOSS train 3.551179237604141 valid 3.557100772857666
EPOCH 77:
  batch 1000 loss: 3.5355940852165224
LOSS train 3.5355940852165224 valid 3.552459716796875
EPOCH 78:
  batch 1000 loss: 3.540469804763794
LOSS train 3.540469804763794 valid 3.5628859996795654
EPOCH 79:
  batch 1000 loss: 3.527489019870758
LOSS train 3.527489019870758 valid 3.590160608291626
EPOCH 80:
  batch 1000 loss: 3.5373156406879427
LOSS train 3.5373156406879427 valid 3.570746898651123
EPOCH 81:
  batch 1000 loss: 3.534035738945007
LOSS train 3.534035738945007 valid 3.5900461673736572
EPOCH 82:
  batch 1000 loss: 3.5446429286003114
LOSS train 3.5446429286003114 valid 3.5541698932647705
EPOCH 83:
  batch 1000 loss: 3.5352596480846405
LOSS train 3.5352596480846405 valid 3.547027349472046
EPOCH 84:
  batch 1000 loss: 3.5257123036384583
LOSS train 3.5257123036384583 valid 3.5590579509735107
EPOCH 85:
  batch 1000 loss: 3.5331024260520936
LOSS train 3.5331024260520936 valid 3.5910403728485107
EPOCH 86:
  batch 1000 loss: 3.496598936319351
LOSS train 3.496598936319351 valid 3.549302101135254
EPOCH 87:
  batch 1000 loss: 3.5392898235321044
LOSS train 3.5392898235321044 valid 3.567096471786499
EPOCH 88:
  batch 1000 loss: 3.5155574026107788
LOSS train 3.5155574026107788 valid 3.5470008850097656
EPOCH 89:
  batch 1000 loss: 3.515568006277084
LOSS train 3.515568006277084 valid 3.5449395179748535
EPOCH 90:
  batch 1000 loss: 3.5326783752441404
LOSS train 3.5326783752441404 valid 3.5442371368408203
EPOCH 91:
  batch 1000 loss: 3.5234622452259066
LOSS train 3.5234622452259066 valid 3.532337188720703
EPOCH 92:
  batch 1000 loss: 3.52869397521019
LOSS train 3.52869397521019 valid 3.5574049949645996
EPOCH 93:
  batch 1000 loss: 3.5129290058612823
LOSS train 3.5129290058612823 valid 3.563192129135132
EPOCH 94:
  batch 1000 loss: 3.5234595193862916
LOSS train 3.5234595193862916 valid 3.5487871170043945
EPOCH 95:
  batch 1000 loss: 3.4985226175785065
LOSS train 3.4985226175785065 valid 3.545189142227173
EPOCH 96:
  batch 1000 loss: 3.50251016497612
LOSS train 3.50251016497612 valid 3.5283496379852295
EPOCH 97:
  batch 1000 loss: 3.4918609104156495
LOSS train 3.4918609104156495 valid 3.542691707611084
EPOCH 98:
  batch 1000 loss: 3.5134634132385254
LOSS train 3.5134634132385254 valid 3.541848659515381
EPOCH 99:
  batch 1000 loss: 3.505761542081833
LOSS train 3.505761542081833 valid 3.553835153579712
EPOCH 100:
  batch 1000 loss: 3.507906580209732
LOSS train 3.507906580209732 valid 3.532076358795166
EPOCH 101:
  batch 1000 loss: 3.519483468770981
LOSS train 3.519483468770981 valid 3.550212860107422
EPOCH 102:
  batch 1000 loss: 3.510454514503479
Epoch 00102: reducing learning rate of group 0 to 2.5000e-04.
LOSS train 3.510454514503479 valid 3.530649423599243
EPOCH 103:
  batch 1000 loss: 3.471318221092224
LOSS train 3.471318221092224 valid 3.51308274269104
EPOCH 104:
  batch 1000 loss: 3.4662861309051514
LOSS train 3.4662861309051514 valid 3.5022106170654297
EPOCH 105:
  batch 1000 loss: 3.4822548282146455
LOSS train 3.4822548282146455 valid 3.505056142807007
EPOCH 106:
  batch 1000 loss: 3.466620176553726
LOSS train 3.466620176553726 valid 3.50887131690979
EPOCH 107:
  batch 1000 loss: 3.4832003920078276
LOSS train 3.4832003920078276 valid 3.5009729862213135
EPOCH 108:
  batch 1000 loss: 3.471177670955658
LOSS train 3.471177670955658 valid 3.4960663318634033
EPOCH 109:
  batch 1000 loss: 3.4704910585880278
LOSS train 3.4704910585880278 valid 3.4944941997528076
EPOCH 110:
  batch 1000 loss: 3.447059665441513
LOSS train 3.447059665441513 valid 3.4942233562469482
EPOCH 111:
  batch 1000 loss: 3.454551642894745
LOSS train 3.454551642894745 valid 3.4986050128936768
EPOCH 112:
  batch 1000 loss: 3.4533664045333863
LOSS train 3.4533664045333863 valid 3.489220380783081
EPOCH 113:
  batch 1000 loss: 3.4620987167358397
LOSS train 3.4620987167358397 valid 3.490342617034912
EPOCH 114:
  batch 1000 loss: 3.4547028827667234
LOSS train 3.4547028827667234 valid 3.4887735843658447
EPOCH 115:
  batch 1000 loss: 3.4566664280891417
LOSS train 3.4566664280891417 valid 3.4932632446289062
EPOCH 116:
  batch 1000 loss: 3.4672517557144165
LOSS train 3.4672517557144165 valid 3.5184547901153564
EPOCH 117:
  batch 1000 loss: 3.4572445573806765
LOSS train 3.4572445573806765 valid 3.511544704437256
EPOCH 118:
  batch 1000 loss: 3.469913092851639
LOSS train 3.469913092851639 valid 3.4935643672943115
EPOCH 119:
  batch 1000 loss: 3.4519319722652435
LOSS train 3.4519319722652435 valid 3.5022013187408447
EPOCH 120:
  batch 1000 loss: 3.4373280715942385
Epoch 00120: reducing learning rate of group 0 to 1.2500e-04.
LOSS train 3.4373280715942385 valid 3.4975409507751465
EPOCH 121:
  batch 1000 loss: 3.428104560136795
LOSS train 3.428104560136795 valid 3.478181838989258
EPOCH 122:
  batch 1000 loss: 3.4402694053649903
LOSS train 3.4402694053649903 valid 3.475219249725342
EPOCH 123:
  batch 1000 loss: 3.4295524535179136
LOSS train 3.4295524535179136 valid 3.4687376022338867
EPOCH 124:
  batch 1000 loss: 3.4326404638290406
LOSS train 3.4326404638290406 valid 3.468997001647949
EPOCH 125:
  batch 1000 loss: 3.432009220838547
LOSS train 3.432009220838547 valid 3.4733614921569824
EPOCH 126:
  batch 1000 loss: 3.4377209186553954
LOSS train 3.4377209186553954 valid 3.4735260009765625
EPOCH 127:
  batch 1000 loss: 3.4201933686733246
LOSS train 3.4201933686733246 valid 3.468388795852661
EPOCH 128:
  batch 1000 loss: 3.426392119884491
LOSS train 3.426392119884491 valid 3.4705560207366943
EPOCH 129:
  batch 1000 loss: 3.4406455554962156
LOSS train 3.4406455554962156 valid 3.470892906188965
EPOCH 130:
  batch 1000 loss: 3.4426804084777833
LOSS train 3.4426804084777833 valid 3.4696755409240723
EPOCH 131:
  batch 1000 loss: 3.4301839590072634
LOSS train 3.4301839590072634 valid 3.4704432487487793
EPOCH 132:
  batch 1000 loss: 3.4252965145111083
LOSS train 3.4252965145111083 valid 3.471560001373291
EPOCH 133:
  batch 1000 loss: 3.453375742673874
Epoch 00133: reducing learning rate of group 0 to 6.2500e-05.
LOSS train 3.453375742673874 valid 3.472290277481079
EPOCH 134:
  batch 1000 loss: 3.4212066831588746
LOSS train 3.4212066831588746 valid 3.4648244380950928
EPOCH 135:
  batch 1000 loss: 3.4126204023361204
LOSS train 3.4126204023361204 valid 3.4658167362213135
EPOCH 136:
  batch 1000 loss: 3.4321760668754577
LOSS train 3.4321760668754577 valid 3.4646286964416504
EPOCH 137:
  batch 1000 loss: 3.4247809433937073
LOSS train 3.4247809433937073 valid 3.465064764022827
EPOCH 138:
  batch 1000 loss: 3.4148061130046843
LOSS train 3.4148061130046843 valid 3.4656379222869873
EPOCH 139:
  batch 1000 loss: 3.423611870765686
LOSS train 3.423611870765686 valid 3.4618513584136963
EPOCH 140:
  batch 1000 loss: 3.408422509908676
LOSS train 3.408422509908676 valid 3.4613537788391113
EPOCH 141:
  batch 1000 loss: 3.425812148809433
LOSS train 3.425812148809433 valid 3.459980010986328
EPOCH 142:
  batch 1000 loss: 3.4237794358730316
LOSS train 3.4237794358730316 valid 3.4604568481445312
EPOCH 143:
  batch 1000 loss: 3.4224734380245208
LOSS train 3.4224734380245208 valid 3.4591360092163086
EPOCH 144:
  batch 1000 loss: 3.4367740581035613
LOSS train 3.4367740581035613 valid 3.4619033336639404
EPOCH 145:
  batch 1000 loss: 3.4211066229343414
LOSS train 3.4211066229343414 valid 3.460300922393799
EPOCH 146:
  batch 1000 loss: 3.415481580734253
LOSS train 3.415481580734253 valid 3.459930181503296
EPOCH 147:
  batch 1000 loss: 3.4183018021583558
LOSS train 3.4183018021583558 valid 3.4601969718933105
EPOCH 148:
  batch 1000 loss: 3.4388742294311525
LOSS train 3.4388742294311525 valid 3.460130214691162
EPOCH 149:
  batch 1000 loss: 3.431869033098221
Epoch 00149: reducing learning rate of group 0 to 3.1250e-05.
LOSS train 3.431869033098221 valid 3.4599008560180664
EPOCH 150:
  batch 1000 loss: 3.411205109834671
LOSS train 3.411205109834671 valid 3.45535945892334
EPOCH 151:
  batch 1000 loss: 3.416051185846329
LOSS train 3.416051185846329 valid 3.4559130668640137
EPOCH 152:
  batch 1000 loss: 3.403697444677353
LOSS train 3.403697444677353 valid 3.455811023712158
EPOCH 153:
  batch 1000 loss: 3.4199292883872987
LOSS train 3.4199292883872987 valid 3.4554033279418945
EPOCH 154:
  batch 1000 loss: 3.43153808426857
LOSS train 3.43153808426857 valid 3.4540600776672363
EPOCH 155:
  batch 1000 loss: 3.3940191006660463
LOSS train 3.3940191006660463 valid 3.45505952835083
EPOCH 156:
  batch 1000 loss: 3.409667763233185
LOSS train 3.409667763233185 valid 3.453387498855591
EPOCH 157:
  batch 1000 loss: 3.415137151956558
LOSS train 3.415137151956558 valid 3.453634262084961
EPOCH 158:
  batch 1000 loss: 3.4208838238716126
LOSS train 3.4208838238716126 valid 3.454119920730591
EPOCH 159:
  batch 1000 loss: 3.416182066202164
LOSS train 3.416182066202164 valid 3.4524457454681396
EPOCH 160:
  batch 1000 loss: 3.413069739818573
LOSS train 3.413069739818573 valid 3.4517996311187744
EPOCH 161:
  batch 1000 loss: 3.4139218606948853
LOSS train 3.4139218606948853 valid 3.451552152633667
EPOCH 162:
  batch 1000 loss: 3.426655079126358
LOSS train 3.426655079126358 valid 3.452784776687622
EPOCH 163:
  batch 1000 loss: 3.4053857803344725
LOSS train 3.4053857803344725 valid 3.4527177810668945
EPOCH 164:
  batch 1000 loss: 3.4288484120368956
LOSS train 3.4288484120368956 valid 3.451282501220703
EPOCH 165:
  batch 1000 loss: 3.4179770896434785
LOSS train 3.4179770896434785 valid 3.450852155685425
EPOCH 166:
  batch 1000 loss: 3.40065252327919
LOSS train 3.40065252327919 valid 3.4510955810546875
EPOCH 167:
  batch 1000 loss: 3.3958215086460113
LOSS train 3.3958215086460113 valid 3.4504611492156982
EPOCH 168:
  batch 1000 loss: 3.4177785797119142
LOSS train 3.4177785797119142 valid 3.4499313831329346
EPOCH 169:
  batch 1000 loss: 3.39874378657341
LOSS train 3.39874378657341 valid 3.4514594078063965
EPOCH 170:
  batch 1000 loss: 3.403888135910034
LOSS train 3.403888135910034 valid 3.4499449729919434
EPOCH 171:
  batch 1000 loss: 3.4142754492759706
LOSS train 3.4142754492759706 valid 3.449615955352783
EPOCH 172:
  batch 1000 loss: 3.4094335451126097
LOSS train 3.4094335451126097 valid 3.4510338306427
EPOCH 173:
  batch 1000 loss: 3.4121130573749543
LOSS train 3.4121130573749543 valid 3.4494211673736572
EPOCH 174:
  batch 1000 loss: 3.404311445951462
LOSS train 3.404311445951462 valid 3.4496755599975586
EPOCH 175:
  batch 1000 loss: 3.408049947977066
LOSS train 3.408049947977066 valid 3.4494690895080566
EPOCH 176:
  batch 1000 loss: 3.4235129251480103
LOSS train 3.4235129251480103 valid 3.4487051963806152
EPOCH 177:
  batch 1000 loss: 3.3943124120235444
LOSS train 3.3943124120235444 valid 3.4484922885894775
EPOCH 178:
  batch 1000 loss: 3.409962384223938
LOSS train 3.409962384223938 valid 3.447946310043335
EPOCH 179:
  batch 1000 loss: 3.407732239961624
LOSS train 3.407732239961624 valid 3.4491119384765625
EPOCH 180:
  batch 1000 loss: 3.4162750086784364
LOSS train 3.4162750086784364 valid 3.4485976696014404
EPOCH 181:
  batch 1000 loss: 3.415881735563278
LOSS train 3.415881735563278 valid 3.4487876892089844
EPOCH 182:
  batch 1000 loss: 3.405370822429657
LOSS train 3.405370822429657 valid 3.4484245777130127
EPOCH 183:
  batch 1000 loss: 3.4170669901371
LOSS train 3.4170669901371 valid 3.44795560836792
EPOCH 184:
  batch 1000 loss: 3.4111313939094545
Epoch 00184: reducing learning rate of group 0 to 1.5625e-05.
LOSS train 3.4111313939094545 valid 3.448634386062622
EPOCH 185:
  batch 1000 loss: 3.4115277700424196
LOSS train 3.4115277700424196 valid 3.446836233139038
EPOCH 186:
  batch 1000 loss: 3.391689905166626
LOSS train 3.391689905166626 valid 3.4466283321380615
EPOCH 187:
  batch 1000 loss: 3.386412805557251
LOSS train 3.386412805557251 valid 3.445974826812744
EPOCH 188:
  batch 1000 loss: 3.4006710917949676
LOSS train 3.4006710917949676 valid 3.445662021636963
EPOCH 189:
  batch 1000 loss: 3.387974798440933
LOSS train 3.387974798440933 valid 3.4463677406311035
EPOCH 190:
  batch 1000 loss: 3.3953932147026062
LOSS train 3.3953932147026062 valid 3.446706533432007
EPOCH 191:
  batch 1000 loss: 3.413295403242111
LOSS train 3.413295403242111 valid 3.4470725059509277
EPOCH 192:
  batch 1000 loss: 3.4173347308635713
LOSS train 3.4173347308635713 valid 3.446565628051758
EPOCH 193:
  batch 1000 loss: 3.418022861003876
Epoch 00193: reducing learning rate of group 0 to 7.8125e-06.
LOSS train 3.418022861003876 valid 3.446986675262451
EPOCH 194:
  batch 1000 loss: 3.4005329496860504
LOSS train 3.4005329496860504 valid 3.4450125694274902
EPOCH 195:
  batch 1000 loss: 3.3931519317626955
LOSS train 3.3931519317626955 valid 3.4448471069335938
EPOCH 196:
  batch 1000 loss: 3.402602864265442
LOSS train 3.402602864265442 valid 3.4449594020843506
EPOCH 197:
  batch 1000 loss: 3.405655242204666
LOSS train 3.405655242204666 valid 3.444913148880005
EPOCH 198:
  batch 1000 loss: 3.3794588990211487
LOSS train 3.3794588990211487 valid 3.44504976272583
EPOCH 199:
  batch 1000 loss: 3.404373911857605
LOSS train 3.404373911857605 valid 3.444895029067993
EPOCH 200:
  batch 1000 loss: 3.420523754119873
Epoch 00200: reducing learning rate of group 0 to 3.9063e-06.
LOSS train 3.420523754119873 valid 3.44539737701416
EPOCH 201:
  batch 1000 loss: 3.4022410516738892
LOSS train 3.4022410516738892 valid 3.4447901248931885
EPOCH 202:
  batch 1000 loss: 3.389380091905594
LOSS train 3.389380091905594 valid 3.4444427490234375
EPOCH 203:
  batch 1000 loss: 3.3951658551692963
LOSS train 3.3951658551692963 valid 3.444641351699829
EPOCH 204:
  batch 1000 loss: 3.4099818847179413
LOSS train 3.4099818847179413 valid 3.4448583126068115
EPOCH 205:
  batch 1000 loss: 3.4161926245689394
LOSS train 3.4161926245689394 valid 3.4460878372192383
EPOCH 206:
  batch 1000 loss: 3.3874554893970488
LOSS train 3.3874554893970488 valid 3.444326400756836
EPOCH 207:
  batch 1000 loss: 3.4065719826221468
LOSS train 3.4065719826221468 valid 3.44504714012146
EPOCH 208:
  batch 1000 loss: 3.3782690525054933
Epoch 00208: reducing learning rate of group 0 to 1.9531e-06.
LOSS train 3.3782690525054933 valid 3.445493221282959
EPOCH 209:
  batch 1000 loss: 3.4013996620178224
LOSS train 3.4013996620178224 valid 3.4444420337677
EPOCH 210:
  batch 1000 loss: 3.4102230250835417
LOSS train 3.4102230250835417 valid 3.444739818572998
EPOCH 211:
  batch 1000 loss: 3.3984420890808105
LOSS train 3.3984420890808105 valid 3.4456522464752197
EPOCH 212:
  batch 1000 loss: 3.410922763347626
LOSS train 3.410922763347626 valid 3.4451770782470703
EPOCH 213:
  batch 1000 loss: 3.406193594455719
LOSS train 3.406193594455719 valid 3.444312810897827
EPOCH 214:
  batch 1000 loss: 3.420179010629654
Epoch 00214: reducing learning rate of group 0 to 9.7656e-07.
LOSS train 3.420179010629654 valid 3.444645881652832
EPOCH 215:
  batch 1000 loss: 3.3950071444511414
LOSS train 3.3950071444511414 valid 3.4454545974731445
EPOCH 216:
  batch 1000 loss: 3.424193972826004
LOSS train 3.424193972826004 valid 3.443979024887085
EPOCH 217:
  batch 1000 loss: 3.39508362030983
LOSS train 3.39508362030983 valid 3.446206569671631
EPOCH 218:
  batch 1000 loss: 3.4019159791469575
LOSS train 3.4019159791469575 valid 3.4444730281829834
EPOCH 219:
  batch 1000 loss: 3.411246140241623
LOSS train 3.411246140241623 valid 3.4437952041625977
EPOCH 220:
  batch 1000 loss: 3.381846610069275
LOSS train 3.381846610069275 valid 3.444870948791504
EPOCH 221:
  batch 1000 loss: 3.417744613170624
LOSS train 3.417744613170624 valid 3.4443435668945312
EPOCH 222:
  batch 1000 loss: 3.3949973032474516
Epoch 00222: reducing learning rate of group 0 to 4.8828e-07.
LOSS train 3.3949973032474516 valid 3.444345235824585
EPOCH 223:
  batch 1000 loss: 3.402667151451111
LOSS train 3.402667151451111 valid 3.4446487426757812
EPOCH 224:
  batch 1000 loss: 3.389953285217285
LOSS train 3.389953285217285 valid 3.444204568862915
EPOCH 225:
  batch 1000 loss: 3.403520782470703
LOSS train 3.403520782470703 valid 3.4445250034332275
EPOCH 226:
  batch 1000 loss: 3.407694610118866
LOSS train 3.407694610118866 valid 3.444577932357788
EPOCH 227:
  batch 1000 loss: 3.397596643924713
LOSS train 3.397596643924713 valid 3.444220781326294
EPOCH 228:
  batch 1000 loss: 3.4001395242214203
Epoch 00228: reducing learning rate of group 0 to 2.4414e-07.
LOSS train 3.4001395242214203 valid 3.444969654083252
EPOCH 229:
  batch 1000 loss: 3.4060006172657014
LOSS train 3.4060006172657014 valid 3.4443745613098145
EPOCH 230:
  batch 1000 loss: 3.4107784979343414
LOSS train 3.4107784979343414 valid 3.4443540573120117
EPOCH 231:
  batch 1000 loss: 3.421958956718445
LOSS train 3.421958956718445 valid 3.4452621936798096
EPOCH 232:
  batch 1000 loss: 3.4047984890937806
LOSS train 3.4047984890937806 valid 3.4453866481781006
EPOCH 233:
  batch 1000 loss: 3.388833881378174
LOSS train 3.388833881378174 valid 3.445207118988037
EPOCH 234:
  batch 1000 loss: 3.422084565639496
Epoch 00234: reducing learning rate of group 0 to 1.2207e-07.
LOSS train 3.422084565639496 valid 3.4444825649261475
EPOCH 235:
  batch 1000 loss: 3.4062429621219636
LOSS train 3.4062429621219636 valid 3.444096565246582
EPOCH 236:
  batch 1000 loss: 3.4073633365631104
LOSS train 3.4073633365631104 valid 3.4443883895874023
EPOCH 237:
  batch 1000 loss: 3.4112459897994993
LOSS train 3.4112459897994993 valid 3.4441277980804443
EPOCH 238:
  batch 1000 loss: 3.4193807284832003
LOSS train 3.4193807284832003 valid 3.443828582763672
EPOCH 239:
  batch 1000 loss: 3.3879715723991395
LOSS train 3.3879715723991395 valid 3.4445643424987793
EPOCH 240:
  batch 1000 loss: 3.393064921617508
Epoch 00240: reducing learning rate of group 0 to 6.1035e-08.
LOSS train 3.393064921617508 valid 3.444406032562256
EPOCH 241:
  batch 1000 loss: 3.4161261711120607
LOSS train 3.4161261711120607 valid 3.4457247257232666
EPOCH 242:
  batch 1000 loss: 3.413185079574585
LOSS train 3.413185079574585 valid 3.4450323581695557
EPOCH 243:
  batch 1000 loss: 3.408771286487579
LOSS train 3.408771286487579 valid 3.4446675777435303
EPOCH 244:
  batch 1000 loss: 3.41287042927742
LOSS train 3.41287042927742 valid 3.4446942806243896
EPOCH 245:
  batch 1000 loss: 3.391990894317627
LOSS train 3.391990894317627 valid 3.444518804550171
EPOCH 246:
  batch 1000 loss: 3.4100014328956605
Epoch 00246: reducing learning rate of group 0 to 3.0518e-08.
LOSS train 3.4100014328956605 valid 3.4449844360351562
EPOCH 247:
  batch 1000 loss: 3.3954501824378966
LOSS train 3.3954501824378966 valid 3.444627046585083
EPOCH 248:
  batch 1000 loss: 3.425383420705795
LOSS train 3.425383420705795 valid 3.4441747665405273
EPOCH 249:
  batch 1000 loss: 3.4187296352386474
LOSS train 3.4187296352386474 valid 3.4445254802703857
EPOCH 250:
  batch 1000 loss: 3.40317684841156
LOSS train 3.40317684841156 valid 3.445058822631836
EPOCH 251:
  batch 1000 loss: 3.393930439710617
LOSS train 3.393930439710617 valid 3.4440536499023438
EPOCH 252:
  batch 1000 loss: 3.404727672100067
Epoch 00252: reducing learning rate of group 0 to 1.5259e-08.
LOSS train 3.404727672100067 valid 3.4453017711639404
EPOCH 253:
  batch 1000 loss: 3.394602781057358
LOSS train 3.394602781057358 valid 3.445314645767212
EPOCH 254:
  batch 1000 loss: 3.4019045975208284
LOSS train 3.4019045975208284 valid 3.445106267929077
EPOCH 255:
  batch 1000 loss: 3.4072278509140013
LOSS train 3.4072278509140013 valid 3.444411277770996
EPOCH 256:
  batch 1000 loss: 3.3986770207881927
LOSS train 3.3986770207881927 valid 3.4447288513183594
EPOCH 257:
  batch 1000 loss: 3.3846183395385743
LOSS train 3.3846183395385743 valid 3.4443359375
EPOCH 258:
  batch 1000 loss: 3.3949678859710692
LOSS train 3.3949678859710692 valid 3.4450953006744385
EPOCH 259:
  batch 1000 loss: 3.4019929118156433
LOSS train 3.4019929118156433 valid 3.445120096206665
EPOCH 260:
  batch 1000 loss: 3.395170371770859
LOSS train 3.395170371770859 valid 3.4457390308380127
EPOCH 261:
  batch 1000 loss: 3.420044213294983
LOSS train 3.420044213294983 valid 3.4449596405029297
EPOCH 262:
  batch 1000 loss: 3.4121202561855317
LOSS train 3.4121202561855317 valid 3.4445860385894775
EPOCH 263:
  batch 1000 loss: 3.4067855086326597
LOSS train 3.4067855086326597 valid 3.444087028503418
EPOCH 264:
  batch 1000 loss: 3.4166493227481842
LOSS train 3.4166493227481842 valid 3.445176362991333
EPOCH 265:
  batch 1000 loss: 3.413688794374466
LOSS train 3.413688794374466 valid 3.4451470375061035
EPOCH 266:
  batch 1000 loss: 3.4045368099212645
LOSS train 3.4045368099212645 valid 3.4451088905334473
EPOCH 267:
  batch 1000 loss: 3.396645148277283
LOSS train 3.396645148277283 valid 3.4443140029907227
EPOCH 268:
  batch 1000 loss: 3.416354433298111
LOSS train 3.416354433298111 valid 3.445586681365967
EPOCH 269:
  batch 1000 loss: 3.4157307779788972
LOSS train 3.4157307779788972 valid 3.4445488452911377
EPOCH 270:
  batch 1000 loss: 3.416991680622101
LOSS train 3.416991680622101 valid 3.4444496631622314
EPOCH 271:
  batch 1000 loss: 3.420624847173691
LOSS train 3.420624847173691 valid 3.4443726539611816
EPOCH 272:
  batch 1000 loss: 3.3890293328762056
LOSS train 3.3890293328762056 valid 3.4438083171844482
EPOCH 273:
  batch 1000 loss: 3.391219785928726
LOSS train 3.391219785928726 valid 3.4444754123687744
EPOCH 274:
  batch 1000 loss: 3.4164284911155702
LOSS train 3.4164284911155702 valid 3.444209575653076
EPOCH 275:
  batch 1000 loss: 3.4074924969673157
LOSS train 3.4074924969673157 valid 3.4454376697540283
EPOCH 276:
  batch 1000 loss: 3.4060976293087006
LOSS train 3.4060976293087006 valid 3.4444258213043213
EPOCH 277:
  batch 1000 loss: 3.4244565687179565
LOSS train 3.4244565687179565 valid 3.445192575454712
EPOCH 278:
  batch 1000 loss: 3.3901463632583617
LOSS train 3.3901463632583617 valid 3.4444730281829834
EPOCH 279:
  batch 1000 loss: 3.400215476512909
LOSS train 3.400215476512909 valid 3.4440414905548096
EPOCH 280:
  batch 1000 loss: 3.4087556402683257
LOSS train 3.4087556402683257 valid 3.44492769241333
EPOCH 281:
  batch 1000 loss: 3.39554762339592
LOSS train 3.39554762339592 valid 3.445223331451416
EPOCH 282:
  batch 1000 loss: 3.4158719108104707
LOSS train 3.4158719108104707 valid 3.444126605987549
EPOCH 283:
  batch 1000 loss: 3.4035798568725584
LOSS train 3.4035798568725584 valid 3.4441909790039062
EPOCH 284:
  batch 1000 loss: 3.4007886142730714
LOSS train 3.4007886142730714 valid 3.445265054702759
EPOCH 285:
  batch 1000 loss: 3.4084076364040374
LOSS train 3.4084076364040374 valid 3.4440841674804688
EPOCH 286:
  batch 1000 loss: 3.406004713535309
LOSS train 3.406004713535309 valid 3.445133686065674
EPOCH 287:
  batch 1000 loss: 3.4089272108078004
LOSS train 3.4089272108078004 valid 3.443746328353882
EPOCH 288:
  batch 1000 loss: 3.406011100530624
LOSS train 3.406011100530624 valid 3.444088935852051
EPOCH 289:
  batch 1000 loss: 3.394408941745758
LOSS train 3.394408941745758 valid 3.444246292114258
EPOCH 290:
  batch 1000 loss: 3.391363589525223
LOSS train 3.391363589525223 valid 3.4461753368377686
EPOCH 291:
  batch 1000 loss: 3.3921894040107725
LOSS train 3.3921894040107725 valid 3.4441230297088623
EPOCH 292:
  batch 1000 loss: 3.3924016132354735
LOSS train 3.3924016132354735 valid 3.4460504055023193
EPOCH 293:
  batch 1000 loss: 3.3985267534255983
LOSS train 3.3985267534255983 valid 3.4441065788269043
EPOCH 294:
  batch 1000 loss: 3.4138030941486357
LOSS train 3.4138030941486357 valid 3.4451406002044678
EPOCH 295:
  batch 1000 loss: 3.403637067556381
LOSS train 3.403637067556381 valid 3.443631887435913
EPOCH 296:
  batch 1000 loss: 3.400397093772888
LOSS train 3.400397093772888 valid 3.445058584213257
EPOCH 297:
  batch 1000 loss: 3.4024268269538878
LOSS train 3.4024268269538878 valid 3.4439470767974854
EPOCH 298:
  batch 1000 loss: 3.3968451797962187
LOSS train 3.3968451797962187 valid 3.4451990127563477
EPOCH 299:
  batch 1000 loss: 3.385432757616043
LOSS train 3.385432757616043 valid 3.443683624267578
EPOCH 300:
  batch 1000 loss: 3.403280762434006
LOSS train 3.403280762434006 valid 3.4441568851470947
EPOCH 301:
  batch 1000 loss: 3.400420962095261
LOSS train 3.400420962095261 valid 3.4443609714508057
EPOCH 302:
  batch 1000 loss: 3.408079822778702
LOSS train 3.408079822778702 valid 3.444180727005005
EPOCH 303:
  batch 1000 loss: 3.3998221023082733
LOSS train 3.3998221023082733 valid 3.4444384574890137
EPOCH 304:
  batch 1000 loss: 3.409560063123703
LOSS train 3.409560063123703 valid 3.4442288875579834
EPOCH 305:
  batch 1000 loss: 3.410322578907013
LOSS train 3.410322578907013 valid 3.444124460220337
EPOCH 306:
  batch 1000 loss: 3.414812084197998
LOSS train 3.414812084197998 valid 3.4458870887756348
EPOCH 307:
  batch 1000 loss: 3.4094736816883087
LOSS train 3.4094736816883087 valid 3.4437198638916016
EPOCH 308:
  batch 1000 loss: 3.4158371119499207
LOSS train 3.4158371119499207 valid 3.4447314739227295
EPOCH 309:
  batch 1000 loss: 3.396346191167831
LOSS train 3.396346191167831 valid 3.445361614227295
EPOCH 310:
  batch 1000 loss: 3.4079664001464844
LOSS train 3.4079664001464844 valid 3.4441494941711426
EPOCH 311:
  batch 1000 loss: 3.4195541121959687
LOSS train 3.4195541121959687 valid 3.444033145904541
EPOCH 312:
  batch 1000 loss: 3.406116950750351
LOSS train 3.406116950750351 valid 3.445122718811035
EPOCH 313:
  batch 1000 loss: 3.3990692021846773
LOSS train 3.3990692021846773 valid 3.444969892501831
EPOCH 314:
  batch 1000 loss: 3.396375680923462
LOSS train 3.396375680923462 valid 3.4444832801818848
EPOCH 315:
  batch 1000 loss: 3.392372622728348
LOSS train 3.392372622728348 valid 3.4445886611938477
EPOCH 316:
  batch 1000 loss: 3.4045675208568573
LOSS train 3.4045675208568573 valid 3.4444684982299805
EPOCH 317:
  batch 1000 loss: 3.4149782912731173
LOSS train 3.4149782912731173 valid 3.445096969604492
EPOCH 318:
  batch 1000 loss: 3.387701865673065
LOSS train 3.387701865673065 valid 3.4437460899353027
EPOCH 319:
  batch 1000 loss: 3.409072470188141
LOSS train 3.409072470188141 valid 3.4442059993743896
EPOCH 320:
  batch 1000 loss: 3.406548299074173
LOSS train 3.406548299074173 valid 3.4460926055908203
EPOCH 321:
  batch 1000 loss: 3.392192653656006
LOSS train 3.392192653656006 valid 3.443755865097046
EPOCH 322:
  batch 1000 loss: 3.4021605343818666
LOSS train 3.4021605343818666 valid 3.4445693492889404
EPOCH 323:
  batch 1000 loss: 3.4047130131721497
LOSS train 3.4047130131721497 valid 3.4463422298431396
EPOCH 324:
  batch 1000 loss: 3.4040491666793824
LOSS train 3.4040491666793824 valid 3.4444310665130615
EPOCH 325:
  batch 1000 loss: 3.4175333955287934
LOSS train 3.4175333955287934 valid 3.445329427719116
EPOCH 326:
  batch 1000 loss: 3.41120214176178
LOSS train 3.41120214176178 valid 3.445383071899414
EPOCH 327:
  batch 1000 loss: 3.4195076081752775
LOSS train 3.4195076081752775 valid 3.4442646503448486
EPOCH 328:
  batch 1000 loss: 3.3968105118274687
LOSS train 3.3968105118274687 valid 3.445024013519287
EPOCH 329:
  batch 1000 loss: 3.4062400934696195
LOSS train 3.4062400934696195 valid 3.444033145904541
EPOCH 330:
  batch 1000 loss: 3.3962512977123263
LOSS train 3.3962512977123263 valid 3.4442453384399414
EPOCH 331:
  batch 1000 loss: 3.395255665540695
LOSS train 3.395255665540695 valid 3.444969892501831
EPOCH 332:
  batch 1000 loss: 3.403583623409271
LOSS train 3.403583623409271 valid 3.44488525390625
EPOCH 333:
  batch 1000 loss: 3.4012547030448914
LOSS train 3.4012547030448914 valid 3.4443891048431396
EPOCH 334:
  batch 1000 loss: 3.4011235208511352
LOSS train 3.4011235208511352 valid 3.4443564414978027
EPOCH 335:
  batch 1000 loss: 3.4043160610198973
LOSS train 3.4043160610198973 valid 3.4442639350891113
EPOCH 336:
  batch 1000 loss: 3.4086509690284728
LOSS train 3.4086509690284728 valid 3.445615530014038
EPOCH 337:
  batch 1000 loss: 3.4057020626068115
LOSS train 3.4057020626068115 valid 3.445061683654785
EPOCH 338:
  batch 1000 loss: 3.4087754528522494
LOSS train 3.4087754528522494 valid 3.4440524578094482
EPOCH 339:
  batch 1000 loss: 3.410181739330292
LOSS train 3.410181739330292 valid 3.445446491241455
EPOCH 340:
  batch 1000 loss: 3.3962776267528536
LOSS train 3.3962776267528536 valid 3.444258213043213
EPOCH 341:
  batch 1000 loss: 3.4063905010223388
LOSS train 3.4063905010223388 valid 3.4440126419067383
EPOCH 342:
  batch 1000 loss: 3.3865394337177275
LOSS train 3.3865394337177275 valid 3.444751262664795
EPOCH 343:
  batch 1000 loss: 3.404328985452652
LOSS train 3.404328985452652 valid 3.4439713954925537
EPOCH 344:
  batch 1000 loss: 3.4165471794605256
LOSS train 3.4165471794605256 valid 3.445377826690674
EPOCH 345:
  batch 1000 loss: 3.395001423597336
LOSS train 3.395001423597336 valid 3.4451403617858887
EPOCH 346:
  batch 1000 loss: 3.3970164012908937
LOSS train 3.3970164012908937 valid 3.444131851196289
EPOCH 347:
  batch 1000 loss: 3.4064963703155517
LOSS train 3.4064963703155517 valid 3.4445323944091797
EPOCH 348:
  batch 1000 loss: 3.406076457977295
LOSS train 3.406076457977295 valid 3.4437415599823
EPOCH 349:
  batch 1000 loss: 3.3975122361183168
LOSS train 3.3975122361183168 valid 3.4450817108154297
EPOCH 350:
  batch 1000 loss: 3.393333737850189
LOSS train 3.393333737850189 valid 3.4443957805633545
EPOCH 351:
  batch 1000 loss: 3.416398664712906
LOSS train 3.416398664712906 valid 3.4441466331481934
EPOCH 352:
  batch 1000 loss: 3.397866168498993
LOSS train 3.397866168498993 valid 3.443849563598633
EPOCH 353:
  batch 1000 loss: 3.3973873341083527
LOSS train 3.3973873341083527 valid 3.4440646171569824
EPOCH 354:
  batch 1000 loss: 3.397357036828995
LOSS train 3.397357036828995 valid 3.4450316429138184
EPOCH 355:
  batch 1000 loss: 3.4037893183231356
LOSS train 3.4037893183231356 valid 3.444182872772217
EPOCH 356:
  batch 1000 loss: 3.3885376334190367
LOSS train 3.3885376334190367 valid 3.4440360069274902
EPOCH 357:
  batch 1000 loss: 3.402040245056152
LOSS train 3.402040245056152 valid 3.4447391033172607
EPOCH 358:
  batch 1000 loss: 3.3840726284980773
LOSS train 3.3840726284980773 valid 3.4442923069000244
EPOCH 359:
  batch 1000 loss: 3.402522843360901
LOSS train 3.402522843360901 valid 3.444888114929199
EPOCH 360:
  batch 1000 loss: 3.4107123584747314
LOSS train 3.4107123584747314 valid 3.445222854614258
EPOCH 361:
  batch 1000 loss: 3.394587525367737
LOSS train 3.394587525367737 valid 3.4442710876464844
EPOCH 362:
  batch 1000 loss: 3.418547461032867
LOSS train 3.418547461032867 valid 3.4450302124023438
EPOCH 363:
  batch 1000 loss: 3.4147262921333312
LOSS train 3.4147262921333312 valid 3.4444501399993896
EPOCH 364:
  batch 1000 loss: 3.4179062592983245
LOSS train 3.4179062592983245 valid 3.444162607192993
EPOCH 365:
  batch 1000 loss: 3.4084834802150725
LOSS train 3.4084834802150725 valid 3.444148063659668
EPOCH 366:
  batch 1000 loss: 3.4057552778720854
LOSS train 3.4057552778720854 valid 3.445301055908203
EPOCH 367:
  batch 1000 loss: 3.4171594603061677
LOSS train 3.4171594603061677 valid 3.4447519779205322
EPOCH 368:
  batch 1000 loss: 3.3801476442813874
LOSS train 3.3801476442813874 valid 3.444230556488037
EPOCH 369:
  batch 1000 loss: 3.4047860934734344
LOSS train 3.4047860934734344 valid 3.4438326358795166
EPOCH 370:

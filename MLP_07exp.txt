nohup: ignoring input
EPOCH 1:
  batch 1000 loss: 6.180139814138412
LOSS train 6.180139814138412 valid 5.16431188583374
EPOCH 2:
  batch 1000 loss: 5.018893861532211
LOSS train 5.018893861532211 valid 4.90161657333374
EPOCH 3:
  batch 1000 loss: 4.8287621901035305
LOSS train 4.8287621901035305 valid 4.781122207641602
EPOCH 4:
  batch 1000 loss: 4.672839506149292
LOSS train 4.672839506149292 valid 4.5960516929626465
EPOCH 5:
  batch 1000 loss: 4.519548558950424
LOSS train 4.519548558950424 valid 4.480627059936523
EPOCH 6:
  batch 1000 loss: 4.389130225419998
LOSS train 4.389130225419998 valid 4.360799312591553
EPOCH 7:
  batch 1000 loss: 4.315337506055832
LOSS train 4.315337506055832 valid 4.256985664367676
EPOCH 8:
  batch 1000 loss: 4.208662889480591
LOSS train 4.208662889480591 valid 4.213192939758301
EPOCH 9:
  batch 1000 loss: 4.097058893680573
LOSS train 4.097058893680573 valid 4.017213344573975
EPOCH 10:
  batch 1000 loss: 3.991890058994293
LOSS train 3.991890058994293 valid 3.9459972381591797
EPOCH 11:
  batch 1000 loss: 3.9342513642311094
LOSS train 3.9342513642311094 valid 3.908360004425049
EPOCH 12:
  batch 1000 loss: 3.89107493519783
LOSS train 3.89107493519783 valid 3.9180779457092285
EPOCH 13:
  batch 1000 loss: 3.8494550678730013
LOSS train 3.8494550678730013 valid 3.8622796535491943
EPOCH 14:
  batch 1000 loss: 3.7796641025543214
LOSS train 3.7796641025543214 valid 3.8197035789489746
EPOCH 15:
  batch 1000 loss: 3.7585342166423796
LOSS train 3.7585342166423796 valid 3.8299007415771484
EPOCH 16:
  batch 1000 loss: 3.7785488843917845
LOSS train 3.7785488843917845 valid 3.7586240768432617
EPOCH 17:
  batch 1000 loss: 3.782553801059723
LOSS train 3.782553801059723 valid 3.725558280944824
EPOCH 18:
  batch 1000 loss: 3.719438493490219
LOSS train 3.719438493490219 valid 3.7591607570648193
EPOCH 19:
  batch 1000 loss: 3.705314735174179
LOSS train 3.705314735174179 valid 3.7255899906158447
EPOCH 20:
  batch 1000 loss: 3.7001272389888764
LOSS train 3.7001272389888764 valid 3.727057456970215
EPOCH 21:
  batch 1000 loss: 3.7254290487766264
LOSS train 3.7254290487766264 valid 3.7157676219940186
EPOCH 22:
  batch 1000 loss: 3.679411870718002
LOSS train 3.679411870718002 valid 3.9562289714813232
EPOCH 23:
  batch 1000 loss: 3.702918078422546
LOSS train 3.702918078422546 valid 3.6849093437194824
EPOCH 24:
  batch 1000 loss: 3.686085676074028
LOSS train 3.686085676074028 valid 3.6843669414520264
EPOCH 25:
  batch 1000 loss: 3.6726071586608886
LOSS train 3.6726071586608886 valid 3.683619737625122
EPOCH 26:
  batch 1000 loss: 3.658385542869568
LOSS train 3.658385542869568 valid 3.676485300064087
EPOCH 27:
  batch 1000 loss: 3.6291281645298006
LOSS train 3.6291281645298006 valid 3.693572521209717
EPOCH 28:
  batch 1000 loss: 3.619682286024094
LOSS train 3.619682286024094 valid 3.651076316833496
EPOCH 29:
  batch 1000 loss: 3.6238588120937347
LOSS train 3.6238588120937347 valid 3.6834142208099365
EPOCH 30:
  batch 1000 loss: 3.6082812819480896
LOSS train 3.6082812819480896 valid 3.5972986221313477
EPOCH 31:
  batch 1000 loss: 3.60341570353508
LOSS train 3.60341570353508 valid 3.6154000759124756
EPOCH 32:
  batch 1000 loss: 3.6212929837703705
LOSS train 3.6212929837703705 valid 3.6191601753234863
EPOCH 33:
  batch 1000 loss: 3.600259163618088
LOSS train 3.600259163618088 valid 3.6224520206451416
EPOCH 34:
  batch 1000 loss: 3.5976720007658005
LOSS train 3.5976720007658005 valid 3.5916476249694824
EPOCH 35:
  batch 1000 loss: 3.601488069653511
LOSS train 3.601488069653511 valid 3.5854344367980957
EPOCH 36:
  batch 1000 loss: 3.5965262417793276
LOSS train 3.5965262417793276 valid 3.5913290977478027
EPOCH 37:
  batch 1000 loss: 3.6241773443222045
LOSS train 3.6241773443222045 valid 3.5743443965911865
EPOCH 38:
  batch 1000 loss: 3.570586476325989
LOSS train 3.570586476325989 valid 3.6093499660491943
EPOCH 39:
  batch 1000 loss: 3.586571957111359
LOSS train 3.586571957111359 valid 3.5750539302825928
EPOCH 40:
  batch 1000 loss: 3.5970834872722626
LOSS train 3.5970834872722626 valid 3.6070220470428467
EPOCH 41:
  batch 1000 loss: 3.5647541506290437
LOSS train 3.5647541506290437 valid 3.536891222000122
EPOCH 42:
  batch 1000 loss: 3.5456992461681365
LOSS train 3.5456992461681365 valid 3.5659360885620117
EPOCH 43:
  batch 1000 loss: 3.5991929409503935
LOSS train 3.5991929409503935 valid 3.535043239593506
EPOCH 44:
  batch 1000 loss: 3.5594803708791733
LOSS train 3.5594803708791733 valid 3.5467464923858643
EPOCH 45:
  batch 1000 loss: 3.554477583885193
LOSS train 3.554477583885193 valid 3.550142526626587
EPOCH 46:
  batch 1000 loss: 3.552810247540474
LOSS train 3.552810247540474 valid 3.5548946857452393
EPOCH 47:
  batch 1000 loss: 3.5407283165454864
LOSS train 3.5407283165454864 valid 3.5735602378845215
EPOCH 48:
  batch 1000 loss: 3.539326668858528
LOSS train 3.539326668858528 valid 3.5347185134887695
EPOCH 49:
  batch 1000 loss: 3.5746586470603945
Epoch 00049: reducing learning rate of group 0 to 5.0000e-04.
LOSS train 3.5746586470603945 valid 3.5380806922912598
EPOCH 50:
  batch 1000 loss: 3.468442727327347
LOSS train 3.468442727327347 valid 3.516542911529541
EPOCH 51:
  batch 1000 loss: 3.468336214661598
LOSS train 3.468336214661598 valid 3.484550952911377
EPOCH 52:
  batch 1000 loss: 3.4855635501146316
LOSS train 3.4855635501146316 valid 3.478513717651367
EPOCH 53:
  batch 1000 loss: 3.4565841796398162
LOSS train 3.4565841796398162 valid 3.464832305908203
EPOCH 54:
  batch 1000 loss: 3.440669341802597
LOSS train 3.440669341802597 valid 3.466736316680908
EPOCH 55:
  batch 1000 loss: 3.449529929637909
LOSS train 3.449529929637909 valid 3.468503952026367
EPOCH 56:
  batch 1000 loss: 3.424992648124695
LOSS train 3.424992648124695 valid 3.46730375289917
EPOCH 57:
  batch 1000 loss: 3.4499609525203705
LOSS train 3.4499609525203705 valid 3.456458568572998
EPOCH 58:
  batch 1000 loss: 3.418422808885574
LOSS train 3.418422808885574 valid 3.4564270973205566
EPOCH 59:
  batch 1000 loss: 3.433692066192627
LOSS train 3.433692066192627 valid 3.4608466625213623
EPOCH 60:
  batch 1000 loss: 3.441106719493866
LOSS train 3.441106719493866 valid 3.4477789402008057
EPOCH 61:
  batch 1000 loss: 3.434946618437767
LOSS train 3.434946618437767 valid 3.4672622680664062
EPOCH 62:
  batch 1000 loss: 3.433740486383438
LOSS train 3.433740486383438 valid 3.4667232036590576
EPOCH 63:
  batch 1000 loss: 3.420368782758713
LOSS train 3.420368782758713 valid 3.451345682144165
EPOCH 64:
  batch 1000 loss: 3.424295067310333
LOSS train 3.424295067310333 valid 3.4434704780578613
EPOCH 65:
  batch 1000 loss: 3.4376672728061677
LOSS train 3.4376672728061677 valid 3.442044496536255
EPOCH 66:
  batch 1000 loss: 3.4427195246219635
LOSS train 3.4427195246219635 valid 3.4365148544311523
EPOCH 67:
  batch 1000 loss: 3.426820483326912
LOSS train 3.426820483326912 valid 3.449366807937622
EPOCH 68:
  batch 1000 loss: 3.3950194553136828
LOSS train 3.3950194553136828 valid 3.4338274002075195
EPOCH 69:
  batch 1000 loss: 3.3892650907039643
LOSS train 3.3892650907039643 valid 3.438922882080078
EPOCH 70:
  batch 1000 loss: 3.426983565211296
LOSS train 3.426983565211296 valid 3.4326841831207275
EPOCH 71:
  batch 1000 loss: 3.418090851187706
LOSS train 3.418090851187706 valid 3.4242849349975586
EPOCH 72:
  batch 1000 loss: 3.4138398576974867
LOSS train 3.4138398576974867 valid 3.43747615814209
EPOCH 73:
  batch 1000 loss: 3.3921963636875154
LOSS train 3.3921963636875154 valid 3.42838191986084
EPOCH 74:
  batch 1000 loss: 3.4102570827007295
LOSS train 3.4102570827007295 valid 3.4391443729400635
EPOCH 75:
  batch 1000 loss: 3.422607995033264
LOSS train 3.422607995033264 valid 3.41465163230896
EPOCH 76:
  batch 1000 loss: 3.4049172859191894
LOSS train 3.4049172859191894 valid 3.4149365425109863
EPOCH 77:
  batch 1000 loss: 3.403116369009018
LOSS train 3.403116369009018 valid 3.4244139194488525
EPOCH 78:
  batch 1000 loss: 3.38342125582695
LOSS train 3.38342125582695 valid 3.418767213821411
EPOCH 79:
  batch 1000 loss: 3.403806032657623
LOSS train 3.403806032657623 valid 3.4564719200134277
EPOCH 80:
  batch 1000 loss: 3.421488946199417
LOSS train 3.421488946199417 valid 3.4298648834228516
EPOCH 81:
  batch 1000 loss: 3.4129046223163604
Epoch 00081: reducing learning rate of group 0 to 2.5000e-04.
LOSS train 3.4129046223163604 valid 3.423333168029785
EPOCH 82:
  batch 1000 loss: 3.3831137043237685
LOSS train 3.3831137043237685 valid 3.4002768993377686
EPOCH 83:
  batch 1000 loss: 3.3674739491939545
LOSS train 3.3674739491939545 valid 3.399057388305664
EPOCH 84:
  batch 1000 loss: 3.38178569316864
LOSS train 3.38178569316864 valid 3.398524761199951
EPOCH 85:
  batch 1000 loss: 3.3675245473384856
LOSS train 3.3675245473384856 valid 3.391984224319458
EPOCH 86:
  batch 1000 loss: 3.3679598410129548
LOSS train 3.3679598410129548 valid 3.389035940170288
EPOCH 87:
  batch 1000 loss: 3.360701694369316
LOSS train 3.360701694369316 valid 3.397038459777832
EPOCH 88:
  batch 1000 loss: 3.349373131155968
LOSS train 3.349373131155968 valid 3.3858351707458496
EPOCH 89:
  batch 1000 loss: 3.359491928577423
LOSS train 3.359491928577423 valid 3.395028591156006
EPOCH 90:
  batch 1000 loss: 3.3527163083553315
LOSS train 3.3527163083553315 valid 3.4091625213623047
EPOCH 91:
  batch 1000 loss: 3.3672006667852403
LOSS train 3.3672006667852403 valid 3.3814330101013184
EPOCH 92:
  batch 1000 loss: 3.367267919063568
LOSS train 3.367267919063568 valid 3.3793299198150635
EPOCH 93:
  batch 1000 loss: 3.341557690858841
LOSS train 3.341557690858841 valid 3.3968677520751953
EPOCH 94:
  batch 1000 loss: 3.362693288683891
LOSS train 3.362693288683891 valid 3.384199619293213
EPOCH 95:
  batch 1000 loss: 3.3608913204669952
LOSS train 3.3608913204669952 valid 3.3800816535949707
EPOCH 96:
  batch 1000 loss: 3.3523071074485777
LOSS train 3.3523071074485777 valid 3.3959598541259766
EPOCH 97:
  batch 1000 loss: 3.3512640887498857
LOSS train 3.3512640887498857 valid 3.3766355514526367
EPOCH 98:
  batch 1000 loss: 3.3670105018615724
LOSS train 3.3670105018615724 valid 3.3754329681396484
EPOCH 99:
  batch 1000 loss: 3.3569148555994035
LOSS train 3.3569148555994035 valid 3.3750147819519043
EPOCH 100:
  batch 1000 loss: 3.368490123152733
LOSS train 3.368490123152733 valid 3.372655153274536
EPOCH 101:
  batch 1000 loss: 3.3566221231222153
LOSS train 3.3566221231222153 valid 3.372218370437622
EPOCH 102:
  batch 1000 loss: 3.3608819017410276
LOSS train 3.3608819017410276 valid 3.386293888092041
EPOCH 103:
  batch 1000 loss: 3.3430262303352354
LOSS train 3.3430262303352354 valid 3.372539520263672
EPOCH 104:
  batch 1000 loss: 3.3531001324653626
LOSS train 3.3531001324653626 valid 3.381061553955078
EPOCH 105:
  batch 1000 loss: 3.3551376078128814
LOSS train 3.3551376078128814 valid 3.3758113384246826
EPOCH 106:
  batch 1000 loss: 3.3327505576610563
LOSS train 3.3327505576610563 valid 3.367696523666382
EPOCH 107:
  batch 1000 loss: 3.337689828872681
LOSS train 3.337689828872681 valid 3.3928415775299072
EPOCH 108:
  batch 1000 loss: 3.3568520312309267
LOSS train 3.3568520312309267 valid 3.3634703159332275
EPOCH 109:
  batch 1000 loss: 3.3514119577407837
LOSS train 3.3514119577407837 valid 3.3706345558166504
EPOCH 110:
  batch 1000 loss: 3.3484933297634125
LOSS train 3.3484933297634125 valid 3.369677782058716
EPOCH 111:
  batch 1000 loss: 3.3243463481664657
LOSS train 3.3243463481664657 valid 3.38374400138855
EPOCH 112:
  batch 1000 loss: 3.3390887863636016
LOSS train 3.3390887863636016 valid 3.3628242015838623
EPOCH 113:
  batch 1000 loss: 3.3620969734191894
LOSS train 3.3620969734191894 valid 3.3747806549072266
EPOCH 114:
  batch 1000 loss: 3.3379261873960493
LOSS train 3.3379261873960493 valid 3.358818531036377
EPOCH 115:
  batch 1000 loss: 3.3468455338478087
LOSS train 3.3468455338478087 valid 3.3579370975494385
EPOCH 116:
  batch 1000 loss: 3.352726815700531
LOSS train 3.352726815700531 valid 3.3605997562408447
EPOCH 117:
  batch 1000 loss: 3.3348437054157256
LOSS train 3.3348437054157256 valid 3.3595848083496094
EPOCH 118:
  batch 1000 loss: 3.338796980142593
LOSS train 3.338796980142593 valid 3.355992555618286
EPOCH 119:
  batch 1000 loss: 3.3301898443698885
LOSS train 3.3301898443698885 valid 3.354586124420166
EPOCH 120:
  batch 1000 loss: 3.324720700263977
LOSS train 3.324720700263977 valid 3.35945725440979
EPOCH 121:
  batch 1000 loss: 3.3444516936540603
LOSS train 3.3444516936540603 valid 3.3595175743103027
EPOCH 122:
  batch 1000 loss: 3.334050688266754
LOSS train 3.334050688266754 valid 3.351606845855713
EPOCH 123:
  batch 1000 loss: 3.3175056645870207
LOSS train 3.3175056645870207 valid 3.363959312438965
EPOCH 124:
  batch 1000 loss: 3.328881311178207
LOSS train 3.328881311178207 valid 3.349351406097412
EPOCH 125:
  batch 1000 loss: 3.33606676530838
LOSS train 3.33606676530838 valid 3.358402729034424
EPOCH 126:
  batch 1000 loss: 3.355155413866043
LOSS train 3.355155413866043 valid 3.349865198135376
EPOCH 127:
  batch 1000 loss: 3.3153222444057464
LOSS train 3.3153222444057464 valid 3.3522257804870605
EPOCH 128:
  batch 1000 loss: 3.3280411523580553
LOSS train 3.3280411523580553 valid 3.3534584045410156
EPOCH 129:
  batch 1000 loss: 3.3343735382556914
LOSS train 3.3343735382556914 valid 3.352041006088257
EPOCH 130:
  batch 1000 loss: 3.340460646390915
Epoch 00130: reducing learning rate of group 0 to 1.2500e-04.
LOSS train 3.340460646390915 valid 3.3548896312713623
EPOCH 131:
  batch 1000 loss: 3.3119888935089112
LOSS train 3.3119888935089112 valid 3.343864679336548
EPOCH 132:
  batch 1000 loss: 3.3358068616390226
LOSS train 3.3358068616390226 valid 3.3425955772399902
EPOCH 133:
  batch 1000 loss: 3.3351948363780974
LOSS train 3.3351948363780974 valid 3.3455958366394043
EPOCH 134:
  batch 1000 loss: 3.3192300777435304
LOSS train 3.3192300777435304 valid 3.342331647872925
EPOCH 135:
  batch 1000 loss: 3.3084273215532303
LOSS train 3.3084273215532303 valid 3.341719388961792
EPOCH 136:
  batch 1000 loss: 3.3211837205886843
LOSS train 3.3211837205886843 valid 3.340826988220215
EPOCH 137:
  batch 1000 loss: 3.327914051294327
LOSS train 3.327914051294327 valid 3.3465771675109863
EPOCH 138:
  batch 1000 loss: 3.307472438335419
LOSS train 3.307472438335419 valid 3.3478658199310303
EPOCH 139:
  batch 1000 loss: 3.315653231024742
LOSS train 3.315653231024742 valid 3.341871500015259
EPOCH 140:
  batch 1000 loss: 3.327132288813591
LOSS train 3.327132288813591 valid 3.341722249984741
EPOCH 141:
  batch 1000 loss: 3.301063332080841
LOSS train 3.301063332080841 valid 3.3361330032348633
EPOCH 142:
  batch 1000 loss: 3.3218861906528474
LOSS train 3.3218861906528474 valid 3.3396713733673096
EPOCH 143:
  batch 1000 loss: 3.3202416219711304
LOSS train 3.3202416219711304 valid 3.3379745483398438
EPOCH 144:
  batch 1000 loss: 3.2969222782850265
LOSS train 3.2969222782850265 valid 3.334958553314209
EPOCH 145:
  batch 1000 loss: 3.3094067298173906
LOSS train 3.3094067298173906 valid 3.3367035388946533
EPOCH 146:
  batch 1000 loss: 3.3252807379961014
LOSS train 3.3252807379961014 valid 3.3378355503082275
EPOCH 147:
  batch 1000 loss: 3.3148092819452284
LOSS train 3.3148092819452284 valid 3.3356049060821533
EPOCH 148:
  batch 1000 loss: 3.300383351325989
LOSS train 3.300383351325989 valid 3.3348517417907715
EPOCH 149:
  batch 1000 loss: 3.307988478899002
LOSS train 3.307988478899002 valid 3.331782817840576
EPOCH 150:
  batch 1000 loss: 3.3052796421051025
LOSS train 3.3052796421051025 valid 3.3486361503601074
EPOCH 151:
  batch 1000 loss: 3.2982846180200576
LOSS train 3.2982846180200576 valid 3.335317373275757
EPOCH 152:
  batch 1000 loss: 3.3061826115846635
LOSS train 3.3061826115846635 valid 3.331990957260132
EPOCH 153:
  batch 1000 loss: 3.295917446613312
LOSS train 3.295917446613312 valid 3.3302218914031982
EPOCH 154:
  batch 1000 loss: 3.2898462810516356
LOSS train 3.2898462810516356 valid 3.3299336433410645
EPOCH 155:
  batch 1000 loss: 3.3154208488464354
LOSS train 3.3154208488464354 valid 3.3340182304382324
EPOCH 156:
  batch 1000 loss: 3.310474820971489
LOSS train 3.310474820971489 valid 3.32674241065979
EPOCH 157:
  batch 1000 loss: 3.3010522968769074
LOSS train 3.3010522968769074 valid 3.330397844314575
EPOCH 158:
  batch 1000 loss: 3.2942328169345854
LOSS train 3.2942328169345854 valid 3.3250675201416016
EPOCH 159:
  batch 1000 loss: 3.2990590023994444
LOSS train 3.2990590023994444 valid 3.3266637325286865
EPOCH 160:
  batch 1000 loss: 3.2958090852499007
LOSS train 3.2958090852499007 valid 3.3288962841033936
EPOCH 161:
  batch 1000 loss: 3.2941612919569017
LOSS train 3.2941612919569017 valid 3.32930588722229
EPOCH 162:
  batch 1000 loss: 3.2933855023384093
LOSS train 3.2933855023384093 valid 3.3248322010040283
EPOCH 163:
  batch 1000 loss: 3.293143865466118
LOSS train 3.293143865466118 valid 3.325889825820923
EPOCH 164:
  batch 1000 loss: 3.2928397901058197
LOSS train 3.2928397901058197 valid 3.32365345954895
EPOCH 165:
  batch 1000 loss: 3.2917884483337403
LOSS train 3.2917884483337403 valid 3.3221468925476074
EPOCH 166:
  batch 1000 loss: 3.308100399017334
LOSS train 3.308100399017334 valid 3.322075366973877
EPOCH 167:
  batch 1000 loss: 3.3036531722545623
LOSS train 3.3036531722545623 valid 3.3306164741516113
EPOCH 168:
  batch 1000 loss: 3.2995157896280287
LOSS train 3.2995157896280287 valid 3.32399845123291
EPOCH 169:
  batch 1000 loss: 3.2897713594436646
LOSS train 3.2897713594436646 valid 3.321643590927124
EPOCH 170:
  batch 1000 loss: 3.2844200570583344
LOSS train 3.2844200570583344 valid 3.320782423019409
EPOCH 171:
  batch 1000 loss: 3.2918021733760834
LOSS train 3.2918021733760834 valid 3.3228886127471924
EPOCH 172:
  batch 1000 loss: 3.2862832993268967
LOSS train 3.2862832993268967 valid 3.3239905834198
EPOCH 173:
  batch 1000 loss: 3.307371815919876
LOSS train 3.307371815919876 valid 3.3248250484466553
EPOCH 174:
  batch 1000 loss: 3.306298998594284
LOSS train 3.306298998594284 valid 3.3212571144104004
EPOCH 175:
  batch 1000 loss: 3.301420149445534
LOSS train 3.301420149445534 valid 3.32356858253479
EPOCH 176:
  batch 1000 loss: 3.2943333349227903
LOSS train 3.2943333349227903 valid 3.32035756111145
EPOCH 177:
  batch 1000 loss: 3.3088441014289858
LOSS train 3.3088441014289858 valid 3.3172359466552734
EPOCH 178:
  batch 1000 loss: 3.293412999153137
LOSS train 3.293412999153137 valid 3.321591377258301
EPOCH 179:
  batch 1000 loss: 3.293259890675545
LOSS train 3.293259890675545 valid 3.3174829483032227
EPOCH 180:
  batch 1000 loss: 3.292302371740341
LOSS train 3.292302371740341 valid 3.3194377422332764
EPOCH 181:
  batch 1000 loss: 3.2874773848056793
LOSS train 3.2874773848056793 valid 3.3208906650543213
EPOCH 182:
  batch 1000 loss: 3.2882777909040453
LOSS train 3.2882777909040453 valid 3.317988634109497
EPOCH 183:
  batch 1000 loss: 3.292120048999786
Epoch 00183: reducing learning rate of group 0 to 6.2500e-05.
LOSS train 3.292120048999786 valid 3.319296360015869
EPOCH 184:
  batch 1000 loss: 3.2873966007232664
LOSS train 3.2873966007232664 valid 3.3126041889190674
EPOCH 185:
  batch 1000 loss: 3.2882244489192964
LOSS train 3.2882244489192964 valid 3.313159704208374
EPOCH 186:
  batch 1000 loss: 3.295097944021225
LOSS train 3.295097944021225 valid 3.3129820823669434
EPOCH 187:
  batch 1000 loss: 3.3010705612897873
LOSS train 3.3010705612897873 valid 3.3132569789886475
EPOCH 188:
  batch 1000 loss: 3.2893370673656466
LOSS train 3.2893370673656466 valid 3.3124568462371826
EPOCH 189:
  batch 1000 loss: 3.286576417684555
LOSS train 3.286576417684555 valid 3.31086802482605
EPOCH 190:
  batch 1000 loss: 3.2747806403636934
LOSS train 3.2747806403636934 valid 3.313567638397217
EPOCH 191:
  batch 1000 loss: 3.290791031241417
LOSS train 3.290791031241417 valid 3.3112144470214844
EPOCH 192:
  batch 1000 loss: 3.2841132562160493
LOSS train 3.2841132562160493 valid 3.313410520553589
EPOCH 193:
  batch 1000 loss: 3.260358712673187
LOSS train 3.260358712673187 valid 3.3114123344421387
EPOCH 194:
  batch 1000 loss: 3.2899235758781433
LOSS train 3.2899235758781433 valid 3.3122169971466064
EPOCH 195:
  batch 1000 loss: 3.262879383325577
LOSS train 3.262879383325577 valid 3.3099610805511475
EPOCH 196:
  batch 1000 loss: 3.2744317497015
LOSS train 3.2744317497015 valid 3.3111367225646973
EPOCH 197:
  batch 1000 loss: 3.2926043573617934
LOSS train 3.2926043573617934 valid 3.3119423389434814
EPOCH 198:
  batch 1000 loss: 3.287416261792183
LOSS train 3.287416261792183 valid 3.3095192909240723
EPOCH 199:
  batch 1000 loss: 3.2737261921167375
LOSS train 3.2737261921167375 valid 3.309436321258545
EPOCH 200:
  batch 1000 loss: 3.2781019480228424
LOSS train 3.2781019480228424 valid 3.3103179931640625
EPOCH 201:
  batch 1000 loss: 3.285183920264244
LOSS train 3.285183920264244 valid 3.3107707500457764
EPOCH 202:
  batch 1000 loss: 3.277042948126793
LOSS train 3.277042948126793 valid 3.3093204498291016
EPOCH 203:
  batch 1000 loss: 3.273156664133072
LOSS train 3.273156664133072 valid 3.3077785968780518
EPOCH 204:
  batch 1000 loss: 3.2878506484031678
LOSS train 3.2878506484031678 valid 3.309119939804077
EPOCH 205:
  batch 1000 loss: 3.2807042009830476
LOSS train 3.2807042009830476 valid 3.308274745941162
EPOCH 206:
  batch 1000 loss: 3.278681778907776
LOSS train 3.278681778907776 valid 3.3080086708068848
EPOCH 207:
  batch 1000 loss: 3.285377010822296
LOSS train 3.285377010822296 valid 3.3081414699554443
EPOCH 208:
  batch 1000 loss: 3.2721226947307587
LOSS train 3.2721226947307587 valid 3.3072335720062256
EPOCH 209:
  batch 1000 loss: 3.2852055292129516
LOSS train 3.2852055292129516 valid 3.3092103004455566
EPOCH 210:
  batch 1000 loss: 3.292496150612831
LOSS train 3.292496150612831 valid 3.311086893081665
EPOCH 211:
  batch 1000 loss: 3.270414385318756
LOSS train 3.270414385318756 valid 3.3083014488220215
EPOCH 212:
  batch 1000 loss: 3.2808446444272996
LOSS train 3.2808446444272996 valid 3.310173273086548
EPOCH 213:
  batch 1000 loss: 3.274711851596832
LOSS train 3.274711851596832 valid 3.307767629623413
EPOCH 214:
  batch 1000 loss: 3.2643970880508424
Epoch 00214: reducing learning rate of group 0 to 3.1250e-05.
LOSS train 3.2643970880508424 valid 3.307189464569092
EPOCH 215:
  batch 1000 loss: 3.256872971534729
LOSS train 3.256872971534729 valid 3.303647994995117
EPOCH 216:
  batch 1000 loss: 3.282049710035324
LOSS train 3.282049710035324 valid 3.304440975189209
EPOCH 217:
  batch 1000 loss: 3.270229150056839
LOSS train 3.270229150056839 valid 3.304725408554077
EPOCH 218:
  batch 1000 loss: 3.2769083375930785
LOSS train 3.2769083375930785 valid 3.302792549133301
EPOCH 219:
  batch 1000 loss: 3.2649608652591704
LOSS train 3.2649608652591704 valid 3.3028132915496826
EPOCH 220:
  batch 1000 loss: 3.2697005681991578
LOSS train 3.2697005681991578 valid 3.302485227584839
EPOCH 221:
  batch 1000 loss: 3.2674186041355133
LOSS train 3.2674186041355133 valid 3.3021252155303955
EPOCH 222:
  batch 1000 loss: 3.279175904750824
LOSS train 3.279175904750824 valid 3.3028478622436523
EPOCH 223:
  batch 1000 loss: 3.288304719209671
LOSS train 3.288304719209671 valid 3.304513454437256
EPOCH 224:
  batch 1000 loss: 3.272657632946968
LOSS train 3.272657632946968 valid 3.3025548458099365
EPOCH 225:
  batch 1000 loss: 3.2688551577329634
LOSS train 3.2688551577329634 valid 3.3027992248535156
EPOCH 226:
  batch 1000 loss: 3.260737545967102
LOSS train 3.260737545967102 valid 3.303314208984375
EPOCH 227:
  batch 1000 loss: 3.2677102398872377
Epoch 00227: reducing learning rate of group 0 to 1.5625e-05.
LOSS train 3.2677102398872377 valid 3.3019909858703613
EPOCH 228:
  batch 1000 loss: 3.2596632772684098
LOSS train 3.2596632772684098 valid 3.302128314971924
EPOCH 229:
  batch 1000 loss: 3.279996209859848
LOSS train 3.279996209859848 valid 3.301776170730591
EPOCH 230:
  batch 1000 loss: 3.2604812175035476
LOSS train 3.2604812175035476 valid 3.3022918701171875
EPOCH 231:
  batch 1000 loss: 3.2923177542686464
LOSS train 3.2923177542686464 valid 3.301325559616089
EPOCH 232:
  batch 1000 loss: 3.2566857154369355
LOSS train 3.2566857154369355 valid 3.3030879497528076
EPOCH 233:
  batch 1000 loss: 3.2648731417655945
LOSS train 3.2648731417655945 valid 3.301443576812744
EPOCH 234:
  batch 1000 loss: 3.26111808013916
LOSS train 3.26111808013916 valid 3.3017005920410156
EPOCH 235:
  batch 1000 loss: 3.2799234493970872
LOSS train 3.2799234493970872 valid 3.3011791706085205
EPOCH 236:
  batch 1000 loss: 3.2801023586988447
LOSS train 3.2801023586988447 valid 3.3020827770233154
EPOCH 237:
  batch 1000 loss: 3.273393941402435
Epoch 00237: reducing learning rate of group 0 to 7.8125e-06.
LOSS train 3.273393941402435 valid 3.30108380317688
EPOCH 238:
  batch 1000 loss: 3.2686545684337616
LOSS train 3.2686545684337616 valid 3.300856113433838
EPOCH 239:
  batch 1000 loss: 3.255736029267311
LOSS train 3.255736029267311 valid 3.3009767532348633
EPOCH 240:
  batch 1000 loss: 3.2669001168012617
LOSS train 3.2669001168012617 valid 3.3007960319519043
EPOCH 241:
  batch 1000 loss: 3.2466655888557434
LOSS train 3.2466655888557434 valid 3.301283836364746
EPOCH 242:
  batch 1000 loss: 3.276393414258957
LOSS train 3.276393414258957 valid 3.3009819984436035
EPOCH 243:
  batch 1000 loss: 3.276748101115227
LOSS train 3.276748101115227 valid 3.3014075756073
EPOCH 244:
  batch 1000 loss: 3.262322490811348
Epoch 00244: reducing learning rate of group 0 to 3.9063e-06.
LOSS train 3.262322490811348 valid 3.3005363941192627
EPOCH 245:
  batch 1000 loss: 3.2614240304231643
LOSS train 3.2614240304231643 valid 3.300143003463745
EPOCH 246:
  batch 1000 loss: 3.2649509695768355
LOSS train 3.2649509695768355 valid 3.300016403198242
EPOCH 247:
  batch 1000 loss: 3.267322615623474
LOSS train 3.267322615623474 valid 3.3008065223693848
EPOCH 248:
  batch 1000 loss: 3.2758279840946196
LOSS train 3.2758279840946196 valid 3.3013341426849365
EPOCH 249:
  batch 1000 loss: 3.2811619527339935
LOSS train 3.2811619527339935 valid 3.2996840476989746
EPOCH 250:
  batch 1000 loss: 3.2733025035858154
LOSS train 3.2733025035858154 valid 3.3004813194274902
EPOCH 251:
  batch 1000 loss: 3.263774748682976
LOSS train 3.263774748682976 valid 3.300568103790283
EPOCH 252:
  batch 1000 loss: 3.278325939655304
LOSS train 3.278325939655304 valid 3.3005220890045166
EPOCH 253:
  batch 1000 loss: 3.255061640262604
LOSS train 3.255061640262604 valid 3.3002991676330566
EPOCH 254:
  batch 1000 loss: 3.2591017068624497
LOSS train 3.2591017068624497 valid 3.3002123832702637
EPOCH 255:
  batch 1000 loss: 3.2796904656887054
Epoch 00255: reducing learning rate of group 0 to 1.9531e-06.
LOSS train 3.2796904656887054 valid 3.301453113555908
EPOCH 256:
  batch 1000 loss: 3.267021618247032
LOSS train 3.267021618247032 valid 3.3001999855041504
EPOCH 257:
  batch 1000 loss: 3.2693849054574966
LOSS train 3.2693849054574966 valid 3.3005218505859375
EPOCH 258:
  batch 1000 loss: 3.260849889397621
LOSS train 3.260849889397621 valid 3.3003222942352295
EPOCH 259:
  batch 1000 loss: 3.256906503200531
LOSS train 3.256906503200531 valid 3.299877882003784
EPOCH 260:
  batch 1000 loss: 3.245028182268143
LOSS train 3.245028182268143 valid 3.300307273864746
EPOCH 261:
  batch 1000 loss: 3.2633518743515015
Epoch 00261: reducing learning rate of group 0 to 9.7656e-07.
LOSS train 3.2633518743515015 valid 3.3003005981445312
EPOCH 262:
  batch 1000 loss: 3.2783574259281156
LOSS train 3.2783574259281156 valid 3.299199104309082
EPOCH 263:
  batch 1000 loss: 3.26030633854866
LOSS train 3.26030633854866 valid 3.300847291946411
EPOCH 264:
  batch 1000 loss: 3.27443581366539
LOSS train 3.27443581366539 valid 3.3001205921173096
EPOCH 265:
  batch 1000 loss: 3.269078975200653
LOSS train 3.269078975200653 valid 3.300480604171753
EPOCH 266:
  batch 1000 loss: 3.2766812851428986
LOSS train 3.2766812851428986 valid 3.3002727031707764
EPOCH 267:
  batch 1000 loss: 3.274068326473236
LOSS train 3.274068326473236 valid 3.3002867698669434
EPOCH 268:
  batch 1000 loss: 3.258333945989609
Epoch 00268: reducing learning rate of group 0 to 4.8828e-07.
LOSS train 3.258333945989609 valid 3.299474000930786
EPOCH 269:
  batch 1000 loss: 3.2727589091062548
LOSS train 3.2727589091062548 valid 3.3011202812194824
EPOCH 270:
  batch 1000 loss: 3.270236910820007
LOSS train 3.270236910820007 valid 3.2996857166290283
EPOCH 271:
  batch 1000 loss: 3.2721678680181503
LOSS train 3.2721678680181503 valid 3.299135208129883
EPOCH 272:
  batch 1000 loss: 3.2628375246524812
LOSS train 3.2628375246524812 valid 3.299999237060547
EPOCH 273:
  batch 1000 loss: 3.256862885713577
LOSS train 3.256862885713577 valid 3.3003592491149902
EPOCH 274:
  batch 1000 loss: 3.27053360748291
Epoch 00274: reducing learning rate of group 0 to 2.4414e-07.
LOSS train 3.27053360748291 valid 3.2997281551361084
EPOCH 275:
  batch 1000 loss: 3.2555768246650696
LOSS train 3.2555768246650696 valid 3.300964117050171
EPOCH 276:
  batch 1000 loss: 3.258546317100525
LOSS train 3.258546317100525 valid 3.300201892852783
EPOCH 277:
  batch 1000 loss: 3.263248707532883
LOSS train 3.263248707532883 valid 3.300395965576172
EPOCH 278:
  batch 1000 loss: 3.2533189966678617
LOSS train 3.2533189966678617 valid 3.30010724067688
EPOCH 279:
  batch 1000 loss: 3.2614927641153337
LOSS train 3.2614927641153337 valid 3.3004767894744873
EPOCH 280:
  batch 1000 loss: 3.293780855178833
Epoch 00280: reducing learning rate of group 0 to 1.2207e-07.
LOSS train 3.293780855178833 valid 3.3001248836517334
EPOCH 281:
  batch 1000 loss: 3.261970185756683
LOSS train 3.261970185756683 valid 3.299706220626831
EPOCH 282:
  batch 1000 loss: 3.272169604063034
LOSS train 3.272169604063034 valid 3.2996697425842285
EPOCH 283:
  batch 1000 loss: 3.2715906450748444
LOSS train 3.2715906450748444 valid 3.3000640869140625
EPOCH 284:
  batch 1000 loss: 3.262853178739548
LOSS train 3.262853178739548 valid 3.299600839614868
EPOCH 285:
  batch 1000 loss: 3.264555098772049
LOSS train 3.264555098772049 valid 3.2999978065490723
EPOCH 286:
  batch 1000 loss: 3.2654290744066237
Epoch 00286: reducing learning rate of group 0 to 6.1035e-08.
LOSS train 3.2654290744066237 valid 3.3000199794769287
EPOCH 287:
  batch 1000 loss: 3.2723770096302034
LOSS train 3.2723770096302034 valid 3.299417734146118
EPOCH 288:
  batch 1000 loss: 3.2562766630649564
LOSS train 3.2562766630649564 valid 3.2996678352355957
EPOCH 289:
  batch 1000 loss: 3.2767218976020813
LOSS train 3.2767218976020813 valid 3.299635171890259
EPOCH 290:
  batch 1000 loss: 3.283425897240639
LOSS train 3.283425897240639 valid 3.299738883972168
EPOCH 291:
  batch 1000 loss: 3.261669019937515
LOSS train 3.261669019937515 valid 3.3000597953796387
EPOCH 292:
  batch 1000 loss: 3.269552206993103
Epoch 00292: reducing learning rate of group 0 to 3.0518e-08.
LOSS train 3.269552206993103 valid 3.301281452178955
EPOCH 293:
  batch 1000 loss: 3.265618095755577
LOSS train 3.265618095755577 valid 3.300574779510498
EPOCH 294:
  batch 1000 loss: 3.2683821474313737
LOSS train 3.2683821474313737 valid 3.2998218536376953
EPOCH 295:
  batch 1000 loss: 3.2798882950544357
LOSS train 3.2798882950544357 valid 3.300493001937866
EPOCH 296:
  batch 1000 loss: 3.282109786987305
LOSS train 3.282109786987305 valid 3.299701452255249
EPOCH 297:
  batch 1000 loss: 3.274776064634323
LOSS train 3.274776064634323 valid 3.3002219200134277
EPOCH 298:
  batch 1000 loss: 3.275967409491539
Epoch 00298: reducing learning rate of group 0 to 1.5259e-08.
LOSS train 3.275967409491539 valid 3.2998147010803223
EPOCH 299:
  batch 1000 loss: 3.28405621778965
LOSS train 3.28405621778965 valid 3.300495147705078
EPOCH 300:
  batch 1000 loss: 3.2819039385318756
LOSS train 3.2819039385318756 valid 3.300048828125
EPOCH 301:
  batch 1000 loss: 3.249530655622482
LOSS train 3.249530655622482 valid 3.2995591163635254
EPOCH 302:
  batch 1000 loss: 3.2556479370594023
LOSS train 3.2556479370594023 valid 3.300135612487793
EPOCH 303:
  batch 1000 loss: 3.257098072528839
LOSS train 3.257098072528839 valid 3.299511671066284
EPOCH 304:
  batch 1000 loss: 3.2497820258140564
LOSS train 3.2497820258140564 valid 3.3005120754241943
EPOCH 305:
  batch 1000 loss: 3.259667403101921
LOSS train 3.259667403101921 valid 3.3006703853607178
EPOCH 306:
  batch 1000 loss: 3.2657452428340914
LOSS train 3.2657452428340914 valid 3.300116539001465
EPOCH 307:
  batch 1000 loss: 3.275503671884537
LOSS train 3.275503671884537 valid 3.3008570671081543
EPOCH 308:
  batch 1000 loss: 3.27055685210228
LOSS train 3.27055685210228 valid 3.3005802631378174
EPOCH 309:
  batch 1000 loss: 3.2718151930570603
LOSS train 3.2718151930570603 valid 3.3003883361816406
EPOCH 310:
  batch 1000 loss: 3.273883779168129
LOSS train 3.273883779168129 valid 3.300546169281006
EPOCH 311:
  batch 1000 loss: 3.2835846807956695
LOSS train 3.2835846807956695 valid 3.3001983165740967
EPOCH 312:
  batch 1000 loss: 3.2740287590026855
LOSS train 3.2740287590026855 valid 3.3003456592559814
EPOCH 313:
  batch 1000 loss: 3.2616936321258545
LOSS train 3.2616936321258545 valid 3.299509048461914
EPOCH 314:
  batch 1000 loss: 3.2767235144376756
LOSS train 3.2767235144376756 valid 3.3000519275665283
EPOCH 315:
  batch 1000 loss: 3.2746274547576903
LOSS train 3.2746274547576903 valid 3.29965877532959
EPOCH 316:
  batch 1000 loss: 3.263878384590149
LOSS train 3.263878384590149 valid 3.2997217178344727
EPOCH 317:
  batch 1000 loss: 3.272252457380295
LOSS train 3.272252457380295 valid 3.300014019012451
EPOCH 318:
  batch 1000 loss: 3.253346467137337
LOSS train 3.253346467137337 valid 3.2991385459899902
EPOCH 319:
  batch 1000 loss: 3.2647712364196777
LOSS train 3.2647712364196777 valid 3.3000972270965576
EPOCH 320:
  batch 1000 loss: 3.2626801946163178
LOSS train 3.2626801946163178 valid 3.2997586727142334
EPOCH 321:
  batch 1000 loss: 3.2666074591875076
LOSS train 3.2666074591875076 valid 3.300055503845215
EPOCH 322:
  batch 1000 loss: 3.2774091794490814
LOSS train 3.2774091794490814 valid 3.300002336502075
EPOCH 323:
  batch 1000 loss: 3.2650312249660494
LOSS train 3.2650312249660494 valid 3.3001787662506104
EPOCH 324:
  batch 1000 loss: 3.263900286793709
LOSS train 3.263900286793709 valid 3.2997424602508545
EPOCH 325:
  batch 1000 loss: 3.259784677028656
LOSS train 3.259784677028656 valid 3.299389123916626
EPOCH 326:
  batch 1000 loss: 3.263356469154358
LOSS train 3.263356469154358 valid 3.3006927967071533
EPOCH 327:
  batch 1000 loss: 3.2695479454994203
LOSS train 3.2695479454994203 valid 3.3002593517303467
EPOCH 328:
  batch 1000 loss: 3.2675873678922653

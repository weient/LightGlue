nohup: ignoring input
EPOCH 1:
  batch 1000 loss: 6.38786815905571
LOSS train 6.38786815905571 valid 5.29026460647583
EPOCH 2:
  batch 1000 loss: 5.141774356365204
LOSS train 5.141774356365204 valid 4.785276889801025
EPOCH 3:
  batch 1000 loss: 4.653166270017624
LOSS train 4.653166270017624 valid 4.489579677581787
EPOCH 4:
  batch 1000 loss: 4.425110756874084
LOSS train 4.425110756874084 valid 4.203397750854492
EPOCH 5:
  batch 1000 loss: 4.1409528000354765
LOSS train 4.1409528000354765 valid 4.061456203460693
EPOCH 6:
  batch 1000 loss: 3.9659231851100922
LOSS train 3.9659231851100922 valid 3.9300105571746826
EPOCH 7:
  batch 1000 loss: 3.859203877210617
LOSS train 3.859203877210617 valid 3.8567252159118652
EPOCH 8:
  batch 1000 loss: 3.8312622082233427
LOSS train 3.8312622082233427 valid 3.819084882736206
EPOCH 9:
  batch 1000 loss: 3.7436997394561766
LOSS train 3.7436997394561766 valid 3.7415080070495605
EPOCH 10:
  batch 1000 loss: 3.7170394024848936
LOSS train 3.7170394024848936 valid 3.794950485229492
EPOCH 11:
  batch 1000 loss: 3.675737975597382
LOSS train 3.675737975597382 valid 3.7460265159606934
EPOCH 12:
  batch 1000 loss: 3.659487653017044
LOSS train 3.659487653017044 valid 3.6724188327789307
EPOCH 13:
  batch 1000 loss: 3.645615260362625
LOSS train 3.645615260362625 valid 3.6973164081573486
EPOCH 14:
  batch 1000 loss: 3.6514336099624636
LOSS train 3.6514336099624636 valid 3.631824254989624
EPOCH 15:
  batch 1000 loss: 3.629678456068039
LOSS train 3.629678456068039 valid 3.6137077808380127
EPOCH 16:
  batch 1000 loss: 3.621653315782547
LOSS train 3.621653315782547 valid 3.588972568511963
EPOCH 17:
  batch 1000 loss: 3.5942241559028627
LOSS train 3.5942241559028627 valid 3.611297369003296
EPOCH 18:
  batch 1000 loss: 3.574276562213898
LOSS train 3.574276562213898 valid 3.66566801071167
EPOCH 19:
  batch 1000 loss: 3.6092077813148498
LOSS train 3.6092077813148498 valid 3.585031509399414
EPOCH 20:
  batch 1000 loss: 3.560620971441269
LOSS train 3.560620971441269 valid 3.586423635482788
EPOCH 21:
  batch 1000 loss: 3.5567335653305054
LOSS train 3.5567335653305054 valid 3.607816457748413
EPOCH 22:
  batch 1000 loss: 3.5529692678451537
LOSS train 3.5529692678451537 valid 3.5416853427886963
EPOCH 23:
  batch 1000 loss: 3.530780473947525
LOSS train 3.530780473947525 valid 3.541137218475342
EPOCH 24:
  batch 1000 loss: 3.5463810949325563
LOSS train 3.5463810949325563 valid 3.534424304962158
EPOCH 25:
  batch 1000 loss: 3.5214140293598173
LOSS train 3.5214140293598173 valid 3.5633654594421387
EPOCH 26:
  batch 1000 loss: 3.534742372274399
LOSS train 3.534742372274399 valid 3.551285982131958
EPOCH 27:
  batch 1000 loss: 3.52654532289505
LOSS train 3.52654532289505 valid 3.541034698486328
EPOCH 28:
  batch 1000 loss: 3.5340142738819122
LOSS train 3.5340142738819122 valid 3.5198988914489746
EPOCH 29:
  batch 1000 loss: 3.486922546386719
LOSS train 3.486922546386719 valid 3.7024033069610596
EPOCH 30:
  batch 1000 loss: 3.499632077217102
LOSS train 3.499632077217102 valid 3.514268398284912
EPOCH 31:
  batch 1000 loss: 3.4980398967266084
LOSS train 3.4980398967266084 valid 3.504061460494995
EPOCH 32:
  batch 1000 loss: 3.5161247844696044
LOSS train 3.5161247844696044 valid 3.492521286010742
EPOCH 33:
  batch 1000 loss: 3.498716338634491
LOSS train 3.498716338634491 valid 3.495527982711792
EPOCH 34:
  batch 1000 loss: 3.481061751127243
LOSS train 3.481061751127243 valid 3.4869415760040283
EPOCH 35:
  batch 1000 loss: 3.491931875944138
LOSS train 3.491931875944138 valid 3.6938529014587402
EPOCH 36:
  batch 1000 loss: 3.468852774143219
LOSS train 3.468852774143219 valid 3.4871530532836914
EPOCH 37:
  batch 1000 loss: 3.4528456773757936
LOSS train 3.4528456773757936 valid 3.5006332397460938
EPOCH 38:
  batch 1000 loss: 3.4709246814250947
LOSS train 3.4709246814250947 valid 3.510406255722046
EPOCH 39:
  batch 1000 loss: 3.457086242675781
LOSS train 3.457086242675781 valid 3.4716315269470215
EPOCH 40:
  batch 1000 loss: 3.452319561600685
LOSS train 3.452319561600685 valid 3.4594268798828125
EPOCH 41:
  batch 1000 loss: 3.4481556822061536
LOSS train 3.4481556822061536 valid 3.460334062576294
EPOCH 42:
  batch 1000 loss: 3.4650365455150602
LOSS train 3.4650365455150602 valid 3.4653103351593018
EPOCH 43:
  batch 1000 loss: 3.446142840385437
LOSS train 3.446142840385437 valid 3.475980520248413
EPOCH 44:
  batch 1000 loss: 3.455918384552002
LOSS train 3.455918384552002 valid 3.4623942375183105
EPOCH 45:
  batch 1000 loss: 3.455863574266434
LOSS train 3.455863574266434 valid 3.4621593952178955
EPOCH 46:
  batch 1000 loss: 3.433663898229599
Epoch 00046: reducing learning rate of group 0 to 5.0000e-04.
LOSS train 3.433663898229599 valid 3.5622379779815674
EPOCH 47:
  batch 1000 loss: 3.3749262830018996
LOSS train 3.3749262830018996 valid 3.4092257022857666
EPOCH 48:
  batch 1000 loss: 3.3788801209926604
LOSS train 3.3788801209926604 valid 3.396209716796875
EPOCH 49:
  batch 1000 loss: 3.376698993563652
LOSS train 3.376698993563652 valid 3.3960328102111816
EPOCH 50:
  batch 1000 loss: 3.373969181895256
LOSS train 3.373969181895256 valid 3.4038078784942627
EPOCH 51:
  batch 1000 loss: 3.3821640934944153
LOSS train 3.3821640934944153 valid 3.406536340713501
EPOCH 52:
  batch 1000 loss: 3.3741594355106352
LOSS train 3.3741594355106352 valid 3.3936374187469482
EPOCH 53:
  batch 1000 loss: 3.365083667039871
LOSS train 3.365083667039871 valid 3.389036178588867
EPOCH 54:
  batch 1000 loss: 3.353251645088196
LOSS train 3.353251645088196 valid 3.386259078979492
EPOCH 55:
  batch 1000 loss: 3.352770591855049
LOSS train 3.352770591855049 valid 3.3934805393218994
EPOCH 56:
  batch 1000 loss: 3.3725198516845705
LOSS train 3.3725198516845705 valid 3.3823630809783936
EPOCH 57:
  batch 1000 loss: 3.3523216409683227
LOSS train 3.3523216409683227 valid 3.3780527114868164
EPOCH 58:
  batch 1000 loss: 3.3536264822483064
LOSS train 3.3536264822483064 valid 3.382847309112549
EPOCH 59:
  batch 1000 loss: 3.364493270635605
LOSS train 3.364493270635605 valid 3.3759219646453857
EPOCH 60:
  batch 1000 loss: 3.3695631618499755
LOSS train 3.3695631618499755 valid 3.380204439163208
EPOCH 61:
  batch 1000 loss: 3.3727199637889864
LOSS train 3.3727199637889864 valid 3.371288299560547
EPOCH 62:
  batch 1000 loss: 3.36449350130558
LOSS train 3.36449350130558 valid 3.3859004974365234
EPOCH 63:
  batch 1000 loss: 3.356670247077942
LOSS train 3.356670247077942 valid 3.3845937252044678
EPOCH 64:
  batch 1000 loss: 3.3469897413253786
LOSS train 3.3469897413253786 valid 3.368051052093506
EPOCH 65:
  batch 1000 loss: 3.3638133153915404
LOSS train 3.3638133153915404 valid 3.3805031776428223
EPOCH 66:
  batch 1000 loss: 3.350028750181198
LOSS train 3.350028750181198 valid 3.3776142597198486
EPOCH 67:
  batch 1000 loss: 3.3471544331312177
LOSS train 3.3471544331312177 valid 3.4157395362854004
EPOCH 68:
  batch 1000 loss: 3.346694398880005
LOSS train 3.346694398880005 valid 3.3704776763916016
EPOCH 69:
  batch 1000 loss: 3.3469506871700285
LOSS train 3.3469506871700285 valid 3.372096300125122
EPOCH 70:
  batch 1000 loss: 3.352896795868874
Epoch 00070: reducing learning rate of group 0 to 2.5000e-04.
LOSS train 3.352896795868874 valid 3.377901554107666
EPOCH 71:
  batch 1000 loss: 3.3173916321992873
LOSS train 3.3173916321992873 valid 3.343938112258911
EPOCH 72:
  batch 1000 loss: 3.3204468528032303
LOSS train 3.3204468528032303 valid 3.3443336486816406
EPOCH 73:
  batch 1000 loss: 3.312208510637283
LOSS train 3.312208510637283 valid 3.341667652130127
EPOCH 74:
  batch 1000 loss: 3.3120411030054093
LOSS train 3.3120411030054093 valid 3.344991683959961
EPOCH 75:
  batch 1000 loss: 3.3085811302661896
LOSS train 3.3085811302661896 valid 3.3387820720672607
EPOCH 76:
  batch 1000 loss: 3.3206115753650667
LOSS train 3.3206115753650667 valid 3.338308811187744
EPOCH 77:
  batch 1000 loss: 3.309304719924927
LOSS train 3.309304719924927 valid 3.339571475982666
EPOCH 78:
  batch 1000 loss: 3.313117656111717
LOSS train 3.313117656111717 valid 3.351623773574829
EPOCH 79:
  batch 1000 loss: 3.305824843645096
LOSS train 3.305824843645096 valid 3.337451219558716
EPOCH 80:
  batch 1000 loss: 3.3104742136001586
LOSS train 3.3104742136001586 valid 3.3424787521362305
EPOCH 81:
  batch 1000 loss: 3.3069455330371857
LOSS train 3.3069455330371857 valid 3.335090160369873
EPOCH 82:
  batch 1000 loss: 3.3211212222576143
LOSS train 3.3211212222576143 valid 3.337900161743164
EPOCH 83:
  batch 1000 loss: 3.3127703893184663
LOSS train 3.3127703893184663 valid 3.336219549179077
EPOCH 84:
  batch 1000 loss: 3.303003867864609
LOSS train 3.303003867864609 valid 3.347543478012085
EPOCH 85:
  batch 1000 loss: 3.3064316041469572
LOSS train 3.3064316041469572 valid 3.3404715061187744
EPOCH 86:
  batch 1000 loss: 3.2866145536899567
LOSS train 3.2866145536899567 valid 3.3339693546295166
EPOCH 87:
  batch 1000 loss: 3.309528084039688
LOSS train 3.309528084039688 valid 3.33787202835083
EPOCH 88:
  batch 1000 loss: 3.295174262523651
LOSS train 3.295174262523651 valid 3.3370282649993896
EPOCH 89:
  batch 1000 loss: 3.293289030432701
LOSS train 3.293289030432701 valid 3.336176872253418
EPOCH 90:
  batch 1000 loss: 3.3053936384916307
LOSS train 3.3053936384916307 valid 3.327488422393799
EPOCH 91:
  batch 1000 loss: 3.3028678796291353
LOSS train 3.3028678796291353 valid 3.353325366973877
EPOCH 92:
  batch 1000 loss: 3.312529137611389
LOSS train 3.312529137611389 valid 3.3338820934295654
EPOCH 93:
  batch 1000 loss: 3.2960001138448716
LOSS train 3.2960001138448716 valid 3.334584951400757
EPOCH 94:
  batch 1000 loss: 3.3035876445770262
LOSS train 3.3035876445770262 valid 3.327186346054077
EPOCH 95:
  batch 1000 loss: 3.2825785953998565
LOSS train 3.2825785953998565 valid 3.3272531032562256
EPOCH 96:
  batch 1000 loss: 3.2875770826339723
LOSS train 3.2875770826339723 valid 3.326530933380127
EPOCH 97:
  batch 1000 loss: 3.28557471036911
LOSS train 3.28557471036911 valid 3.326897621154785
EPOCH 98:
  batch 1000 loss: 3.2977207317352293
LOSS train 3.2977207317352293 valid 3.3229973316192627
EPOCH 99:
  batch 1000 loss: 3.2910046832561495
LOSS train 3.2910046832561495 valid 3.3269877433776855
EPOCH 100:
  batch 1000 loss: 3.290532378435135
LOSS train 3.290532378435135 valid 3.351665735244751
EPOCH 101:
  batch 1000 loss: 3.303124658346176
LOSS train 3.303124658346176 valid 3.3280489444732666
EPOCH 102:
  batch 1000 loss: 3.2894195543527602
LOSS train 3.2894195543527602 valid 3.330751895904541
EPOCH 103:
  batch 1000 loss: 3.2997249166965483
LOSS train 3.2997249166965483 valid 3.3360795974731445
EPOCH 104:
  batch 1000 loss: 3.2973831350803375
Epoch 00104: reducing learning rate of group 0 to 1.2500e-04.
LOSS train 3.2973831350803375 valid 3.3333020210266113
EPOCH 105:
  batch 1000 loss: 3.2953339244127275
LOSS train 3.2953339244127275 valid 3.3159472942352295
EPOCH 106:
  batch 1000 loss: 3.271650925040245
LOSS train 3.271650925040245 valid 3.3116116523742676
EPOCH 107:
  batch 1000 loss: 3.2848256282806396
LOSS train 3.2848256282806396 valid 3.312429904937744
EPOCH 108:
  batch 1000 loss: 3.2859504395723342
LOSS train 3.2859504395723342 valid 3.3147053718566895
EPOCH 109:
  batch 1000 loss: 3.2842460260391237
LOSS train 3.2842460260391237 valid 3.312005043029785
EPOCH 110:
  batch 1000 loss: 3.2713457691669463
LOSS train 3.2713457691669463 valid 3.3098926544189453
EPOCH 111:
  batch 1000 loss: 3.2716898666620255
LOSS train 3.2716898666620255 valid 3.312059164047241
EPOCH 112:
  batch 1000 loss: 3.274591603040695
LOSS train 3.274591603040695 valid 3.3102123737335205
EPOCH 113:
  batch 1000 loss: 3.281996146440506
LOSS train 3.281996146440506 valid 3.3085625171661377
EPOCH 114:
  batch 1000 loss: 3.269398964881897
LOSS train 3.269398964881897 valid 3.309227466583252
EPOCH 115:
  batch 1000 loss: 3.2761534237861634
LOSS train 3.2761534237861634 valid 3.310047149658203
EPOCH 116:
  batch 1000 loss: 3.284298683166504
LOSS train 3.284298683166504 valid 3.3079376220703125
EPOCH 117:
  batch 1000 loss: 3.26930204474926
LOSS train 3.26930204474926 valid 3.308540105819702
EPOCH 118:
  batch 1000 loss: 3.2789709531068802
LOSS train 3.2789709531068802 valid 3.310695171356201
EPOCH 119:
  batch 1000 loss: 3.2689031406641007
LOSS train 3.2689031406641007 valid 3.308990716934204
EPOCH 120:
  batch 1000 loss: 3.261461658000946
LOSS train 3.261461658000946 valid 3.312922239303589
EPOCH 121:
  batch 1000 loss: 3.263722043991089
LOSS train 3.263722043991089 valid 3.309530019760132
EPOCH 122:
  batch 1000 loss: 3.276814487695694
Epoch 00122: reducing learning rate of group 0 to 6.2500e-05.
LOSS train 3.276814487695694 valid 3.3102753162384033
EPOCH 123:
  batch 1000 loss: 3.258916557788849
LOSS train 3.258916557788849 valid 3.3036270141601562
EPOCH 124:
  batch 1000 loss: 3.2669612889289854
LOSS train 3.2669612889289854 valid 3.302492141723633
EPOCH 125:
  batch 1000 loss: 3.26217715382576
LOSS train 3.26217715382576 valid 3.3015222549438477
EPOCH 126:
  batch 1000 loss: 3.269549321651459
LOSS train 3.269549321651459 valid 3.302408456802368
EPOCH 127:
  batch 1000 loss: 3.2627089564800262
LOSS train 3.2627089564800262 valid 3.3015334606170654
EPOCH 128:
  batch 1000 loss: 3.256193222284317
LOSS train 3.256193222284317 valid 3.3028104305267334
EPOCH 129:
  batch 1000 loss: 3.2743840974569323
LOSS train 3.2743840974569323 valid 3.303306818008423
EPOCH 130:
  batch 1000 loss: 3.275291732788086
LOSS train 3.275291732788086 valid 3.301490545272827
EPOCH 131:
  batch 1000 loss: 3.26329996740818
Epoch 00131: reducing learning rate of group 0 to 3.1250e-05.
LOSS train 3.26329996740818 valid 3.3013811111450195
EPOCH 132:
  batch 1000 loss: 3.2569154537916183
LOSS train 3.2569154537916183 valid 3.3003439903259277
EPOCH 133:
  batch 1000 loss: 3.279467899918556
LOSS train 3.279467899918556 valid 3.2991156578063965
EPOCH 134:
  batch 1000 loss: 3.2560956836938857
LOSS train 3.2560956836938857 valid 3.2994258403778076
EPOCH 135:
  batch 1000 loss: 3.2539361435174943
LOSS train 3.2539361435174943 valid 3.300086736679077
EPOCH 136:
  batch 1000 loss: 3.2718745880126954
LOSS train 3.2718745880126954 valid 3.299203634262085
EPOCH 137:
  batch 1000 loss: 3.2616149089336397
LOSS train 3.2616149089336397 valid 3.2989423274993896
EPOCH 138:
  batch 1000 loss: 3.2508961079120637
LOSS train 3.2508961079120637 valid 3.298985242843628
EPOCH 139:
  batch 1000 loss: 3.2606105120182036
LOSS train 3.2606105120182036 valid 3.2982194423675537
EPOCH 140:
  batch 1000 loss: 3.2480714218616487
LOSS train 3.2480714218616487 valid 3.298811912536621
EPOCH 141:
  batch 1000 loss: 3.2638246365785597
LOSS train 3.2638246365785597 valid 3.297640562057495
EPOCH 142:
  batch 1000 loss: 3.2632180013656615
LOSS train 3.2632180013656615 valid 3.2980995178222656
EPOCH 143:
  batch 1000 loss: 3.2608312511444093
LOSS train 3.2608312511444093 valid 3.298280715942383
EPOCH 144:
  batch 1000 loss: 3.2751049778461456
LOSS train 3.2751049778461456 valid 3.2987382411956787
EPOCH 145:
  batch 1000 loss: 3.260549096941948
LOSS train 3.260549096941948 valid 3.2980692386627197
EPOCH 146:
  batch 1000 loss: 3.2600632314682008
LOSS train 3.2600632314682008 valid 3.298774480819702
EPOCH 147:
  batch 1000 loss: 3.2579981188774108
Epoch 00147: reducing learning rate of group 0 to 1.5625e-05.
LOSS train 3.2579981188774108 valid 3.298325300216675
EPOCH 148:
  batch 1000 loss: 3.2759651703834534
LOSS train 3.2759651703834534 valid 3.2968382835388184
EPOCH 149:
  batch 1000 loss: 3.2703269048929213
LOSS train 3.2703269048929213 valid 3.296835422515869
EPOCH 150:
  batch 1000 loss: 3.257211560368538
LOSS train 3.257211560368538 valid 3.2959787845611572
EPOCH 151:
  batch 1000 loss: 3.2582101130485537
LOSS train 3.2582101130485537 valid 3.2956182956695557
EPOCH 152:
  batch 1000 loss: 3.2519976885318758
LOSS train 3.2519976885318758 valid 3.296384572982788
EPOCH 153:
  batch 1000 loss: 3.2641059517860413
LOSS train 3.2641059517860413 valid 3.296907663345337
EPOCH 154:
  batch 1000 loss: 3.2729987140893937
LOSS train 3.2729987140893937 valid 3.296696186065674
EPOCH 155:
  batch 1000 loss: 3.2451987841129304
LOSS train 3.2451987841129304 valid 3.2963626384735107
EPOCH 156:
  batch 1000 loss: 3.258986360788345
LOSS train 3.258986360788345 valid 3.2963831424713135
EPOCH 157:
  batch 1000 loss: 3.2677778413295746
Epoch 00157: reducing learning rate of group 0 to 7.8125e-06.
LOSS train 3.2677778413295746 valid 3.2960689067840576
EPOCH 158:
  batch 1000 loss: 3.263165964007378
LOSS train 3.263165964007378 valid 3.295541524887085
EPOCH 159:
  batch 1000 loss: 3.2620339604616166
LOSS train 3.2620339604616166 valid 3.295297145843506
EPOCH 160:
  batch 1000 loss: 3.259535948038101
LOSS train 3.259535948038101 valid 3.2961232662200928
EPOCH 161:
  batch 1000 loss: 3.2559286187887193
LOSS train 3.2559286187887193 valid 3.2961485385894775
EPOCH 162:
  batch 1000 loss: 3.2692099167108535
LOSS train 3.2692099167108535 valid 3.2963690757751465
EPOCH 163:
  batch 1000 loss: 3.253007177233696
Epoch 00163: reducing learning rate of group 0 to 3.9063e-06.
LOSS train 3.253007177233696 valid 3.2954609394073486
EPOCH 164:
  batch 1000 loss: 3.270536105632782
LOSS train 3.270536105632782 valid 3.2959094047546387
EPOCH 165:
  batch 1000 loss: 3.2586221418380736
LOSS train 3.2586221418380736 valid 3.296694040298462
EPOCH 166:
  batch 1000 loss: 3.245697834610939
LOSS train 3.245697834610939 valid 3.2970240116119385
EPOCH 167:
  batch 1000 loss: 3.247698225736618
LOSS train 3.247698225736618 valid 3.295117139816284
EPOCH 168:
  batch 1000 loss: 3.2601549780368804
LOSS train 3.2601549780368804 valid 3.2953739166259766
EPOCH 169:
  batch 1000 loss: 3.2468615661859515
LOSS train 3.2468615661859515 valid 3.2954671382904053
EPOCH 170:
  batch 1000 loss: 3.254981341600418
LOSS train 3.254981341600418 valid 3.295517683029175
EPOCH 171:
  batch 1000 loss: 3.2603509558439256
LOSS train 3.2603509558439256 valid 3.295640707015991
EPOCH 172:
  batch 1000 loss: 3.2598290612697602
LOSS train 3.2598290612697602 valid 3.2963368892669678
EPOCH 173:
  batch 1000 loss: 3.254521996021271
Epoch 00173: reducing learning rate of group 0 to 1.9531e-06.
LOSS train 3.254521996021271 valid 3.295661687850952
EPOCH 174:
  batch 1000 loss: 3.2519845490455626
LOSS train 3.2519845490455626 valid 3.295031785964966
EPOCH 175:
  batch 1000 loss: 3.2565103890895846
LOSS train 3.2565103890895846 valid 3.2951788902282715
EPOCH 176:
  batch 1000 loss: 3.2705703486204145
LOSS train 3.2705703486204145 valid 3.2956788539886475
EPOCH 177:
  batch 1000 loss: 3.2469923763275146
LOSS train 3.2469923763275146 valid 3.2947287559509277
EPOCH 178:
  batch 1000 loss: 3.2586024967432023
LOSS train 3.2586024967432023 valid 3.2949695587158203
EPOCH 179:
  batch 1000 loss: 3.2571979955434798
LOSS train 3.2571979955434798 valid 3.2960970401763916
EPOCH 180:
  batch 1000 loss: 3.259754070520401
LOSS train 3.259754070520401 valid 3.2946009635925293
EPOCH 181:
  batch 1000 loss: 3.2620249625444413
LOSS train 3.2620249625444413 valid 3.295219898223877
EPOCH 182:
  batch 1000 loss: 3.2516635831594467
LOSS train 3.2516635831594467 valid 3.295767307281494
EPOCH 183:
  batch 1000 loss: 3.264702495574951
Epoch 00183: reducing learning rate of group 0 to 9.7656e-07.
LOSS train 3.264702495574951 valid 3.2956624031066895
EPOCH 184:
  batch 1000 loss: 3.261444705247879
LOSS train 3.261444705247879 valid 3.2949719429016113
EPOCH 185:
  batch 1000 loss: 3.2611390337944033
LOSS train 3.2611390337944033 valid 3.2958338260650635
EPOCH 186:
  batch 1000 loss: 3.2401621890068055
LOSS train 3.2401621890068055 valid 3.294145345687866
EPOCH 187:
  batch 1000 loss: 3.240948198080063
LOSS train 3.240948198080063 valid 3.295121192932129
EPOCH 188:
  batch 1000 loss: 3.2546270227432252
LOSS train 3.2546270227432252 valid 3.2958216667175293
EPOCH 189:
  batch 1000 loss: 3.2471224749088288
LOSS train 3.2471224749088288 valid 3.295025110244751
EPOCH 190:
  batch 1000 loss: 3.2503288354873656
LOSS train 3.2503288354873656 valid 3.295417547225952
EPOCH 191:
  batch 1000 loss: 3.2630177537202836
LOSS train 3.2630177537202836 valid 3.295489549636841
EPOCH 192:
  batch 1000 loss: 3.2656676470041277
Epoch 00192: reducing learning rate of group 0 to 4.8828e-07.
LOSS train 3.2656676470041277 valid 3.2952523231506348
EPOCH 193:
  batch 1000 loss: 3.2687518270015716
LOSS train 3.2687518270015716 valid 3.2960753440856934
EPOCH 194:
  batch 1000 loss: 3.2573260065317156
LOSS train 3.2573260065317156 valid 3.2964682579040527
EPOCH 195:
  batch 1000 loss: 3.2503275632858277
LOSS train 3.2503275632858277 valid 3.295607328414917
EPOCH 196:
  batch 1000 loss: 3.2553579049110413
LOSS train 3.2553579049110413 valid 3.2952542304992676
EPOCH 197:
  batch 1000 loss: 3.2565301835536955
LOSS train 3.2565301835536955 valid 3.2950618267059326
EPOCH 198:
  batch 1000 loss: 3.2387589907646177
Epoch 00198: reducing learning rate of group 0 to 2.4414e-07.
LOSS train 3.2387589907646177 valid 3.295433282852173
EPOCH 199:
  batch 1000 loss: 3.253392452478409
LOSS train 3.253392452478409 valid 3.2952332496643066
EPOCH 200:
  batch 1000 loss: 3.271282060742378
LOSS train 3.271282060742378 valid 3.295525074005127
EPOCH 201:
  batch 1000 loss: 3.259525994300842
LOSS train 3.259525994300842 valid 3.2957992553710938
EPOCH 202:
  batch 1000 loss: 3.241716134548187
LOSS train 3.241716134548187 valid 3.2953741550445557
EPOCH 203:
  batch 1000 loss: 3.247721470117569
LOSS train 3.247721470117569 valid 3.2948977947235107
EPOCH 204:
  batch 1000 loss: 3.263878604054451
Epoch 00204: reducing learning rate of group 0 to 1.2207e-07.
LOSS train 3.263878604054451 valid 3.297043561935425
EPOCH 205:
  batch 1000 loss: 3.267892604827881
LOSS train 3.267892604827881 valid 3.2951977252960205
EPOCH 206:
  batch 1000 loss: 3.2445189841985704
LOSS train 3.2445189841985704 valid 3.295238733291626
EPOCH 207:
  batch 1000 loss: 3.2553997194766997
LOSS train 3.2553997194766997 valid 3.2945451736450195
EPOCH 208:
  batch 1000 loss: 3.2325818531513213
LOSS train 3.2325818531513213 valid 3.2954516410827637
EPOCH 209:
  batch 1000 loss: 3.254457112073898
LOSS train 3.254457112073898 valid 3.2952847480773926
EPOCH 210:
  batch 1000 loss: 3.2644987664222715
Epoch 00210: reducing learning rate of group 0 to 6.1035e-08.
LOSS train 3.2644987664222715 valid 3.2949230670928955
EPOCH 211:
  batch 1000 loss: 3.2550517262220384
LOSS train 3.2550517262220384 valid 3.295837879180908
EPOCH 212:
  batch 1000 loss: 3.2604087867736817
LOSS train 3.2604087867736817 valid 3.295334577560425
EPOCH 213:
  batch 1000 loss: 3.25824656188488
LOSS train 3.25824656188488 valid 3.295236110687256
EPOCH 214:
  batch 1000 loss: 3.273350231409073
LOSS train 3.273350231409073 valid 3.295363187789917
EPOCH 215:
  batch 1000 loss: 3.2508654459714887
LOSS train 3.2508654459714887 valid 3.2948124408721924
EPOCH 216:
  batch 1000 loss: 3.2749734303951263
Epoch 00216: reducing learning rate of group 0 to 3.0518e-08.
LOSS train 3.2749734303951263 valid 3.2954800128936768
EPOCH 217:
  batch 1000 loss: 3.248679352521896
LOSS train 3.248679352521896 valid 3.294522762298584
EPOCH 218:
  batch 1000 loss: 3.2580670907497407
LOSS train 3.2580670907497407 valid 3.29478120803833
EPOCH 219:
  batch 1000 loss: 3.26531299161911
LOSS train 3.26531299161911 valid 3.295833110809326
EPOCH 220:
  batch 1000 loss: 3.235895160317421
LOSS train 3.235895160317421 valid 3.295532703399658
EPOCH 221:
  batch 1000 loss: 3.271245194077492
LOSS train 3.271245194077492 valid 3.295767307281494
EPOCH 222:
  batch 1000 loss: 3.252199601650238
Epoch 00222: reducing learning rate of group 0 to 1.5259e-08.
LOSS train 3.252199601650238 valid 3.2949326038360596
EPOCH 223:
  batch 1000 loss: 3.257018377542496
LOSS train 3.257018377542496 valid 3.2950849533081055
EPOCH 224:
  batch 1000 loss: 3.246271506309509
LOSS train 3.246271506309509 valid 3.2948410511016846
EPOCH 225:
  batch 1000 loss: 3.2585919337272644
LOSS train 3.2585919337272644 valid 3.2949938774108887
EPOCH 226:
  batch 1000 loss: 3.2647247135639192
LOSS train 3.2647247135639192 valid 3.295924186706543
EPOCH 227:
  batch 1000 loss: 3.255608302116394
LOSS train 3.255608302116394 valid 3.295950174331665
EPOCH 228:
  batch 1000 loss: 3.2549518078565596
LOSS train 3.2549518078565596 valid 3.296154260635376
EPOCH 229:
  batch 1000 loss: 3.259819132566452
LOSS train 3.259819132566452 valid 3.295440912246704
EPOCH 230:
  batch 1000 loss: 3.267933521270752
LOSS train 3.267933521270752 valid 3.295814037322998
EPOCH 231:
  batch 1000 loss: 3.2733232194185256
LOSS train 3.2733232194185256 valid 3.2954020500183105
EPOCH 232:
  batch 1000 loss: 3.2608461351394653
LOSS train 3.2608461351394653 valid 3.2952442169189453
EPOCH 233:
  batch 1000 loss: 3.241961515426636
LOSS train 3.241961515426636 valid 3.2949836254119873
EPOCH 234:
  batch 1000 loss: 3.275893336057663
LOSS train 3.275893336057663 valid 3.2946314811706543
EPOCH 235:
  batch 1000 loss: 3.263554258823395
LOSS train 3.263554258823395 valid 3.2948849201202393
EPOCH 236:
  batch 1000 loss: 3.262321455001831
LOSS train 3.262321455001831 valid 3.297536849975586
EPOCH 237:
  batch 1000 loss: 3.2657485049962998
LOSS train 3.2657485049962998 valid 3.2954883575439453
EPOCH 238:
  batch 1000 loss: 3.2699435487985613
LOSS train 3.2699435487985613 valid 3.296321392059326
EPOCH 239:
  batch 1000 loss: 3.241917893409729
LOSS train 3.241917893409729 valid 3.2954354286193848
EPOCH 240:
  batch 1000 loss: 3.247809497833252
LOSS train 3.247809497833252 valid 3.2952847480773926
EPOCH 241:
  batch 1000 loss: 3.2690082619190215
LOSS train 3.2690082619190215 valid 3.2955644130706787
EPOCH 242:
  batch 1000 loss: 3.265425960659981
LOSS train 3.265425960659981 valid 3.295536756515503
EPOCH 243:
  batch 1000 loss: 3.2625645678043367
LOSS train 3.2625645678043367 valid 3.2945361137390137
EPOCH 244:
  batch 1000 loss: 3.270393802165985
LOSS train 3.270393802165985 valid 3.295527219772339
EPOCH 245:
  batch 1000 loss: 3.2567967636585236
LOSS train 3.2567967636585236 valid 3.2954659461975098
EPOCH 246:
  batch 1000 loss: 3.2616292794942856
LOSS train 3.2616292794942856 valid 3.29569149017334
EPOCH 247:
  batch 1000 loss: 3.2499489114284517
LOSS train 3.2499489114284517 valid 3.294273614883423
EPOCH 248:
  batch 1000 loss: 3.279218248844147
LOSS train 3.279218248844147 valid 3.2956764698028564
EPOCH 249:
  batch 1000 loss: 3.2705668890476227
LOSS train 3.2705668890476227 valid 3.295849561691284
EPOCH 250:
  batch 1000 loss: 3.260744594573975
LOSS train 3.260744594573975 valid 3.295396566390991
EPOCH 251:
  batch 1000 loss: 3.2503917865753174
LOSS train 3.2503917865753174 valid 3.2954659461975098
EPOCH 252:
  batch 1000 loss: 3.2609076471328735
LOSS train 3.2609076471328735 valid 3.295407295227051
EPOCH 253:
  batch 1000 loss: 3.2514694514274596
LOSS train 3.2514694514274596 valid 3.294761896133423
EPOCH 254:
  batch 1000 loss: 3.259677621126175
LOSS train 3.259677621126175 valid 3.2958385944366455
EPOCH 255:
  batch 1000 loss: 3.2602813671827318
LOSS train 3.2602813671827318 valid 3.2955989837646484
EPOCH 256:
  batch 1000 loss: 3.2502506411075593
LOSS train 3.2502506411075593 valid 3.295149564743042
EPOCH 257:
  batch 1000 loss: 3.2393375676870346
LOSS train 3.2393375676870346 valid 3.2953836917877197
EPOCH 258:
  batch 1000 loss: 3.2469706790447237
LOSS train 3.2469706790447237 valid 3.295546054840088
EPOCH 259:
  batch 1000 loss: 3.2550154404640197
LOSS train 3.2550154404640197 valid 3.2953693866729736
EPOCH 260:
  batch 1000 loss: 3.2527036788463595
LOSS train 3.2527036788463595 valid 3.294525146484375
EPOCH 261:
  batch 1000 loss: 3.2751045010089874
LOSS train 3.2751045010089874 valid 3.296600818634033
EPOCH 262:
  batch 1000 loss: 3.2674929611682892
LOSS train 3.2674929611682892 valid 3.2955257892608643
EPOCH 263:
  batch 1000 loss: 3.259477308511734
LOSS train 3.259477308511734 valid 3.2957632541656494
EPOCH 264:
  batch 1000 loss: 3.2683398773670196
LOSS train 3.2683398773670196 valid 3.2958216667175293
EPOCH 265:
  batch 1000 loss: 3.268114919543266
LOSS train 3.268114919543266 valid 3.2954792976379395
EPOCH 266:
  batch 1000 loss: 3.259466467380524
LOSS train 3.259466467380524 valid 3.295382261276245
EPOCH 267:
  batch 1000 loss: 3.2522332458496095
LOSS train 3.2522332458496095 valid 3.2954211235046387
EPOCH 268:
  batch 1000 loss: 3.269754115462303
LOSS train 3.269754115462303 valid 3.295670747756958
EPOCH 269:
  batch 1000 loss: 3.2647275450229647
LOSS train 3.2647275450229647 valid 3.295480966567993
EPOCH 270:
  batch 1000 loss: 3.269116860866547
LOSS train 3.269116860866547 valid 3.29536509513855
EPOCH 271:
  batch 1000 loss: 3.2762659471035005
LOSS train 3.2762659471035005 valid 3.2953197956085205
EPOCH 272:
  batch 1000 loss: 3.243821499347687
LOSS train 3.243821499347687 valid 3.295100450515747
EPOCH 273:
  batch 1000 loss: 3.2473077191114426
LOSS train 3.2473077191114426 valid 3.2958614826202393
EPOCH 274:
  batch 1000 loss: 3.2659807493686674
LOSS train 3.2659807493686674 valid 3.2949979305267334
EPOCH 275:
  batch 1000 loss: 3.261468781709671
LOSS train 3.261468781709671 valid 3.295393228530884
EPOCH 276:
  batch 1000 loss: 3.2593405764102936
LOSS train 3.2593405764102936 valid 3.295485019683838
EPOCH 277:
  batch 1000 loss: 3.2743819932937623
LOSS train 3.2743819932937623 valid 3.29548978805542
EPOCH 278:
  batch 1000 loss: 3.2453795404434205
LOSS train 3.2453795404434205 valid 3.2963459491729736
EPOCH 279:
  batch 1000 loss: 3.2589558181762697
LOSS train 3.2589558181762697 valid 3.2951817512512207
EPOCH 280:
  batch 1000 loss: 3.265053795337677
LOSS train 3.265053795337677 valid 3.295151710510254
EPOCH 281:
  batch 1000 loss: 3.2513638607263564
LOSS train 3.2513638607263564 valid 3.294569730758667
EPOCH 282:
  batch 1000 loss: 3.269768945455551
LOSS train 3.269768945455551 valid 3.2949318885803223
EPOCH 283:
  batch 1000 loss: 3.2586828870773314
LOSS train 3.2586828870773314 valid 3.2954604625701904
EPOCH 284:
  batch 1000 loss: 3.256642288208008
LOSS train 3.256642288208008 valid 3.294469118118286
EPOCH 285:
  batch 1000 loss: 3.2638482860326765
LOSS train 3.2638482860326765 valid 3.294957399368286
EPOCH 286:
  batch 1000 loss: 3.2619331814050674
LOSS train 3.2619331814050674 valid 3.2966465950012207
EPOCH 287:
  batch 1000 loss: 3.2602916003465654
LOSS train 3.2602916003465654 valid 3.294602870941162
EPOCH 288:
  batch 1000 loss: 3.257845365166664
LOSS train 3.257845365166664 valid 3.295713424682617
EPOCH 289:
  batch 1000 loss: 3.252935865998268
LOSS train 3.252935865998268 valid 3.2947511672973633
EPOCH 290:
  batch 1000 loss: 3.246882869243622
LOSS train 3.246882869243622 valid 3.295717477798462
EPOCH 291:
  batch 1000 loss: 3.2494067907333375
LOSS train 3.2494067907333375 valid 3.2962844371795654
EPOCH 292:
  batch 1000 loss: 3.250090206503868
LOSS train 3.250090206503868 valid 3.296008348464966
EPOCH 293:
  batch 1000 loss: 3.251838634133339
LOSS train 3.251838634133339 valid 3.2955405712127686
EPOCH 294:
  batch 1000 loss: 3.27028265953064
LOSS train 3.27028265953064 valid 3.2956459522247314
EPOCH 295:
  batch 1000 loss: 3.259605547428131
LOSS train 3.259605547428131 valid 3.295396327972412
EPOCH 296:
  batch 1000 loss: 3.2572329251766203
LOSS train 3.2572329251766203 valid 3.2959675788879395
EPOCH 297:
  batch 1000 loss: 3.25880970287323
LOSS train 3.25880970287323 valid 3.296072483062744
EPOCH 298:
  batch 1000 loss: 3.253407541036606
LOSS train 3.253407541036606 valid 3.294416666030884
EPOCH 299:
  batch 1000 loss: 3.240020139336586
LOSS train 3.240020139336586 valid 3.296207904815674
EPOCH 300:
  batch 1000 loss: 3.258590841770172
LOSS train 3.258590841770172 valid 3.294748306274414
EPOCH 301:
  batch 1000 loss: 3.2555002992153166
LOSS train 3.2555002992153166 valid 3.2956106662750244
EPOCH 302:
  batch 1000 loss: 3.264867098093033
LOSS train 3.264867098093033 valid 3.2956645488739014
EPOCH 303:
  batch 1000 loss: 3.2590031585693358
LOSS train 3.2590031585693358 valid 3.2953972816467285
EPOCH 304:
  batch 1000 loss: 3.2651181156635283
LOSS train 3.2651181156635283 valid 3.2963716983795166
EPOCH 305:
  batch 1000 loss: 3.261084266424179
LOSS train 3.261084266424179 valid 3.296494960784912
EPOCH 306:
  batch 1000 loss: 3.263947993636131
LOSS train 3.263947993636131 valid 3.296462059020996
EPOCH 307:
  batch 1000 loss: 3.2671987875699995
LOSS train 3.2671987875699995 valid 3.295280694961548
EPOCH 308:
  batch 1000 loss: 3.268319987773895
LOSS train 3.268319987773895 valid 3.295036554336548
EPOCH 309:
  batch 1000 loss: 3.251684818148613
LOSS train 3.251684818148613 valid 3.2950353622436523
EPOCH 310:
  batch 1000 loss: 3.2606731548309327
LOSS train 3.2606731548309327 valid 3.295257091522217
EPOCH 311:
  batch 1000 loss: 3.271837414264679
LOSS train 3.271837414264679 valid 3.295363664627075
EPOCH 312:
  batch 1000 loss: 3.2606512172222137
LOSS train 3.2606512172222137 valid 3.2954204082489014
EPOCH 313:
  batch 1000 loss: 3.2500835806131363
LOSS train 3.2500835806131363 valid 3.2960498332977295
EPOCH 314:
  batch 1000 loss: 3.257231242895126
LOSS train 3.257231242895126 valid 3.2953925132751465
EPOCH 315:
  batch 1000 loss: 3.2501626241207124
LOSS train 3.2501626241207124 valid 3.295427083969116
EPOCH 316:
  batch 1000 loss: 3.2558335237503053
LOSS train 3.2558335237503053 valid 3.2957160472869873
EPOCH 317:
  batch 1000 loss: 3.2654322148561477
LOSS train 3.2654322148561477 valid 3.295564651489258
EPOCH 318:
  batch 1000 loss: 3.2448321089744567
LOSS train 3.2448321089744567 valid 3.2953836917877197
EPOCH 319:
  batch 1000 loss: 3.2621865277290345
LOSS train 3.2621865277290345 valid 3.295365333557129
EPOCH 320:
  batch 1000 loss: 3.261046204328537
LOSS train 3.261046204328537 valid 3.2963473796844482
EPOCH 321:
  batch 1000 loss: 3.2529450101852415
LOSS train 3.2529450101852415 valid 3.2957282066345215
EPOCH 322:
  batch 1000 loss: 3.253633492708206
LOSS train 3.253633492708206 valid 3.294931411743164
EPOCH 323:
  batch 1000 loss: 3.2589250588417054
LOSS train 3.2589250588417054 valid 3.2964022159576416
EPOCH 324:
  batch 1000 loss: 3.2577465888261794
LOSS train 3.2577465888261794 valid 3.2969865798950195
EPOCH 325:
  batch 1000 loss: 3.2736353355646135
LOSS train 3.2736353355646135 valid 3.295469045639038
EPOCH 326:
  batch 1000 loss: 3.262667989850044
LOSS train 3.262667989850044 valid 3.2958576679229736
EPOCH 327:
  batch 1000 loss: 3.2671507124900816
LOSS train 3.2671507124900816 valid 3.295384168624878
EPOCH 328:
  batch 1000 loss: 3.2561031239032747
LOSS train 3.2561031239032747 valid 3.295344114303589
EPOCH 329:
  batch 1000 loss: 3.262263277053833
LOSS train 3.262263277053833 valid 3.2963969707489014
EPOCH 330:
  batch 1000 loss: 3.2539193012714387
LOSS train 3.2539193012714387 valid 3.2957608699798584
EPOCH 331:
  batch 1000 loss: 3.2591974730491637
LOSS train 3.2591974730491637 valid 3.294304132461548
EPOCH 332:
  batch 1000 loss: 3.259750960469246
LOSS train 3.259750960469246 valid 3.2951409816741943
EPOCH 333:
  batch 1000 loss: 3.2558982833623884
LOSS train 3.2558982833623884 valid 3.2952706813812256
EPOCH 334:
  batch 1000 loss: 3.2583192207813263
LOSS train 3.2583192207813263 valid 3.2946653366088867
EPOCH 335:
  batch 1000 loss: 3.256493571400642
LOSS train 3.256493571400642 valid 3.2953970432281494
EPOCH 336:
  batch 1000 loss: 3.2556513983011244
LOSS train 3.2556513983011244 valid 3.294779062271118
EPOCH 337:
  batch 1000 loss: 3.259564979314804
LOSS train 3.259564979314804 valid 3.294606924057007
EPOCH 338:
  batch 1000 loss: 3.2631721115112304
LOSS train 3.2631721115112304 valid 3.2955379486083984
EPOCH 339:
  batch 1000 loss: 3.268184654951096
LOSS train 3.268184654951096 valid 3.2950925827026367
EPOCH 340:
  batch 1000 loss: 3.252314841270447
LOSS train 3.252314841270447 valid 3.29556941986084
EPOCH 341:
  batch 1000 loss: 3.2617718876600263
LOSS train 3.2617718876600263 valid 3.2949488162994385
EPOCH 342:
  batch 1000 loss: 3.242501796245575
LOSS train 3.242501796245575 valid 3.2955949306488037
EPOCH 343:
  batch 1000 loss: 3.258323767900467
LOSS train 3.258323767900467 valid 3.2947654724121094
EPOCH 344:
  batch 1000 loss: 3.2697509355545042
LOSS train 3.2697509355545042 valid 3.2958734035491943
EPOCH 345:
  batch 1000 loss: 3.2487108588218687
LOSS train 3.2487108588218687 valid 3.296566963195801
EPOCH 346:
  batch 1000 loss: 3.2503127247095107
LOSS train 3.2503127247095107 valid 3.295912027359009
EPOCH 347:
  batch 1000 loss: 3.2624767899513243
LOSS train 3.2624767899513243 valid 3.2956466674804688
EPOCH 348:
  batch 1000 loss: 3.263505267620087
LOSS train 3.263505267620087 valid 3.2949225902557373
EPOCH 349:
  batch 1000 loss: 3.254183657646179
LOSS train 3.254183657646179 valid 3.295667886734009
EPOCH 350:
  batch 1000 loss: 3.2522028735876085
LOSS train 3.2522028735876085 valid 3.2958693504333496
EPOCH 351:
  batch 1000 loss: 3.2694474279880525
LOSS train 3.2694474279880525 valid 3.2948849201202393
EPOCH 352:
  batch 1000 loss: 3.2515042545795443
LOSS train 3.2515042545795443 valid 3.2949516773223877
EPOCH 353:
  batch 1000 loss: 3.252325176358223
LOSS train 3.252325176358223 valid 3.2960658073425293
EPOCH 354:

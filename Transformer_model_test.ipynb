{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, List, Callable, Tuple\n",
    "from torchrl.modules import MLP\n",
    "from Transformer_model import myGlue\n",
    "pruning_keypoint_thresholds = 1024\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 'mps', 'cpu'\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['keypoints', 'keypoint_scores', 'descriptors', 'image_size'])\n",
      "5578\n"
     ]
    }
   ],
   "source": [
    "x = torch.load('/home/koki/LightGlue/scannet_dataset/scannet_embed/scene0000_00.pth')\n",
    "print(x[0].keys())\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]['descriptors'].shape\n",
    "x[1]['descriptors'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 3])\n",
      "=================   torch.Size([1, 2048, 3])\n",
      "Split Tensor 1 shape: torch.Size([1, 2048, 3])\n",
      "Split Tensor 2 shape: torch.Size([1, 2048, 3])\n",
      "Split Tensor 3 shape: torch.Size([1, 2048, 3])\n",
      "Split Tensor 4 shape: torch.Size([1, 2048, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a sample tensor\n",
    "tensor = torch.randn(4, 2048, 3)\n",
    "\n",
    "# Split the tensor into 4 tensors\n",
    "split_tensors = torch.split(tensor, 1, dim=0)\n",
    "print(split_tensors[0].shape)\n",
    "# Convert the split_tensors to a list of tensors (if needed)\n",
    "split_tensors_list = list(split_tensors)\n",
    "data = []\n",
    "data.extend(split_tensors_list)\n",
    "print('=================  ',data[0].shape)\n",
    "# Print the shapes of the split tensors\n",
    "for i, split_tensor in enumerate(split_tensors_list):\n",
    "    print(f\"Split Tensor {i + 1} shape: {split_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myGlue(\n",
      "  (input_proj): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (transformers): ModuleList(\n",
      "    (0): MyTransformerLayer(\n",
      "      (self_attn): SelfBlock(\n",
      "        (toqkv): Linear(in_features=3, out_features=9, bias=True)\n",
      "        (out_proj): Linear(in_features=3, out_features=3, bias=True)\n",
      "        (inner_attn): Attention()\n",
      "      )\n",
      "      (cross_attn): CrossBlock()\n",
      "    )\n",
      "    (1): MyTransformerLayer(\n",
      "      (self_attn): SelfBlock(\n",
      "        (toqkv): Linear(in_features=16, out_features=48, bias=True)\n",
      "        (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (inner_attn): Attention()\n",
      "      )\n",
      "      (cross_attn): CrossBlock()\n",
      "    )\n",
      "    (2): MyTransformerLayer(\n",
      "      (self_attn): SelfBlock(\n",
      "        (toqkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (inner_attn): Attention()\n",
      "      )\n",
      "      (cross_attn): CrossBlock()\n",
      "    )\n",
      "    (3): MyTransformerLayer(\n",
      "      (self_attn): SelfBlock(\n",
      "        (toqkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (inner_attn): Attention()\n",
      "      )\n",
      "      (cross_attn): CrossBlock()\n",
      "    )\n",
      "    (4): MyTransformerLayer(\n",
      "      (self_attn): SelfBlock(\n",
      "        (toqkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (inner_attn): Attention()\n",
      "      )\n",
      "      (cross_attn): CrossBlock()\n",
      "    )\n",
      "  )\n",
      "  (log_assignment): ModuleList(\n",
      "    (0): MatchAssignment(\n",
      "      (matchability): Linear(in_features=3, out_features=1, bias=True)\n",
      "      (final_proj): Linear(in_features=3, out_features=3, bias=True)\n",
      "    )\n",
      "    (1): MatchAssignment(\n",
      "      (matchability): Linear(in_features=16, out_features=1, bias=True)\n",
      "      (final_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "    (2): MatchAssignment(\n",
      "      (matchability): Linear(in_features=32, out_features=1, bias=True)\n",
      "      (final_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (3): MatchAssignment(\n",
      "      (matchability): Linear(in_features=64, out_features=1, bias=True)\n",
      "      (final_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (4): MatchAssignment(\n",
      "      (matchability): Linear(in_features=128, out_features=1, bias=True)\n",
      "      (final_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (token_confidence): ModuleList(\n",
      "    (0): TokenConfidence(\n",
      "      (token): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=1, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (1): TokenConfidence(\n",
      "      (token): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=1, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (2): TokenConfidence(\n",
      "      (token): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=1, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (3): TokenConfidence(\n",
      "      (token): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=1, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (4): TokenConfidence(\n",
      "      (token): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformers = myGlue()\n",
    "print(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchAssignment(nn.Module):\n",
    "    def __init__(self,dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.matchability = nn.Linear(dim, 1, bias=True)\n",
    "        self.final_proj = nn.Linear(dim, dim, bias=True)\n",
    "    def forward(self, desc0: torch.Tensor, desc1: torch.Tensor):\n",
    "        mdesc0, mdesc1 = self.final_proj(desc0), self.final_proj(desc1)\n",
    "        _, _, d = mdesc0.shape\n",
    "        mdesc0, mdesc1 = mdesc0 / d**.25, mdesc1 / d**.25\n",
    "        sim = torch.einsum('bmd,bnd->bmn', mdesc0, mdesc1)\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1343,  0.3721, -0.4150],\n",
      "         [ 0.3869, -0.4664,  0.7231],\n",
      "         [ 0.0855,  0.0220,  0.0622],\n",
      "         [ 0.1266, -0.1156,  0.2077]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gg = MatchAssignment(2)\n",
    "desc0 = torch.randn(1,4,2)\n",
    "desc1 = torch.randn(1,3,2)\n",
    "\n",
    "sim = gg(desc0,desc1)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.has_sdp = hasattr(F, 'scaled_dot_product_attention')\n",
    "\n",
    "    def forward(self, q, k, v, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if self.has_sdp:\n",
    "            args = [x.contiguous() for x in [q, k, v]]\n",
    "            v = F.scaled_dot_product_attention(*args, attn_mask=mask)\n",
    "            return v if mask is None else v.nan_to_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Optionally use the context manager to ensure one of the fused kernels is run\n",
    "query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "value = torch.rand(32, 8, 128, 128, dtype=torch.float16, device=\"cuda\")\n",
    "with torch.backends.cuda.sdp_kernel(enable_math=False):\n",
    "    x = F.scaled_dot_product_attention(query, key, value)\n",
    "print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
